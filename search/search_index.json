{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AntFlow","text":"<p>Stop waiting for the slowest task. Start processing smarter.</p> <p> </p> <p>AntFlow is a modern async execution library for Python that solves a fundamental problem in batch processing: the slowest task bottleneck.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>When processing batches of async tasks (API calls, file operations, database queries), traditional approaches wait for ALL tasks in a batch to complete before starting the next batch:</p> <pre><code># Traditional approach - inefficient\nfor batch in chunks(items, 10):\n    await asyncio.gather(*[process(item) for item in batch])\n    # \ud83d\udc0c Wait for ALL 10 to finish, even if 9 are done\n</code></pre> <p>The bottleneck: If 9 tasks complete in 1 minute but 1 task takes 30 minutes, you waste 29 minutes of potential processing time.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>AntFlow uses independent worker pools where workers continuously grab new tasks as soon as they finish:</p> <pre><code># AntFlow approach - efficient\nstage = Stage(name=\"Process\", workers=10, tasks=[process])\npipeline = Pipeline(stages=[stage])\nawait pipeline.run(items)\n# \u2728 Workers never wait - always processing\n</code></pre> <p>The benefit: As soon as any worker finishes, it picks the next task. Zero idle time. Maximum throughput.</p>"},{"location":"#real-world-example","title":"Real-World Example","text":"<p>Processing 1000 batches through OpenAI's Batch API:</p>"},{"location":"#before-antflow","title":"Before AntFlow","text":"<ul> <li>Process 10 batches at a time</li> <li>Wait for all 10 to complete</li> <li>1 slow batch = entire group waits</li> <li>Total time: 6+ hours \u23f0</li> </ul>"},{"location":"#after-antflow","title":"After AntFlow","text":"<ul> <li>10 workers process continuously</li> <li>Slow batches don't block fast ones</li> <li>Automatic retry on failures</li> <li>Total time: 2 hours \u26a1</li> </ul> <p>Result: 3x faster processing with better reliability</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install antflow\n</code></pre>"},{"location":"#simple-example","title":"Simple Example","text":"<pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def fetch_data(item_id):\n    # Your async operation\n    return f\"data_{item_id}\"\n\nasync def process_data(data):\n    # Your processing logic\n    return data.upper()\n\nasync def main():\n    # Define pipeline stages\n    fetch_stage = Stage(name=\"Fetch\", workers=10, tasks=[fetch_data])\n    process_stage = Stage(name=\"Process\", workers=5, tasks=[process_data])\n\n    # Create pipeline\n    pipeline = Pipeline(stages=[fetch_stage, process_stage])\n\n    # Process 100 items\n    results = await pipeline.run(range(100))\n    print(f\"Processed {len(results)} items\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#worker-pool-architecture","title":"\ud83d\ude80 Worker Pool Architecture","text":"<p>Independent workers that never block each other, ensuring optimal resource utilization.</p>"},{"location":"#multi-stage-pipelines","title":"\ud83d\udd04 Multi-Stage Pipelines","text":"<p>Chain operations with configurable worker pools per stage for complex ETL workflows.</p>"},{"location":"#built-in-resilience","title":"\ud83d\udcaa Built-in Resilience","text":"<p>Per-task and per-stage retry strategies with exponential backoff handle transient failures automatically.</p>"},{"location":"#real-time-monitoring","title":"\ud83d\udcca Real-time Monitoring","text":"<p>StatusTracker for real-time item tracking with event-driven monitoring and statistics.</p>"},{"location":"#worker-level-tracking","title":"\ud83d\udc77 Worker-Level Tracking","text":"<p>Track which specific worker processes each item with unique worker IDs and custom item IDs.</p>"},{"location":"#familiar-api","title":"\ud83c\udfaf Familiar API","text":"<p>Drop-in async replacement for <code>concurrent.futures</code> with <code>submit()</code>, <code>map()</code>, and <code>as_completed()</code>.</p>"},{"location":"#type-safe","title":"\ud83d\udd27 Type Safe","text":"<p>Full type hints throughout for better IDE support and fewer bugs.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>AntFlow is perfect for:</p> <ul> <li>Batch API Processing - OpenAI, Anthropic, AWS, any batch API</li> <li>ETL Pipelines - Extract, transform, load data at scale</li> <li>Web Scraping - Fetch, parse, and store web data efficiently</li> <li>Data Processing - Process large datasets with retry logic</li> <li>Microservices - Chain async service calls with error handling</li> <li>Batch Operations - Any scenario where you process many items asynchronously</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":""},{"location":"#asyncexecutor","title":"AsyncExecutor","text":"<p>Similar to <code>concurrent.futures.ThreadPoolExecutor</code> but for async operations:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def process(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\nasync def main():\n    async with AsyncExecutor(max_workers=10) as executor:\n        results = await executor.map(process, range(100))\n        print(results)\n\nasyncio.run(main())\n</code></pre>"},{"location":"#pipeline-stages","title":"Pipeline Stages","text":"<p>Build multi-stage workflows with independent worker pools:</p> <pre><code>from antflow import Pipeline, Stage\n\n# Different worker counts optimized for each stage\nfetch_stage = Stage(name=\"Fetch\", workers=10, tasks=[fetch])      # I/O bound\nprocess_stage = Stage(name=\"Process\", workers=5, tasks=[process]) # CPU bound\nsave_stage = Stage(name=\"Save\", workers=3, tasks=[save])          # Rate limited\n\npipeline = Pipeline(stages=[fetch_stage, process_stage, save_stage])\n</code></pre>"},{"location":"#retry-strategies","title":"Retry Strategies","text":"<p>Per-Task Retry (independent operations): <pre><code>stage = Stage(\n    name=\"APICall\",\n    workers=10,\n    tasks=[call_api],\n    retry=\"per_task\",\n    task_attempts=5,\n    task_wait=2.0\n)\n</code></pre></p> <p>Per-Stage Retry (transactional operations): <pre><code>stage = Stage(\n    name=\"Transaction\",\n    workers=3,\n    tasks=[begin_tx, update_db, commit_tx],\n    retry=\"per_stage\",\n    stage_attempts=3\n)\n</code></pre></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your learning path:</p>   -   **New to AntFlow?**      [Quick Start Guide \u2192](getting-started/quickstart.md)  -   **From concurrent.futures?**      [AsyncExecutor Guide \u2192](user-guide/executor.md)  -   **Building pipelines?**      [Pipeline Guide \u2192](user-guide/pipeline.md)  -   **Ready to dive deep?**      [API Reference \u2192](api/index.md)"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>tenacity &gt;= 8.0.0</li> </ul>"},{"location":"#why-antflow","title":"Why AntFlow?","text":"<p>AntFlow was born from a real production problem: processing thousands of batches through OpenAI's API. Traditional batch processing wasted hours waiting for slow tasks to complete.</p> <p>The solution? Independent workers that never wait.</p> <p>When a worker finishes a task, it immediately grabs the next one. No coordination overhead. No bottlenecks. Just continuous, efficient processing.</p> <p>If you're processing data through APIs, building ETL pipelines, or running any kind of batch async operations, AntFlow will save you time.</p> <p>Start processing smarter, not harder. \ud83d\ude80</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub: github.com/rodolfonobrega/antflow</li> <li>PyPI: pypi.org/project/antflow</li> <li>Issues: Report a bug</li> </ul>"},{"location":"#license","title":"License","text":"<p>AntFlow is released under the MIT License.</p> <p> Made with \u2764\ufe0f to solve real problems in production </p>"},{"location":"contributing/","title":"Contributing to AntFlow","text":"<p>Thank you for your interest in contributing to AntFlow! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>pip</li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/rodolfonobrega/antflow.git\ncd antflow\n</code></pre></p> </li> <li> <p>Install in development mode with dev dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>Run all tests: <pre><code>pytest tests/ -v\n</code></pre></p> <p>Run specific test file: <pre><code>pytest tests/test_executor.py -v\n</code></pre></p> <p>Run with coverage: <pre><code>pytest tests/ --cov=antflow --cov-report=html\n</code></pre></p>"},{"location":"contributing/#code-quality","title":"Code Quality","text":""},{"location":"contributing/#linting","title":"Linting","text":"<p>We use <code>ruff</code> for linting: <pre><code>ruff check antflow/\n</code></pre></p> <p>Auto-fix issues: <pre><code>ruff check --fix antflow/\n</code></pre></p>"},{"location":"contributing/#type-checking","title":"Type Checking","text":"<p>We use <code>mypy</code> for type checking: <pre><code>mypy antflow/\n</code></pre></p>"},{"location":"contributing/#formatting","title":"Formatting","text":"<p>Format code with ruff: <pre><code>ruff format antflow/\n</code></pre></p>"},{"location":"contributing/#running-examples","title":"Running Examples","text":"<p>Test examples to ensure they work: <pre><code>python examples/basic_executor.py\npython examples/basic_pipeline.py\npython examples/advanced_pipeline.py\npython examples/real_world_example.py\n</code></pre></p>"},{"location":"contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"contributing/#general-principles","title":"General Principles","text":"<ul> <li>Write clear, readable code</li> <li>Follow PEP 8 guidelines</li> <li>Use type hints for all function signatures</li> <li>Write descriptive docstrings</li> <li>Keep functions focused and single-purpose</li> </ul>"},{"location":"contributing/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>async def my_function(x: int, y: str = \"default\") -&gt; bool:\n    \"\"\"\n    Brief description of what the function does.\n\n    Args:\n        x: Description of x parameter\n        y: Description of y parameter with default\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: Description of when this is raised\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Always use type hints:</p> <pre><code>from typing import Any, Dict, List, Optional\n\nasync def process_items(\n    items: List[Dict[str, Any]],\n    timeout: Optional[float] = None\n) -&gt; List[Any]:\n    pass\n</code></pre>"},{"location":"contributing/#import-organization","title":"Import Organization","text":"<p>Organize imports in this order: 1. Standard library imports 2. Third-party imports 3. Local application imports</p> <pre><code>import asyncio\nimport logging\nfrom typing import Any, List\n\nfrom tenacity import retry\n\nfrom antflow.exceptions import AntFlowError\nfrom antflow.types import TaskFunc\n</code></pre>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Use <code>pytest</code> for all tests</li> <li>Mark async tests with <code>@pytest.mark.asyncio</code></li> <li>Write descriptive test names</li> <li>Test both success and failure cases</li> <li>Include docstrings in test functions</li> </ul> <p>Example:</p> <pre><code>import pytest\nfrom antflow import AsyncExecutor\n\n@pytest.mark.asyncio\nasync def test_executor_handles_task_failure():\n    \"\"\"Test that executor properly handles and propagates task failures.\"\"\"\n    async def failing_task(x):\n        raise ValueError(\"Task failed\")\n\n    async with AsyncExecutor(max_workers=2) as executor:\n        future = executor.submit(failing_task, 1)\n\n        with pytest.raises(ValueError, match=\"Task failed\"):\n            await future.result()\n</code></pre>"},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for &gt;90% code coverage</li> <li>Test edge cases and error conditions</li> <li>Include integration tests for complex workflows</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>pytest tests/ -v</code></li> <li>Run linting: <code>ruff check antflow/</code></li> <li>Run type checking: <code>mypy antflow/</code></li> <li>Ensure examples still work</li> <li>Update documentation if needed</li> <li>Add tests for new features</li> </ol>"},{"location":"contributing/#pr-guidelines","title":"PR Guidelines","text":"<ol> <li>Title: Use a clear, descriptive title</li> <li>Good: \"Add support for async iterables in Pipeline.feed_async()\"</li> <li> <p>Bad: \"Fix bug\"</p> </li> <li> <p>Description: Include:</p> </li> <li>What changes were made</li> <li>Why the changes were necessary</li> <li>How to test the changes</li> <li> <p>Any breaking changes</p> </li> <li> <p>Commits:</p> </li> <li>Write clear commit messages</li> <li>Keep commits focused and atomic</li> <li> <p>Reference issues when applicable</p> </li> <li> <p>Code Review:</p> </li> <li>Be responsive to feedback</li> <li>Make requested changes promptly</li> <li>Ask questions if feedback is unclear</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#feature-request-process","title":"Feature Request Process","text":"<ol> <li>Open an issue describing the feature</li> <li>Discuss the design and approach</li> <li>Wait for approval before implementing</li> <li>Follow the implementation guidelines below</li> </ol>"},{"location":"contributing/#implementation-guidelines","title":"Implementation Guidelines","text":"<ol> <li>Start with types: Define new types/protocols in <code>types.py</code></li> <li>Add exceptions: Define specific exceptions in <code>exceptions.py</code></li> <li>Implement core logic: Add main implementation</li> <li>Write tests: Comprehensive test coverage</li> <li>Add documentation: Update relevant docs</li> <li>Add examples: Create example demonstrating the feature</li> <li>Update API reference: Document all public APIs</li> </ol>"},{"location":"contributing/#backward-compatibility","title":"Backward Compatibility","text":"<ul> <li>Maintain backward compatibility when possible</li> <li>Deprecate features before removing them</li> <li>Document breaking changes clearly</li> <li>Provide migration guides for major changes</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#types-of-documentation","title":"Types of Documentation","text":"<ol> <li>API Documentation: <code>docs/api_reference.md</code></li> <li>User Guides: <code>docs/executor.md</code>, <code>docs/pipeline.md</code></li> <li>Examples: <code>examples/</code> directory</li> <li>README: High-level overview and quick start</li> </ol>"},{"location":"contributing/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Show both simple and advanced usage</li> <li>Keep documentation in sync with code</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include: - Python version - AntFlow version - Minimal reproducible example - Expected behavior - Actual behavior - Stack trace if applicable</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Include: - Clear description of the feature - Use cases and motivation - Proposed API (if applicable) - Examples of how it would be used</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Provide constructive feedback</li> <li>Focus on what is best for the community</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>Use GitHub Issues for bugs and features</li> <li>Use GitHub Discussions for questions</li> <li>Be patient and helpful with others</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>(For maintainers)</p> <ol> <li>Update version in <code>_version.py</code></li> <li>Update CHANGELOG.md</li> <li>Run full test suite</li> <li>Build and test package locally</li> <li>Create git tag</li> <li>Push to PyPI</li> <li>Create GitHub release</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, feel free to: - Open an issue - Start a discussion on GitHub - Reach out to maintainers</p> <p>Thank you for contributing to AntFlow! \ud83d\ude80</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the AntFlow API Reference. This documentation provides detailed information about the classes, functions, and types available in the library.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"Module Description Pipeline The heart of AntFlow. Contains <code>Pipeline</code> and <code>Stage</code> classes for building multi-step workflows. StatusTracker Observability layer. Contains <code>StatusTracker</code> and event definitions for real-time monitoring. AsyncExecutor Simple concurrent execution. Contains <code>AsyncExecutor</code> (like <code>concurrent.futures</code>) and <code>AsyncFuture</code>."},{"location":"api/#support-modules","title":"Support Modules","text":"Module Description Types Data structures and type definitions (<code>TaskEvent</code>, <code>WorkerMetrics</code>, etc.). Utils Helper functions for logging and error handling. Exceptions The exception hierarchy used throughout the library."},{"location":"api/display/","title":"Display Module API","text":"<p>The <code>antflow.display</code> module provides progress bars and dashboards for pipeline monitoring.</p>"},{"location":"api/display/#progressdisplay","title":"ProgressDisplay","text":"<p>Minimal terminal progress bar without external dependencies.</p> <pre><code>from antflow.display import ProgressDisplay\n</code></pre>"},{"location":"api/display/#constructor","title":"Constructor","text":"<pre><code>ProgressDisplay(\n    total: int,\n    width: int = 30,\n    fill_char: str = \"\u2588\",\n    empty_char: str = \"\u2591\",\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>total</code>: Total number of items to process</li> <li><code>width</code>: Width of the progress bar in characters</li> <li><code>fill_char</code>: Character for completed portion</li> <li><code>empty_char</code>: Character for remaining portion</li> </ul>"},{"location":"api/display/#methods","title":"Methods","text":"<ul> <li><code>on_start(pipeline, total_items)</code>: Initialize progress tracking</li> <li><code>on_update(snapshot)</code>: Update display with current state</li> <li><code>on_finish(results, summary)</code>: Show completion message</li> </ul>"},{"location":"api/display/#usage","title":"Usage","text":"<pre><code># Automatic via pipeline\nresults = await pipeline.run(items, progress=True)\n\n# Manual usage\nprogress = ProgressDisplay(total=100)\nprogress.on_start(pipeline, 100)\n# ... during processing ...\nprogress.on_update(snapshot)\nprogress.on_finish(results, summary)\n</code></pre>"},{"location":"api/display/#basedashboard","title":"BaseDashboard","text":"<p>Abstract base class for custom dashboards.</p> <pre><code>from antflow.display import BaseDashboard\n</code></pre>"},{"location":"api/display/#methods_1","title":"Methods","text":"<ul> <li><code>on_start(pipeline, total_items)</code>: Called when pipeline starts</li> <li><code>on_update(snapshot)</code>: Called periodically (calls <code>render()</code>)</li> <li><code>on_finish(results, summary)</code>: Called when pipeline completes</li> <li><code>render(snapshot)</code>: Abstract method - implement display logic</li> </ul>"},{"location":"api/display/#example","title":"Example","text":"<pre><code>class MyDashboard(BaseDashboard):\n    def render(self, snapshot):\n        stats = snapshot.pipeline_stats\n        print(f\"Progress: {stats.items_processed}\")\n</code></pre>"},{"location":"api/display/#compactdashboard","title":"CompactDashboard","text":"<p>Rich-based compact dashboard showing essential metrics.</p> <pre><code>from antflow.display import CompactDashboard\n</code></pre>"},{"location":"api/display/#constructor_1","title":"Constructor","text":"<pre><code>CompactDashboard(refresh_rate: float = 4.0)\n</code></pre> <p>Parameters:</p> <ul> <li><code>refresh_rate</code>: Display refresh rate in Hz</li> </ul>"},{"location":"api/display/#display","title":"Display","text":"<p>Shows a single panel with:</p> <ul> <li>Progress bar with percentage</li> <li>Current stage activity</li> <li>Processing rate and ETA</li> <li>Success/failure counts</li> </ul>"},{"location":"api/display/#usage_1","title":"Usage","text":"<pre><code>results = await pipeline.run(items, dashboard=\"compact\")\n</code></pre>"},{"location":"api/display/#detaileddashboard","title":"DetailedDashboard","text":"<p>Rich-based dashboard with stage-level metrics.</p> <pre><code>from antflow.display import DetailedDashboard\n</code></pre>"},{"location":"api/display/#constructor_2","title":"Constructor","text":"<pre><code>DetailedDashboard(refresh_rate: float = 4.0)\n</code></pre>"},{"location":"api/display/#display_1","title":"Display","text":"<p>Shows:</p> <ul> <li>Overall progress bar with rate and ETA</li> <li>Per-stage table with worker counts</li> <li>Worker performance metrics table</li> </ul>"},{"location":"api/display/#usage_2","title":"Usage","text":"<pre><code>results = await pipeline.run(items, dashboard=\"detailed\")\n</code></pre>"},{"location":"api/display/#fulldashboard","title":"FullDashboard","text":"<p>Rich-based comprehensive dashboard with full monitoring.</p> <pre><code>from antflow.display import FullDashboard\n</code></pre>"},{"location":"api/display/#constructor_3","title":"Constructor","text":"<pre><code>FullDashboard(\n    refresh_rate: float = 4.0,\n    max_items_shown: int = 20,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>refresh_rate</code>: Display refresh rate in Hz</li> <li><code>max_items_shown</code>: Maximum items to show in item tracker</li> </ul>"},{"location":"api/display/#display_2","title":"Display","text":"<p>Shows:</p> <ul> <li>Overview panel with statistics</li> <li>Stage metrics table</li> <li>Worker monitoring table</li> <li>Item tracking table (requires <code>StatusTracker</code>)</li> </ul>"},{"location":"api/display/#usage_3","title":"Usage","text":"<pre><code>tracker = StatusTracker()\npipeline = Pipeline(stages=[...], status_tracker=tracker)\nresults = await pipeline.run(items, dashboard=\"full\")\n</code></pre>"},{"location":"api/display/#dashboardprotocol","title":"DashboardProtocol","text":"<p>Protocol for custom dashboard implementations.</p> <pre><code>from antflow import DashboardProtocol\n</code></pre>"},{"location":"api/display/#methods_2","title":"Methods","text":"<pre><code>def on_start(self, pipeline: Pipeline, total_items: int) -&gt; None:\n    \"\"\"Called when pipeline execution starts.\"\"\"\n    ...\n\ndef on_update(self, snapshot: DashboardSnapshot) -&gt; None:\n    \"\"\"Called periodically with current pipeline state.\"\"\"\n    ...\n\ndef on_finish(\n    self,\n    results: List[PipelineResult],\n    summary: ErrorSummary\n) -&gt; None:\n    \"\"\"Called when pipeline execution completes.\"\"\"\n    ...\n</code></pre>"},{"location":"api/display/#usage_4","title":"Usage","text":"<pre><code>class MyDashboard:\n    def on_start(self, pipeline, total_items):\n        print(f\"Starting {total_items} items\")\n\n    def on_update(self, snapshot):\n        print(f\"Processed: {snapshot.pipeline_stats.items_processed}\")\n\n    def on_finish(self, results, summary):\n        print(f\"Done: {len(results)} results\")\n\nresults = await pipeline.run(items, custom_dashboard=MyDashboard())\n</code></pre>"},{"location":"api/display/#types","title":"Types","text":""},{"location":"api/display/#dashboardsnapshot","title":"DashboardSnapshot","text":"<pre><code>@dataclass\nclass DashboardSnapshot:\n    worker_states: Dict[str, WorkerState]\n    worker_metrics: Dict[str, WorkerMetrics]\n    pipeline_stats: PipelineStats\n    timestamp: float\n</code></pre>"},{"location":"api/display/#errorsummary","title":"ErrorSummary","text":"<pre><code>@dataclass\nclass ErrorSummary:\n    total_failed: int\n    errors_by_type: Dict[str, int]\n    errors_by_stage: Dict[str, int]\n    failed_items: List[FailedItem]\n</code></pre>"},{"location":"api/display/#faileditem","title":"FailedItem","text":"<pre><code>@dataclass\nclass FailedItem:\n    item_id: Any\n    error: str\n    error_type: str\n    stage: str\n    attempts: int\n    timestamp: float\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions API","text":"<p>The <code>antflow.exceptions</code> module defines the custom exception hierarchy used by AntFlow.</p>"},{"location":"api/exceptions/#overview","title":"Overview","text":"<p>All exceptions inherit from the base <code>AntFlowError</code>. This allows you to catch any AntFlow-related error with a single <code>except</code> block.</p>"},{"location":"api/exceptions/#exception-reference","title":"Exception Reference","text":""},{"location":"api/exceptions/#antflow.exceptions.AntFlowError","title":"AntFlowError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all antflow errors.</p>"},{"location":"api/exceptions/#antflow.exceptions.ExecutorShutdownError","title":"ExecutorShutdownError","text":"<p>               Bases: <code>AntFlowError</code></p> <p>Raised when attempting to use an executor that has been shut down.</p>"},{"location":"api/exceptions/#antflow.exceptions.PipelineError","title":"PipelineError","text":"<p>               Bases: <code>AntFlowError</code></p> <p>Base exception for pipeline-specific errors.</p>"},{"location":"api/exceptions/#antflow.exceptions.StageValidationError","title":"StageValidationError","text":"<p>               Bases: <code>PipelineError</code></p> <p>Raised when a stage configuration is invalid.</p>"},{"location":"api/exceptions/#antflow.exceptions.TaskFailedError","title":"TaskFailedError","text":"<pre><code>TaskFailedError(task_name: str, original_exception: Exception)\n</code></pre> <p>               Bases: <code>AntFlowError</code></p> <p>Wrapper for task failures that preserves the original exception.</p>"},{"location":"api/executor/","title":"AsyncExecutor API","text":"<p>The <code>antflow.executor</code> module provides a familiar interface for concurrent async execution, modeled after Python's <code>concurrent.futures</code>.</p>"},{"location":"api/executor/#overview","title":"Overview","text":"<p>The AsyncExecutor manages a pool of workers to execute async functions concurrently. It is ideal for simple parallel processing tasks where you don't need the full complexity of a multi-stage pipeline.</p> <p>Key features:</p> <ul> <li><code>submit()</code>: Schedule a single task.</li> <li><code>map()</code>: Apply a function to an iterable and return results as a list.</li> <li><code>map_iter()</code>: Apply a function to an iterable, yielding results (async iterator).</li> <li><code>as_completed()</code>: Iterate over futures as they finish.</li> <li><code>wait()</code>: Wait for a collection of futures with flexible conditions.</li> </ul>"},{"location":"api/executor/#usage-example","title":"Usage Example","text":"<pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def process_item(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\nasync def main():\n    # Use as a context manager\n    async with AsyncExecutor(max_workers=5) as executor:\n\n        # 1. Submit a single task\n        future = executor.submit(process_item, 10)\n        result = await future.result()\n        print(f\"Result: {result}\")\n\n        # 2. Map over a list - returns list directly\n        results = await executor.map(process_item, range(5))\n        print(f\"Results: {results}\")\n\n        # 3. Map with streaming (async iterator)\n        async for res in executor.map_iter(process_item, range(5)):\n            print(f\"Streamed: {res}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/executor/#class-reference","title":"Class Reference","text":""},{"location":"api/executor/#asyncfuture","title":"AsyncFuture","text":""},{"location":"api/executor/#antflow.executor.AsyncFuture","title":"AsyncFuture","text":"<pre><code>AsyncFuture(sequence_id: int)\n</code></pre> <p>An async-compatible future that holds the result of an async task. Similar to concurrent.futures.Future but for asyncio.</p>"},{"location":"api/executor/#antflow.executor.AsyncFuture.set_result","title":"set_result","text":"<pre><code>set_result(result: Any) -&gt; None\n</code></pre> <p>Set the result and mark the future as done.</p>"},{"location":"api/executor/#antflow.executor.AsyncFuture.set_exception","title":"set_exception","text":"<pre><code>set_exception(exception: Exception) -&gt; None\n</code></pre> <p>Set an exception and mark the future as done.</p>"},{"location":"api/executor/#antflow.executor.AsyncFuture.result","title":"result  <code>async</code>","text":"<pre><code>result(timeout: float | None = None) -&gt; Any\n</code></pre> <p>Wait for and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>Maximum time to wait in seconds</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The task result</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout is exceeded</p> <code>Exception</code> <p>The exception set by set_exception()</p>"},{"location":"api/executor/#antflow.executor.AsyncFuture.done","title":"done","text":"<pre><code>done() -&gt; bool\n</code></pre> <p>Return True if the future is done.</p>"},{"location":"api/executor/#antflow.executor.AsyncFuture.exception","title":"exception","text":"<pre><code>exception() -&gt; Exception | None\n</code></pre> <p>Return the exception set on this future, or None.</p>"},{"location":"api/executor/#asyncexecutor","title":"AsyncExecutor","text":""},{"location":"api/executor/#antflow.executor.AsyncExecutor","title":"AsyncExecutor","text":"<pre><code>AsyncExecutor(max_workers: int = 5)\n</code></pre> <p>An async executor with concurrent.futures-style API. Manages a pool of workers that execute async tasks concurrently.</p> <p>Initialize the executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers</p> <code>5</code>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.submit","title":"submit","text":"<pre><code>submit(fn: Callable[..., Any], *args: Any, retries: int = 0, retry_delay: float = 0.1, semaphore: Semaphore | None = None, **kwargs: Any) -&gt; AsyncFuture\n</code></pre> <p>Submit a task for execution.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>Async callable to execute</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for fn</p> <code>()</code> <code>retries</code> <code>int</code> <p>Number of retries on failure</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Delay between retries in seconds</p> <code>0.1</code> <code>semaphore</code> <code>Semaphore | None</code> <p>Optional semaphore to limit concurrency</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for fn</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncFuture</code> <p>AsyncFuture that can be awaited for the result</p> <p>Raises:</p> Type Description <code>ExecutorShutdownError</code> <p>If executor has been shut down</p>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.map","title":"map  <code>async</code>","text":"<pre><code>map(fn: Callable[[T], Any], *iterables: Iterable[T], timeout: float | None = None, retries: int = 0, retry_delay: float = 0.1) -&gt; List[R]\n</code></pre> <p>Map an async function over iterables and return results as a list.</p> <p>Similar to <code>concurrent.futures.Executor.map()</code>, but returns a list directly instead of an iterator (since <code>list(executor.map(...))</code> is the common pattern).</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[T], Any]</code> <p>Async callable to map</p> required <code>*iterables</code> <code>Iterable[T]</code> <p>Iterables to map over</p> <code>()</code> <code>timeout</code> <code>float | None</code> <p>Maximum time to wait for each result</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of retries on failure</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Delay between retries in seconds</p> <code>0.1</code> <p>Returns:</p> Type Description <code>List[R]</code> <p>List of results from fn applied to each input, in input order</p> <p>Raises:</p> Type Description <code>ExecutorShutdownError</code> <p>If executor has been shut down</p> Example <pre><code>async with AsyncExecutor(max_workers=5) as executor:\n    results = await executor.map(process, range(100))\n    print(results)  # [0, 2, 4, 6, ...]\n</code></pre>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.map_iter","title":"map_iter  <code>async</code>","text":"<pre><code>map_iter(fn: Callable[[T], Any], *iterables: Iterable[T], timeout: float | None = None, retries: int = 0, retry_delay: float = 0.1) -&gt; AsyncIterator[R]\n</code></pre> <p>Map an async function over iterables, yielding results in input order.</p> <p>Use this instead of <code>map()</code> when you need streaming behavior: - Process results as they arrive - Handle large datasets without loading all results into memory - Early exit when a condition is met</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[T], Any]</code> <p>Async callable to map</p> required <code>*iterables</code> <code>Iterable[T]</code> <p>Iterables to map over</p> <code>()</code> <code>timeout</code> <code>float | None</code> <p>Maximum time to wait for each result</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of retries on failure</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Delay between retries in seconds</p> <code>0.1</code> <p>Yields:</p> Type Description <code>AsyncIterator[R]</code> <p>Results from fn applied to each input</p> <p>Raises:</p> Type Description <code>ExecutorShutdownError</code> <p>If executor has been shut down</p> Example <pre><code>async with AsyncExecutor(max_workers=5) as executor:\n    async for result in executor.map_iter(process, range(100)):\n        print(result)\n        if result &gt; 50:\n            break  # Early exit\n</code></pre>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.as_completed","title":"as_completed  <code>async</code>","text":"<pre><code>as_completed(futures: list[AsyncFuture], timeout: float | None = None) -&gt; AsyncIterator[AsyncFuture]\n</code></pre> <p>Yield futures as they complete.</p> <p>Parameters:</p> Name Type Description Default <code>futures</code> <code>list[AsyncFuture]</code> <p>List of futures to wait for</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time to wait for all futures</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncIterator[AsyncFuture]</code> <p>Futures as they complete</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout is exceeded</p>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.wait","title":"wait  <code>async</code>","text":"<pre><code>wait(futures: Iterable[AsyncFuture[R]], timeout: float | None = None, return_when: WaitStrategy = WaitStrategy.ALL_COMPLETED) -&gt; Tuple[Set[AsyncFuture[R]], Set[AsyncFuture[R]]]\n</code></pre> <p>Wait for futures to complete with different strategies.</p> <p>Similar to concurrent.futures.wait() but for async operations.</p> <p>Parameters:</p> Name Type Description Default <code>futures</code> <code>Iterable[AsyncFuture[R]]</code> <p>Iterable of AsyncFuture objects to wait for</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time to wait in seconds</p> <code>None</code> <code>return_when</code> <code>WaitStrategy</code> <p>Strategy for when to return: - FIRST_COMPLETED: Return when any future completes - FIRST_EXCEPTION: Return when any future raises an exception - ALL_COMPLETED: Return when all futures complete (default)</p> <code>ALL_COMPLETED</code> <p>Returns:</p> Type Description <code>Tuple[Set[AsyncFuture[R]], Set[AsyncFuture[R]]]</code> <p>Tuple of (done, not_done) sets of futures</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout is exceeded (with ALL_COMPLETED)</p> Example <pre><code>futures = [executor.submit(task, i) for i in range(10)]\ndone, pending = await executor.wait(\n    futures,\n    return_when=WaitStrategy.FIRST_EXCEPTION\n)\n</code></pre>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown(wait: bool = True, cancel_futures: bool = False) -&gt; None\n</code></pre> <p>Shut down the executor.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>If True, wait for all pending tasks to complete</p> <code>True</code> <code>cancel_futures</code> <code>bool</code> <p>If True, cancel all pending futures</p> <code>False</code>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; 'AsyncExecutor'\n</code></pre> <p>Context manager entry.</p>"},{"location":"api/executor/#antflow.executor.AsyncExecutor.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Context manager exit with automatic shutdown.</p>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>The <code>antflow.pipeline</code> module provides the core functionality for building and running multi-stage asynchronous workflows.</p>"},{"location":"api/pipeline/#overview","title":"Overview","text":"<p>A Pipeline consists of a sequence of Stages. Each stage has its own pool of workers and processes items independently. Data flows from one stage to the next automatically.</p> <p>Key components:</p> <ul> <li>Pipeline: The main orchestrator that manages stages and data flow.</li> <li>Stage: Configuration for a single processing step, including worker count and retry logic.</li> </ul>"},{"location":"api/pipeline/#usage-example","title":"Usage Example","text":"<pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def fetch_data(url):\n    # ... fetch logic ...\n    return data\n\nasync def process_data(data):\n    # ... process logic ...\n    return result\n\nasync def main():\n    # Define stages\n    stage1 = Stage(name=\"Fetch\", workers=5, tasks=[fetch_data])\n    stage2 = Stage(name=\"Process\", workers=2, tasks=[process_data])\n\n    # Create pipeline\n    pipeline = Pipeline(stages=[stage1, stage2])\n\n    # Run\n    urls = [\"http://example.com/1\", \"http://example.com/2\"]\n    results = await pipeline.run(urls)\n\n    for result in results:\n        print(f\"ID: {result.id}, Value: {result.value}\")\n</code></pre>"},{"location":"api/pipeline/#class-reference","title":"Class Reference","text":""},{"location":"api/pipeline/#stage","title":"Stage","text":""},{"location":"api/pipeline/#antflow.pipeline.Stage","title":"Stage  <code>dataclass</code>","text":"<pre><code>Stage(name: str, workers: int, tasks: Sequence[TaskFunc], retry: str = 'per_task', task_attempts: int = 3, task_wait_seconds: float = 1.0, stage_attempts: int = 3, unpack_args: bool = False, task_concurrency_limits: Dict[str, int] = dict(), on_success: Optional[Callable[[Any, Any, Dict[str, Any]], Any]] = None, on_failure: Optional[Callable[[Any, Exception, Dict[str, Any]], Any]] = None, skip_if: Optional[Callable[[Any], bool]] = None, queue_capacity: Optional[int] = None)\n</code></pre> <p>A stage in the pipeline that processes items through a sequence of tasks.</p>"},{"location":"api/pipeline/#antflow.pipeline.Stage.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate stage configuration.</p> <p>Raises:</p> Type Description <code>StageValidationError</code> <p>If configuration is invalid</p>"},{"location":"api/pipeline/#pipeline","title":"Pipeline","text":""},{"location":"api/pipeline/#antflow.pipeline.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(stages: List[Stage], collect_results: bool = True, status_tracker: Optional[StatusTracker] = None)\n</code></pre> <p>A multi-stage async pipeline with worker pools and flexible retry strategies.</p> <p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>List[Stage]</code> <p>List of Stage objects defining the pipeline</p> required <code>collect_results</code> <code>bool</code> <p>If True, collects results from the final stage into <code>self.results</code>. If False, results are discarded after processing (useful for fire-and-forget or side-effect only pipelines). Defaults to True.</p> <code>True</code> <code>status_tracker</code> <code>Optional[StatusTracker]</code> <p>Optional StatusTracker for monitoring item status</p> <code>None</code> <p>Raises:</p> Type Description <code>PipelineError</code> <p>If stages list is empty</p> <code>StageValidationError</code> <p>If any stage configuration is invalid</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.results","title":"results  <code>property</code>","text":"<pre><code>results: List[PipelineResult]\n</code></pre> <p>Get collected results, sorted by original input sequence.</p> <p>Only contains data if <code>collect_results=True</code> was passed to <code>__init__</code>.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.create","title":"create  <code>classmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Create a pipeline using the fluent builder API.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder for chaining</p> Example <pre><code>results = await (\n    Pipeline.create()\n    .add(\"Fetch\", fetch, workers=10)\n    .add(\"Process\", process, workers=5)\n    .run(items, progress=True)\n)\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.quick","title":"quick  <code>async</code> <code>classmethod</code>","text":"<pre><code>quick(items: Sequence[Any], tasks: Union[TaskFunc, List[TaskFunc]], workers: int = 5, retries: int = 3, progress: bool = False, dashboard: Optional[Literal['compact', 'detailed', 'full']] = None) -&gt; List[PipelineResult]\n</code></pre> <p>One-liner pipeline for simple use cases.</p> <p>Creates a single-stage pipeline if one task is provided, or a multi-stage pipeline with one stage per task if a list is provided.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Sequence[Any]</code> <p>Items to process</p> required <code>tasks</code> <code>Union[TaskFunc, List[TaskFunc]]</code> <p>Single task function or list of task functions</p> required <code>workers</code> <code>int</code> <p>Number of workers per stage</p> <code>5</code> <code>retries</code> <code>int</code> <p>Number of retry attempts per task</p> <code>3</code> <code>progress</code> <code>bool</code> <p>Show progress bar</p> <code>False</code> <code>dashboard</code> <code>Optional[Literal['compact', 'detailed', 'full']]</code> <p>Dashboard type (\"compact\", \"detailed\", \"full\")</p> <code>None</code> <p>Returns:</p> Type Description <code>List[PipelineResult]</code> <p>List of PipelineResult objects</p> Example <pre><code># Single task\nresults = await Pipeline.quick(items, process, workers=10)\n\n# Multiple tasks (one stage per task)\nresults = await Pipeline.quick(\n    items,\n    [fetch, process, save],\n    workers=5,\n    progress=True\n)\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; PipelineStats\n</code></pre> <p>Get current pipeline statistics.</p> <p>Returns:</p> Type Description <code>PipelineStats</code> <p>PipelineStats with current metrics</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_worker_names","title":"get_worker_names","text":"<pre><code>get_worker_names() -&gt; Dict[str, List[str]]\n</code></pre> <p>Get all worker names organized by stage.</p> <p>Useful for tracking which workers exist before pipeline runs.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dictionary mapping stage name to list of worker names</p> <p>Example:</p> <pre><code>```python\npipeline = Pipeline(stages=[\n    Stage(name=\"Fetch\", workers=3, tasks=[fetch]),\n    Stage(name=\"Process\", workers=2, tasks=[process])\n])\npipeline.get_worker_names()\n# {\n#     \"Fetch\": [\"Fetch-W0\", \"Fetch-W1\", \"Fetch-W2\"],\n#     \"Process\": [\"Process-W0\", \"Process-W1\"]\n# }\n```\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_worker_states","title":"get_worker_states","text":"<pre><code>get_worker_states() -&gt; Dict[str, WorkerState]\n</code></pre> <p>Get current state of all workers.</p> <p>Returns:</p> Type Description <code>Dict[str, WorkerState]</code> <p>Dictionary mapping worker name to WorkerState</p> Example <pre><code>states = pipeline.get_worker_states()\nfor name, state in states.items():\n    if state.status == \"busy\":\n        print(f\"{name} processing item {state.current_item_id}\")\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_worker_metrics","title":"get_worker_metrics","text":"<pre><code>get_worker_metrics() -&gt; Dict[str, WorkerMetrics]\n</code></pre> <p>Get performance metrics for all workers.</p> <p>Returns:</p> Type Description <code>Dict[str, WorkerMetrics]</code> <p>Dictionary mapping worker name to WorkerMetrics</p> Example <pre><code>metrics = pipeline.get_worker_metrics()\nfor name, metric in metrics.items():\n    print(f\"{name}: {metric.items_processed} items, \"\n          f\"avg {metric.avg_processing_time:.2f}s\")\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_dashboard_snapshot","title":"get_dashboard_snapshot","text":"<pre><code>get_dashboard_snapshot() -&gt; DashboardSnapshot\n</code></pre> <p>Get complete dashboard snapshot with all current state.</p> <p>Returns:</p> Type Description <code>DashboardSnapshot</code> <p>DashboardSnapshot with worker states, metrics, and pipeline stats</p> Example <pre><code>snapshot = pipeline.get_dashboard_snapshot()\nprint(f\"Active workers: {sum(1 for s in snapshot.worker_states.values() if s.status == 'busy')}\")\nprint(f\"Items processed: {snapshot.pipeline_stats.items_processed}\")\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.get_error_summary","title":"get_error_summary","text":"<pre><code>get_error_summary() -&gt; ErrorSummary\n</code></pre> <p>Get aggregated error information from pipeline execution.</p> <p>If a StatusTracker is configured, returns detailed error information. Otherwise, returns a summary based on pipeline-level counts.</p> <p>Returns:</p> Type Description <code>ErrorSummary</code> <p>ErrorSummary with failure statistics</p> Example <pre><code>summary = pipeline.get_error_summary()\nprint(f\"Total failed: {summary.total_failed}\")\nfor error_type, count in summary.errors_by_type.items():\n    print(f\"  {error_type}: {count}\")\n</code></pre>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.feed","title":"feed  <code>async</code>","text":"<pre><code>feed(items: Sequence[Any], target_stage: Optional[str] = None, priority: int = 100) -&gt; None\n</code></pre> <p>Feed items into a specific stage of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Sequence[Any]</code> <p>Sequence of items to process</p> required <code>target_stage</code> <code>Optional[str]</code> <p>Name of the stage to inject items into.          If None, feeds into the first stage.</p> <code>None</code> <code>priority</code> <code>int</code> <p>Priority level (lower = higher priority). Default 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target_stage is provided but not found.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.feed_async","title":"feed_async  <code>async</code>","text":"<pre><code>feed_async(items: AsyncIterable[Any], target_stage: Optional[str] = None, priority: int = 100) -&gt; None\n</code></pre> <p>Feed items from an async iterable into a specific stage.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>AsyncIterable[Any]</code> <p>Async iterable of items to process</p> required <code>target_stage</code> <code>Optional[str]</code> <p>Name of the stage to inject items into.          If None, feeds into the first stage.</p> <code>None</code> <code>priority</code> <code>int</code> <p>Priority level (lower = higher priority). Default 100.</p> <code>100</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target_stage is provided but not found.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.start","title":"start  <code>async</code>","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the pipeline workers in the background.</p> <p>This method initializes the worker pool and starts processing items immediately as they are available in the queues. Use <code>feed()</code> to add items.</p> <p>Raises:</p> Type Description <code>PipelineError</code> <p>If pipeline is already running</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.join","title":"join  <code>async</code>","text":"<pre><code>join() -&gt; None\n</code></pre> <p>Wait for all enqueued items to be processed and stop workers.</p> <p>This method: 1. Waits for all queues to be empty (all items processed) 2. Signals workers to stop 3. Waits for the worker pool to shutdown</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.run","title":"run  <code>async</code>","text":"<pre><code>run(items: Sequence[Any], progress: bool = False, dashboard: Optional[Literal['compact', 'detailed', 'full']] = None, custom_dashboard: Optional[DashboardProtocol] = None, dashboard_update_interval: float = 0.5) -&gt; List[PipelineResult]\n</code></pre> <p>Run the pipeline end-to-end with the given items.</p> <p>This is a convenience wrapper around <code>start()</code>, <code>feed()</code>, and <code>join()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Sequence[Any]</code> <p>Items to process through the pipeline.</p> required <code>progress</code> <code>bool</code> <p>Show minimal progress bar (mutually exclusive with dashboard).</p> <code>False</code> <code>dashboard</code> <code>Optional[Literal['compact', 'detailed', 'full']]</code> <p>Built-in dashboard type: \"compact\", \"detailed\", or \"full\".</p> <code>None</code> <code>custom_dashboard</code> <code>Optional[DashboardProtocol]</code> <p>User-provided dashboard implementing DashboardProtocol.</p> <code>None</code> <code>dashboard_update_interval</code> <code>float</code> <p>How often to update the dashboard in seconds (default: 0.5). Only applies to DashboardProtocol-based displays (progress, dashboard, custom_dashboard). Lower values = more frequent updates but higher CPU usage. Recommended range: 0.1 to 1.0 seconds.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>List[PipelineResult]</code> <p>List of PipelineResult objects.</p> Example <pre><code># Simple progress bar\nresults = await pipeline.run(items, progress=True)\n\n# Compact dashboard with faster updates\nresults = await pipeline.run(items, dashboard=\"compact\", dashboard_update_interval=0.2)\n\n# Custom dashboard with slower updates (lower CPU usage)\nresults = await pipeline.run(items, custom_dashboard=MyDashboard(), dashboard_update_interval=1.0)\n</code></pre> Note <p>The dashboard update mechanism uses polling: a background task calls <code>display.on_update(snapshot)</code> every <code>dashboard_update_interval</code> seconds. This is efficient because: - Snapshots are lightweight (just reading current state) - No events are generated - we only read existing data - The interval is configurable to balance responsiveness vs CPU usage</p> <p>For event-driven monitoring without polling, use <code>StatusTracker</code> callbacks instead.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.stream","title":"stream  <code>async</code>","text":"<pre><code>stream(items: Sequence[Any], progress: bool = False) -&gt; AsyncIterator[PipelineResult]\n</code></pre> <p>Stream results as they complete.</p> <p>Unlike run() which returns all results at once, stream() yields results as soon as they complete. Results are yielded in completion order, not input order.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Sequence[Any]</code> <p>Items to process through the pipeline</p> required <code>progress</code> <code>bool</code> <p>Show minimal progress bar</p> <code>False</code> <p>Yields:</p> Type Description <code>AsyncIterator[PipelineResult]</code> <p>PipelineResult objects as they complete</p> Example <pre><code>async for result in pipeline.stream(items):\n    print(f\"Got result: {result.value}\")\n    if some_condition:\n        break  # Early exit supported\n</code></pre> Note <ul> <li>Results are yielded in completion order, not input order</li> <li>Use run() if you need results in input order</li> <li>Early exit (break) is supported and will shutdown the pipeline</li> <li>stream() uses an internal queue and does NOT modify <code>collect_results</code></li> <li>When streaming, results are NOT stored in <code>_results</code> regardless of   <code>collect_results</code> setting</li> <li>After streaming completes, <code>_results</code> remains empty (preserves original   <code>collect_results</code> setting)</li> </ul>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shut down the pipeline gracefully or forcefully.</p> <p>If queues are not empty, this might leave items unprocessed depending on how workers react to stop_event.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; 'Pipeline'\n</code></pre> <p>Context manager entry.</p>"},{"location":"api/pipeline/#antflow.pipeline.Pipeline.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Context manager exit with automatic shutdown.</p>"},{"location":"api/tracker/","title":"StatusTracker API","text":"<p>The <code>antflow.tracker</code> module provides real-time monitoring and event tracking for pipelines.</p>"},{"location":"api/tracker/#overview","title":"Overview","text":"<p>The StatusTracker is the observability layer of AntFlow. It allows you to: 1.  Monitor Items: Track the lifecycle of data items as they move through stages. 2.  Monitor Tasks: Get granular events for every task execution (start, success, retry, failure). 3.  Build Dashboards: Use the event stream to power real-time UIs.</p>"},{"location":"api/tracker/#usage-example","title":"Usage Example","text":"<pre><code>from antflow import Pipeline, Stage, StatusTracker\n\n# 1. Define a custom handler\nasync def on_event(event):\n    if event.status == \"failed\":\n        print(f\"\ud83d\udea8 Alert: Item {event.item_id} failed in {event.stage}\")\n\n# 2. Initialize tracker\ntracker = StatusTracker(on_status_change=on_event)\n\n# 3. Attach to pipeline\npipeline = Pipeline(stages=[...], status_tracker=tracker)\n\n# 4. Run\nawait pipeline.run(items)\n</code></pre>"},{"location":"api/tracker/#customizing-behavior","title":"Customizing Behavior","text":"<p>You can customize how status changes are handled by providing callbacks or subclassing <code>StatusTracker</code>.</p>"},{"location":"api/tracker/#using-callbacks","title":"Using Callbacks","text":"<p>The easiest way to react to events is by passing async callbacks during initialization:</p> <pre><code>async def on_fail(event: TaskEvent):\n    # Send alert to Slack/Discord\n    await send_alert(f\"Item {event.item_id} failed: {event.error}\")\n\ntracker = StatusTracker(\n    on_task_fail=on_fail,\n    on_status_change=lambda e: print(f\"Status: {e.status}\")\n)\n</code></pre>"},{"location":"api/tracker/#accessing-worker-job-status","title":"Accessing Worker &amp; Job Status","text":"<p>The <code>StatusTracker</code> focuses on Item progress. To monitor Workers, use the <code>Pipeline</code> methods:</p> <pre><code># Item Status (via Tracker)\nitem_status = tracker.get_status(item_id=123)\n\n# Worker Status (via Pipeline)\nworker_states = pipeline.get_worker_states()\nfor name, state in worker_states.items():\n    print(f\"Worker {name}: {state.status}\")\n</code></pre>"},{"location":"api/tracker/#available-callbacks","title":"Available Callbacks","text":"<p>You can provide these async callbacks to the <code>StatusTracker</code> constructor to react to specific events:</p> Callback Signature Description <code>on_status_change</code> <code>async def fn(event: StatusEvent)</code> Triggered when an item's status changes (queued, in_progress, completed, failed). <code>on_task_start</code> <code>async def fn(event: TaskEvent)</code> Triggered when a specific task function starts execution. <code>on_task_complete</code> <code>async def fn(event: TaskEvent)</code> Triggered when a task function completes successfully. <code>on_task_retry</code> <code>async def fn(event: TaskEvent)</code> Triggered when a task fails but will be retried. <code>on_task_fail</code> <code>async def fn(event: TaskEvent)</code> Triggered when a task fails permanently (after all retries)."},{"location":"api/tracker/#event-objects","title":"Event Objects","text":"<p>For detailed properties of <code>StatusEvent</code> and <code>TaskEvent</code>, please refer to the Types API documentation.</p>"},{"location":"api/tracker/#event-types","title":"Event Types","text":"<p>For quick reference, here are the possible values for status and event types:</p>"},{"location":"api/tracker/#item-status-statuseventstatus","title":"Item Status (<code>StatusEvent.status</code>)","text":"Value Description <code>queued</code> Item has been added to a stage's input queue. <code>in_progress</code> Worker has picked up the item and started processing. <code>completed</code> All tasks in the stage finished successfully. <code>failed</code> Stage execution failed (after all retries). <code>retrying</code> Item failed processing and is waiting to be retried (per-stage retry). Metadata includes <code>attempt</code> and <code>error</code>."},{"location":"api/tracker/#task-events-taskeventevent_type","title":"Task Events (<code>TaskEvent.event_type</code>)","text":"Value Description <code>start</code> A specific task function started running. <code>complete</code> Task function returned successfully. <code>retry</code> Task failed but has retries remaining. <code>fail</code> Task failed and has no retries left."},{"location":"api/tracker/#class-reference","title":"Class Reference","text":""},{"location":"api/tracker/#statustracker","title":"StatusTracker","text":""},{"location":"api/tracker/#antflow.tracker.StatusTracker","title":"StatusTracker","text":"<pre><code>StatusTracker(on_status_change: Optional[Callable[[StatusEvent], Awaitable[None]]] = None, on_task_start: Optional[Callable[[TaskEvent], Awaitable[None]]] = None, on_task_complete: Optional[Callable[[TaskEvent], Awaitable[None]]] = None, on_task_retry: Optional[Callable[[TaskEvent], Awaitable[None]]] = None, on_task_fail: Optional[Callable[[TaskEvent], Awaitable[None]]] = None)\n</code></pre> <p>Tracks status changes for items flowing through a pipeline.</p> <p>Provides methods to query current status, filter by status, get statistics, and retrieve event history.</p> Example <pre><code>tracker = StatusTracker()\n\nasync def on_change(event: StatusEvent):\n    print(f\"Item {event.item_id}: {event.status}\")\n\ntracker.on_status_change = on_change\npipeline = Pipeline(stages=[...], status_tracker=tracker)\n\nresults = await pipeline.run(items)\nprint(tracker.get_stats())\n</code></pre> <p>Initialize the status tracker.</p> <p>Parameters:</p> Name Type Description Default <code>on_status_change</code> <code>Optional[Callable[[StatusEvent], Awaitable[None]]]</code> <p>Optional callback invoked on each item StatusEvent</p> <code>None</code> <code>on_task_start</code> <code>Optional[Callable[[TaskEvent], Awaitable[None]]]</code> <p>Optional callback when a task starts executing (TaskEvent)</p> <code>None</code> <code>on_task_complete</code> <code>Optional[Callable[[TaskEvent], Awaitable[None]]]</code> <p>Optional callback when a task completes successfully (TaskEvent)</p> <code>None</code> <code>on_task_retry</code> <code>Optional[Callable[[TaskEvent], Awaitable[None]]]</code> <p>Optional callback when a task is retrying after failure (TaskEvent)</p> <code>None</code> <code>on_task_fail</code> <code>Optional[Callable[[TaskEvent], Awaitable[None]]]</code> <p>Optional callback when a task fails after all retries (TaskEvent)</p> <code>None</code>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_status","title":"get_status","text":"<pre><code>get_status(item_id: Any) -&gt; StatusEvent | None\n</code></pre> <p>Get the current status of an item.</p> <p>Parameters:</p> Name Type Description Default <code>item_id</code> <code>Any</code> <p>The item identifier</p> required <p>Returns:</p> Type Description <code>StatusEvent | None</code> <p>The most recent StatusEvent for the item, or None if not found</p>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_by_status","title":"get_by_status","text":"<pre><code>get_by_status(status: StatusType) -&gt; List[StatusEvent]\n</code></pre> <p>Get all items currently in a given status.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusType</code> <p>The status to filter by</p> required <p>Returns:</p> Type Description <code>List[StatusEvent]</code> <p>List of StatusEvents for items with the given status</p>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; Dict[str, int]\n</code></pre> <p>Get aggregate statistics by status.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary mapping status names to counts</p>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_history","title":"get_history","text":"<pre><code>get_history(item_id: Any) -&gt; List[StatusEvent]\n</code></pre> <p>Get the full event history for an item.</p> <p>Parameters:</p> Name Type Description Default <code>item_id</code> <code>Any</code> <p>The item identifier</p> required <p>Returns:</p> Type Description <code>List[StatusEvent]</code> <p>List of all StatusEvents for the item, in chronological order</p>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_failed_items","title":"get_failed_items","text":"<pre><code>get_failed_items() -&gt; List[FailedItem]\n</code></pre> <p>Get details of all failed items.</p> <p>Returns:</p> Type Description <code>List[FailedItem]</code> <p>List of FailedItem with failure details</p>"},{"location":"api/tracker/#antflow.tracker.StatusTracker.get_error_summary","title":"get_error_summary","text":"<pre><code>get_error_summary() -&gt; ErrorSummary\n</code></pre> <p>Get aggregated error information.</p> <p>Returns:</p> Type Description <code>ErrorSummary</code> <p>ErrorSummary with failure statistics and details</p> Example <pre><code>summary = tracker.get_error_summary()\nprint(f\"Total failed: {summary.total_failed}\")\nfor error_type, count in summary.errors_by_type.items():\n    print(f\"  {error_type}: {count}\")\n</code></pre>"},{"location":"api/types/","title":"Types API","text":"<p>The <code>antflow.types</code> module defines the core data structures used for type hinting and data exchange between components.</p>"},{"location":"api/types/#overview","title":"Overview","text":"<p>This module contains Data Classes that represent the state of the system, including events, metrics, and snapshots.</p>"},{"location":"api/types/#enumerated-types","title":"Enumerated Types","text":""},{"location":"api/types/#statustype","title":"StatusType","text":"<p>Represents the current state of an item in the pipeline.</p> Value Description <code>queued</code> Item is waiting in a stage's input queue. <code>in_progress</code> Item is currently being processed by a worker. <code>completed</code> Item has successfully finished processing in the stage. <code>failed</code> Item failed processing (after exhausting retries). <code>retrying</code> Item failed but is queued for a retry (per-stage retry strategy)."},{"location":"api/types/#workerstatus","title":"WorkerStatus","text":"<p>Represents the current activity state of a worker.</p> Value Description <code>idle</code> Worker is waiting for new items. <code>busy</code> Worker is currently processing an item."},{"location":"api/types/#taskeventtype","title":"TaskEventType","text":"<p>Represents the type of event occurring at the task level.</p> Value Description <code>start</code> A task function has started execution. <code>complete</code> A task function has completed successfully. <code>retry</code> A task failed and is being retried. <code>fail</code> A task failed permanently (after exhausting retries)."},{"location":"api/types/#workerstate","title":"WorkerState","text":"Field Type Description <code>status</code> <code>WorkerStatus</code> <code>idle</code> or <code>busy</code>. <code>current_item_id</code> <code>Optional[Any]</code> ID of the item currently being processed. <code>current_task</code> <code>Optional[str]</code> Name of the task function currently being executed. <code>processing_since</code> <code>Optional[float]</code> Timestamp when the current item started. <code>stage</code> <code>str</code> Name of the stage the worker belongs to."},{"location":"api/types/#data-classes","title":"Data Classes","text":""},{"location":"api/types/#antflow.types.PipelineResult","title":"PipelineResult  <code>dataclass</code>","text":"<pre><code>PipelineResult(id: Any, value: Any, sequence_id: int, metadata: Dict[str, Any] = dict(), error: Optional[Exception] = None)\n</code></pre> <p>Result of a pipeline execution for a single item.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Any</code> <p>Unique identifier of the item</p> <code>value</code> <code>Any</code> <p>The final processed value</p> <code>sequence_id</code> <code>int</code> <p>Internal sequence number for ordering</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata from the input item</p> <code>error</code> <code>Optional[Exception]</code> <p>Exception if processing failed (usually None for successful results)</p>"},{"location":"api/types/#antflow.types.PipelineResult.is_success","title":"is_success  <code>property</code>","text":"<pre><code>is_success: bool\n</code></pre> <p>Check if processing was successful.</p>"},{"location":"api/types/#antflow.types.StageStats","title":"StageStats  <code>dataclass</code>","text":"<pre><code>StageStats(stage_name: str, pending_items: int, in_progress_items: int, completed_items: int, failed_items: int)\n</code></pre> <p>Statistics for a single stage.</p> <p>Attributes:</p> Name Type Description <code>stage_name</code> <code>str</code> <p>Name of the stage</p> <code>pending_items</code> <code>int</code> <p>Number of items in the queue</p> <code>in_progress_items</code> <code>int</code> <p>Number of items currently being processed (busy workers)</p> <code>completed_items</code> <code>int</code> <p>Total items successfully processed by this stage</p> <code>failed_items</code> <code>int</code> <p>Total items failed in this stage</p>"},{"location":"api/types/#antflow.types.PipelineStats","title":"PipelineStats  <code>dataclass</code>","text":"<pre><code>PipelineStats(items_processed: int, items_failed: int, items_in_flight: int, queue_sizes: Dict[str, int], stage_stats: Dict[str, StageStats] = dict())\n</code></pre> <p>Aggregate statistics for the pipeline.</p> <p>Attributes:</p> Name Type Description <code>items_processed</code> <code>int</code> <p>Total number of items successfully processed (final output)</p> <code>items_failed</code> <code>int</code> <p>Total number of items that failed</p> <code>items_in_flight</code> <code>int</code> <p>Number of items currently being processed anywhere</p> <code>queue_sizes</code> <code>Dict[str, int]</code> <p>Dictionary mapping stage names to current queue sizes</p> <code>stage_stats</code> <code>Dict[str, StageStats]</code> <p>Detailed statistics per stage</p>"},{"location":"api/types/#antflow.types.WorkerState","title":"WorkerState  <code>dataclass</code>","text":"<pre><code>WorkerState(worker_name: str, stage: str, status: WorkerStatus, current_item_id: Optional[Any] = None, current_task: Optional[str] = None, processing_since: Optional[float] = None)\n</code></pre> <p>Current state of a worker.</p> <p>Attributes:</p> Name Type Description <code>worker_name</code> <code>str</code> <p>Unique name of the worker (e.g., 'Fetch-W0')</p> <code>stage</code> <code>str</code> <p>Name of the stage this worker belongs to</p> <code>status</code> <code>WorkerStatus</code> <p>Current status ('idle' or 'busy')</p> <code>current_item_id</code> <code>Optional[Any]</code> <p>ID of the item currently being processed (if busy)</p> <code>processing_since</code> <code>Optional[float]</code> <p>Timestamp when current processing started</p>"},{"location":"api/types/#antflow.types.WorkerMetrics","title":"WorkerMetrics  <code>dataclass</code>","text":"<pre><code>WorkerMetrics(worker_name: str, stage: str, items_processed: int = 0, items_failed: int = 0, total_processing_time: float = 0.0, last_active: Optional[float] = None)\n</code></pre> <p>Performance metrics for a single worker.</p> <p>Attributes:</p> Name Type Description <code>worker_name</code> <code>str</code> <p>Unique name of the worker</p> <code>stage</code> <code>str</code> <p>Name of the stage</p> <code>items_processed</code> <code>int</code> <p>Count of successfully processed items</p> <code>items_failed</code> <code>int</code> <p>Count of failed items</p> <code>total_processing_time</code> <code>float</code> <p>Cumulative processing time in seconds</p> <code>last_active</code> <code>Optional[float]</code> <p>Timestamp of last activity</p>"},{"location":"api/types/#antflow.types.WorkerMetrics.avg_processing_time","title":"avg_processing_time  <code>property</code>","text":"<pre><code>avg_processing_time: float\n</code></pre> <p>Calculate average processing time per item.</p>"},{"location":"api/types/#antflow.types.TaskEvent","title":"TaskEvent  <code>dataclass</code>","text":"<pre><code>TaskEvent(item_id: Any, stage: str, task_name: str, worker: str, event_type: TaskEventType, attempt: int, timestamp: float, error: Optional[Exception] = None, duration: Optional[float] = None)\n</code></pre> <p>Event emitted for task-level operations within a stage.</p> <p>Provides granular visibility into individual task execution, including retries and failures at the task level.</p> <p>Attributes:</p> Name Type Description <code>item_id</code> <code>Any</code> <p>Item being processed</p> <code>stage</code> <code>str</code> <p>Stage name</p> <code>task_name</code> <code>str</code> <p>Name of the specific task function</p> <code>worker</code> <code>str</code> <p>Worker name processing the task</p> <code>event_type</code> <code>TaskEventType</code> <p>Type of event ([TaskEventType][antflow.types.TaskEventType])</p> <code>attempt</code> <code>int</code> <p>Current attempt number (1-indexed)</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp when event occurred</p> <code>error</code> <code>Optional[Exception]</code> <p>Exception if task failed or is retrying (None otherwise)</p> <code>duration</code> <code>Optional[float]</code> <p>Time taken to execute task in seconds (None for start events)</p>"},{"location":"api/types/#antflow.types.DashboardSnapshot","title":"DashboardSnapshot  <code>dataclass</code>","text":"<pre><code>DashboardSnapshot(worker_states: Dict[str, WorkerState], worker_metrics: Dict[str, WorkerMetrics], pipeline_stats: PipelineStats, error_summary: ErrorSummary, timestamp: float)\n</code></pre> <p>Snapshot of the entire pipeline state for monitoring.</p> <p>Attributes:</p> Name Type Description <code>worker_states</code> <code>Dict[str, WorkerState]</code> <p>Dictionary of all WorkerState</p> <code>worker_metrics</code> <code>Dict[str, WorkerMetrics]</code> <p>Dictionary of all WorkerMetrics</p> <code>pipeline_stats</code> <code>PipelineStats</code> <p>Aggregate PipelineStats</p> <code>timestamp</code> <code>float</code> <p>Timestamp when snapshot was taken</p>"},{"location":"api/types/#antflow.types.StatusEvent","title":"StatusEvent  <code>dataclass</code>","text":"<pre><code>StatusEvent(item_id: Any, stage: str | None, status: StatusType, worker: str | None, timestamp: float, metadata: Dict[str, Any] = dict())\n</code></pre> <p>Represents a status change event for an item in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>item_id</code> <code>Any</code> <p>Unique identifier for the item</p> <code>stage</code> <code>str | None</code> <p>Name of the stage (None if not stage-specific)</p> <code>status</code> <code>StatusType</code> <p>Current status of the item</p> <code>worker</code> <code>str | None</code> <p>Name of the worker processing the item (if applicable)</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp when the event occurred</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata about the event</p>"},{"location":"api/types/#antflow.types.StatusEvent.worker_id","title":"worker_id  <code>property</code>","text":"<pre><code>worker_id: int | None\n</code></pre> <p>Extract worker ID from worker name.</p> <p>Examples:</p> <p>\"ProcessBatch-W5\" -&gt; 5 \"Fetch-W0\" -&gt; 0 None -&gt; None</p> <p>Returns:</p> Type Description <code>int | None</code> <p>Worker ID (0-indexed) or None if not available</p>"},{"location":"api/types/#antflow.types.FailedItem","title":"FailedItem  <code>dataclass</code>","text":"<pre><code>FailedItem(item_id: Any, error: str, error_type: str, stage: str, attempts: int, timestamp: float)\n</code></pre> <p>Details of a single failed item.</p> <p>Attributes:</p> Name Type Description <code>item_id</code> <code>Any</code> <p>Unique identifier of the failed item</p> <code>error</code> <code>str</code> <p>Error message string</p> <code>error_type</code> <code>str</code> <p>Type name of the exception</p> <code>stage</code> <code>str</code> <p>Stage where failure occurred</p> <code>attempts</code> <code>int</code> <p>Number of attempts made before failure</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp when failure occurred</p>"},{"location":"api/types/#antflow.types.ErrorSummary","title":"ErrorSummary  <code>dataclass</code>","text":"<pre><code>ErrorSummary(total_failed: int, errors_by_type: Dict[str, int], errors_by_stage: Dict[str, int], failed_items: List[FailedItem])\n</code></pre> <p>Aggregated error information from pipeline execution.</p> <p>Attributes:</p> Name Type Description <code>total_failed</code> <code>int</code> <p>Total count of failed items</p> <code>errors_by_type</code> <code>Dict[str, int]</code> <p>Count of errors grouped by exception type</p> <code>errors_by_stage</code> <code>Dict[str, int]</code> <p>Count of errors grouped by stage name</p> <code>failed_items</code> <code>List[FailedItem]</code> <p>List of individual failed item details</p>"},{"location":"api/types/#antflow.types.DashboardProtocol","title":"DashboardProtocol","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for custom dashboard implementations.</p> <p>Implement this protocol to create custom dashboards that integrate with Pipeline.run()'s dashboard parameter.</p> Example <pre><code>class MyDashboard:\n    def on_start(self, pipeline, total_items):\n        print(f\"Starting {total_items} items\")\n\n    def on_update(self, snapshot):\n        print(f\"Progress: {snapshot.pipeline_stats.items_processed}\")\n\n    def on_finish(self, results, summary):\n        print(f\"Done! {len(results)} results, {summary.total_failed} failed\")\n\nresults = await pipeline.run(items, custom_dashboard=MyDashboard())\n</code></pre>"},{"location":"api/types/#antflow.types.DashboardProtocol.on_start","title":"on_start","text":"<pre><code>on_start(pipeline: Pipeline, total_items: int) -&gt; None\n</code></pre> <p>Called when pipeline execution starts.</p>"},{"location":"api/types/#antflow.types.DashboardProtocol.on_update","title":"on_update","text":"<pre><code>on_update(snapshot: DashboardSnapshot) -&gt; None\n</code></pre> <p>Called periodically with current pipeline state.</p>"},{"location":"api/types/#antflow.types.DashboardProtocol.on_finish","title":"on_finish","text":"<pre><code>on_finish(results: List[PipelineResult], summary: ErrorSummary) -&gt; None\n</code></pre> <p>Called when pipeline execution completes.</p>"},{"location":"api/utils/","title":"Utilities API","text":"<p>Helper functions and utilities used internally by AntFlow.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>The <code>antflow.utils</code> module provides standardized logging and error handling utilities.</p>"},{"location":"api/utils/#function-reference","title":"Function Reference","text":""},{"location":"api/utils/#antflow.utils.extract_exception","title":"extract_exception","text":"<pre><code>extract_exception(error: Exception) -&gt; Exception\n</code></pre> <p>Extract the original exception from a RetryError or return the error as-is.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception to extract from</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>The original exception if available, otherwise the input error</p>"},{"location":"api/utils/#antflow.utils.setup_logger","title":"setup_logger","text":"<pre><code>setup_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger\n</code></pre> <p>Set up a logger with consistent formatting.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name</p> required <code>level</code> <code>int</code> <p>Logging level</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p>"},{"location":"examples/","title":"Examples Index","text":"<p>This directory contains a comprehensive list of example scripts available in the <code>examples/</code> directory of the repository. These scripts demonstrate various features and patterns of AntFlow.</p>"},{"location":"examples/#basic-usage","title":"Basic Usage","text":"Example Description basic_executor.py Simple usage of <code>AsyncExecutor</code> for concurrent task execution. basic_example.py Basic <code>Pipeline</code> setup with sequential stages. executor_wait_strategies.py Demonstrates different <code>WaitStrategy</code> options for <code>AsyncExecutor</code>. builder_pattern.py Using the <code>PipelineBuilder</code> (Fluent API) to construct pipelines."},{"location":"examples/#pipeline-patterns","title":"Pipeline Patterns","text":"Example Description advanced_pipeline.py Complex pipeline with multiple stages, retries, and error handling. real_world_example.py A realistic ETL scenario simulating data ingestion, processing, and storage. streaming_results.py Processing results as they complete using the <code>stream()</code> method. priority_demo.py Handling task priorities (Low/High) in the pipeline queue. task_limits_openai.py New: Managing strict API rate limits using <code>task_concurrency_limits</code>. backpressure_demo.py New: Demonstrating automatic backpressure and queue capacity limits. resume_checkpoint.py Manual checkpointing and resuming pipeline execution from a specific point."},{"location":"examples/#monitoring-tracking","title":"Monitoring &amp; Tracking","text":"Example Description monitoring_status_tracker.py Callbacks &amp; Event-Driven Monitoring - Complete example using <code>StatusTracker</code> with <code>on_status_change</code> and task-level callbacks (<code>on_task_start</code>, <code>on_task_complete</code>, <code>on_task_retry</code>, <code>on_task_fail</code>). monitoring_workers.py Monitoring the state and activity of individual workers."},{"location":"examples/#dashboards","title":"Dashboards","text":"Example Description dashboard_levels.py Recommended: Comparing <code>compact</code>, <code>detailed</code>, and <code>full</code> built-in dashboards. custom_dashboard.py Implementing custom dashboards using <code>DashboardProtocol</code> (polling-based). custom_dashboard_callbacks.py NEW: Implementing custom dashboards using <code>StatusTracker</code> callbacks (event-driven alternative). web_dashboard/ Complete FastAPI + WebSocket dashboard for web browsers."},{"location":"examples/#detailed-guides","title":"Detailed Guides","text":"<p>For step-by-step explanations of these concepts, check out our guide pages:</p> <ul> <li>Basic Examples</li> <li>Advanced Examples</li> <li>Pipeline Guide</li> <li>Dashboard Guide</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>Complex real-world examples demonstrating advanced AntFlow features.</p>"},{"location":"examples/advanced/#pipeline-with-retry-strategies","title":"Pipeline with Retry Strategies","text":"<p>This example shows different retry strategies for handling failures:</p> <pre><code>import asyncio\nimport random\nfrom antflow import Pipeline, Stage\n\n# Tasks that may fail\nasync def fetch_api_data(item_id: int) -&gt; dict:\n    \"\"\"Simulate fetching data from an API (may fail randomly).\"\"\"\n    await asyncio.sleep(0.05)\n    if random.random() &lt; 0.2:\n        raise ConnectionError(f\"API connection failed for item {item_id}\")\n    return {\"id\": item_id, \"data\": f\"raw_data_{item_id}\"}\n\nasync def validate_data(data: dict) -&gt; dict:\n    \"\"\"Validate fetched data.\"\"\"\n    await asyncio.sleep(0.03)\n    if \"data\" not in data:\n        raise ValueError(\"Invalid data structure\")\n    data[\"validated\"] = True\n    return data\n\nasync def save_to_database(data: dict) -&gt; dict:\n    \"\"\"Simulate saving to database (may fail randomly).\"\"\"\n    await asyncio.sleep(0.06)\n    if random.random() &lt; 0.15:\n        raise IOError(f\"Database write failed for item {data['id']}\")\n    data[\"saved\"] = True\n    return data\n\n# Callbacks for monitoring\nasync def on_fetch_success(payload):\n    print(f\"  \u2705 Fetched item {payload['id']}\")\n\nasync def on_fetch_failure(payload):\n    print(f\"  \u274c Failed to fetch item {payload['id']}: {payload['error']}\")\n\nasync def on_fetch_retry(payload):\n    print(f\"  \ud83d\udd04 Retrying fetch for item {payload['id']} (attempt {payload['attempt']})\")\n\nasync def on_save_task_retry(task_name, item_id, error):\n    print(f\"  \ud83d\udd04 Task {task_name} retrying for item {item_id}: {error}\")\n\nasync def on_save_task_failure(task_name, item_id, error):\n    print(f\"  \u274c Task {task_name} failed for item {item_id}: {error}\")\n\nasync def main():\n    # Fetch stage: per-stage retry for connection errors\n    fetch_stage = Stage(\n        name=\"Fetch\",\n        workers=3,\n        tasks=[fetch_api_data],\n        retry=\"per_stage\",\n        stage_attempts=3\n    )\n\n    # Process stage: per-task retry for validation\n    process_stage = Stage(\n        name=\"Process\",\n        workers=2,\n        tasks=[validate_data],\n        retry=\"per_task\",\n        task_attempts=3,\n        task_wait_seconds=0.5\n    )\n\n    # Save stage: per-task retry\n    save_stage = Stage(\n        name=\"Save\",\n        workers=2,\n        tasks=[save_to_database],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=1.0\n    )\n\n    pipeline = Pipeline(\n        stages=[fetch_stage, process_stage, save_stage]\n    )\n\n    items = list(range(15))\n    results = await pipeline.run(items)\n\n    stats = pipeline.get_stats()\n    print(f\"\\n\ud83d\udcca Statistics:\")\n    print(f\"  Items processed: {stats.items_processed}\")\n    print(f\"  Items failed: {stats.items_failed}\")\n    print(f\"  Success rate: {stats.items_processed/(stats.items_processed + stats.items_failed)*100:.1f}%\")\n\nasyncio.run(main())\n</code></pre> <p>Key Features: - Per-stage retry for connection errors (transient failures) - Per-task retry for database writes - Comprehensive callbacks for monitoring - Statistics reporting</p>"},{"location":"examples/advanced/#real-world-etl-pipeline","title":"Real-World ETL Pipeline","text":"<p>Complete ETL example with data processing, validation, and error handling:</p> <pre><code>import asyncio\nimport random\nfrom typing import Any, Dict\nfrom antflow import Pipeline, Stage\n\nclass DataProcessor:\n    \"\"\"Example data processor with realistic ETL operations.\"\"\"\n\n    async def fetch_user_data(self, user_id: int) -&gt; Dict[str, Any]:\n        \"\"\"Fetch user data from external API.\"\"\"\n        await asyncio.sleep(0.1)\n\n        if random.random() &lt; 0.1:\n            raise ConnectionError(f\"Failed to fetch user {user_id}\")\n\n        return {\n            \"user_id\": user_id,\n            \"name\": f\"User {user_id}\",\n            \"email\": f\"user{user_id}@example.com\",\n            \"created_at\": \"2025-01-01\"\n        }\n\n    async def validate_user_data(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate user data structure and content.\"\"\"\n        await asyncio.sleep(0.05)\n\n        required_fields = [\"user_id\", \"name\", \"email\"]\n        for field in required_fields:\n            if field not in data:\n                raise ValueError(f\"Missing required field: {field}\")\n\n        if \"@\" not in data[\"email\"]:\n            raise ValueError(f\"Invalid email: {data['email']}\")\n\n        data[\"validated\"] = True\n        return data\n\n    async def transform_user_data(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Transform data to target format.\"\"\"\n        await asyncio.sleep(0.05)\n\n        return {\n            \"id\": data[\"user_id\"],\n            \"full_name\": data[\"name\"].upper(),\n            \"email_address\": data[\"email\"].lower(),\n            \"registration_date\": data[\"created_at\"],\n            \"status\": \"active\"\n        }\n\n    async def enrich_with_metadata(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Enrich data with additional metadata.\"\"\"\n        await asyncio.sleep(0.08)\n\n        data[\"metadata\"] = {\n            \"processed_at\": \"2025-10-09\",\n            \"version\": \"1.0\",\n            \"source\": \"api\"\n        }\n\n        return data\n\n    async def calculate_metrics(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Calculate user metrics.\"\"\"\n        await asyncio.sleep(0.06)\n\n        data[\"metrics\"] = {\n            \"score\": random.randint(1, 100),\n            \"rank\": random.choice([\"bronze\", \"silver\", \"gold\"]),\n            \"engagement\": random.uniform(0, 1)\n        }\n\n        return data\n\n    async def save_to_database(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Save processed data to database.\"\"\"\n        await asyncio.sleep(0.1)\n\n        if random.random() &lt; 0.05:\n            raise IOError(f\"Database write failed for user {data['id']}\")\n\n        data[\"saved\"] = True\n        data[\"db_id\"] = f\"db_{data['id']}\"\n\n        return data\n\nasync def on_stage_complete(payload):\n    stage = payload.get(\"stage\", \"Unknown\")\n    user_id = payload.get(\"id\", \"?\")\n    print(f\"  [{stage}] Completed processing user {user_id}\")\n\nasync def on_stage_failed(payload):\n    stage = payload.get(\"stage\", \"Unknown\")\n    user_id = payload.get(\"id\", \"?\")\n    error = payload.get(\"error\", \"Unknown error\")\n    print(f\"  [{stage}] \u274c Failed for user {user_id}: {error}\")\n\nasync def main():\n    print(\"=\" * 60)\n    print(\"Real-World ETL Pipeline: User Data Processing\")\n    print(\"=\" * 60)\n    print()\n\n    processor = DataProcessor()\n\n    # Ingestion stage with high concurrency and retry\n    ingestion_stage = Stage(\n        name=\"Ingestion\",\n        workers=5,\n        tasks=[processor.fetch_user_data],\n        retry=\"per_task\",\n        task_attempts=3,\n        task_wait_seconds=1.0,\n        on_success=on_stage_complete,\n        on_failure=on_stage_failed\n    )\n\n    # Validation and transformation\n    validation_stage = Stage(\n        name=\"Validation\",\n        workers=3,\n        tasks=[processor.validate_user_data, processor.transform_user_data],\n        retry=\"per_task\",\n        task_attempts=2,\n        task_wait_seconds=0.5,\n        on_success=on_stage_complete,\n        on_failure=on_stage_failed\n    )\n\n    # Data enrichment\n    enrichment_stage = Stage(\n        name=\"Enrichment\",\n        workers=3,\n        tasks=[processor.enrich_with_metadata, processor.calculate_metrics],\n        retry=\"per_stage\",\n        stage_attempts=2,\n        on_success=on_stage_complete,\n        on_failure=on_stage_failed\n    )\n\n    # Database persistence\n    persistence_stage = Stage(\n        name=\"Persistence\",\n        workers=2,\n        tasks=[processor.save_to_database],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=2.0\n    )\n\n    pipeline = Pipeline(\n        stages=[ingestion_stage, validation_stage, enrichment_stage, persistence_stage],\n        collect_results=True\n    )\n\n    user_ids = list(range(1, 51))\n\n    print(f\"\ud83d\udce5 Starting ETL pipeline for {len(user_ids)} users\")\n    print(f\"\ud83d\udcca Pipeline configuration:\")\n    for stage in pipeline.stages:\n        print(f\"  - {stage.name}: {stage.workers} workers, \"\n              f\"{len(stage.tasks)} tasks, retry={stage.retry}\")\n    print()\n\n    results = await pipeline.run(user_ids)\n\n    print()\n    print(\"=\" * 60)\n    print(\"Pipeline Execution Complete\")\n    print(\"=\" * 60)\n    print()\n\n    stats = pipeline.get_stats()\n\n    print(\"\ud83d\udcca Final Statistics:\")\n    print(f\"  Total items: {len(user_ids)}\")\n    print(f\"  Successfully processed: {stats.items_processed}\")\n    print(f\"  Failed: {stats.items_failed}\")\n    print(f\"  Success rate: {stats.items_processed/len(user_ids)*100:.1f}%\")\n\n    if results:\n        print(f\"\\n\u2705 Successfully processed {len(results)} users\")\n        print(\"\\nSample output (first 3 users):\")\n        for result in results[:3]:\n            data = result.value\n            print(f\"\\n  User ID: {data['id']}\")\n            print(f\"  Name: {data['full_name']}\")\n            print(f\"  Email: {data['email_address']}\")\n            print(f\"  Metrics: Score={data['metrics']['score']}, \"\n                  f\"Rank={data['metrics']['rank']}\")\n\nif __name__ == \"__main__\":\n    random.seed(42)  # For reproducible results\n    asyncio.run(main())\n</code></pre> <p>Key Features: - Complete ETL workflow with 4 stages - Class-based organization for related operations - Different retry strategies per stage - Comprehensive error handling and monitoring - Statistics and reporting</p>"},{"location":"examples/advanced/#task-level-concurrency-limits-rate-limiting","title":"Task-Level Concurrency Limits (Rate Limiting)","text":"<p>You can limit the number of concurrent executions for specific tasks within a stage. This is crucial for rate-limited APIs or database connections.</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def upload(item):\n    \"\"\"Simulate a rate-limited API upload.\"\"\"\n    await asyncio.sleep(0.5)\n    return item\n\nasync def poll(item):\n    \"\"\"Simulate long-running polling (not rate-limited).\"\"\"\n    await asyncio.sleep(2.0)\n    return f\"Done_{item}\"\n\nasync def main():\n    # Define a stage with 50 workers, but limit 'upload' to 2 concurrent calls\n    stage = Stage(\n        name=\"API_Stage\",\n        workers=50,\n        tasks=[upload, poll],\n        task_concurrency_limits={\n            \"upload\": 2  # Strict limit for this specific task\n        }\n    )\n\n    pipeline = Pipeline(stages=[stage])\n    results = await pipeline.run(range(20), progress=True)\n    print(f\"Processed {len(results)} items\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Key Insight: This ensures that while up to 50 items are being monitored (polling), only 2 are ever being uploaded at the same time.</p>"},{"location":"examples/advanced/#monitoring-pipeline-execution","title":"Monitoring Pipeline Execution","text":"<p>Real-time monitoring of pipeline progress:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def task1(x): return x + 1\nasync def task2(x): return x * 2\n\nasync def monitor_pipeline(pipeline, interval=1.0):\n    \"\"\"Monitor pipeline execution in real-time.\"\"\"\n    while True:\n        stats = pipeline.get_stats()\n        print(\n            f\"\ud83d\udcca Progress: {stats.items_processed} processed, \"\n            f\"{stats.items_failed} failed, \"\n            f\"{stats.items_in_flight} in-flight\"\n        )\n        await asyncio.sleep(interval)\n\nasync def main():\n    # Define your pipeline stages\n    stage1 = Stage(name=\"Stage1\", workers=5, tasks=[task1])\n    stage2 = Stage(name=\"Stage2\", workers=3, tasks=[task2])\n\n    pipeline = Pipeline(stages=[stage1, stage2])\n    items = range(100)\n\n    # Run monitoring and pipeline concurrently\n    async with asyncio.TaskGroup() as tg:\n        # Start monitoring\n        monitor_task = tg.create_task(monitor_pipeline(pipeline, interval=0.1))\n\n        # Run pipeline\n        results = await pipeline.run(items)\n\n        # Stop monitoring\n        monitor_task.cancel()\n\n    print(f\"Completed: {len(results)} items\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/advanced/#collecting-and-analyzing-failures","title":"Collecting and Analyzing Failures","text":"<p>Track failed items for retry or analysis:</p> <pre><code>import asyncio\nimport random\nfrom antflow import Pipeline, Stage\n\nclass FailureCollector:\n    def __init__(self):\n        self.failures = []\n\n    async def on_failure(self, item_id, error, metadata):\n        self.failures.append({\n            'id': item_id,\n            'error': str(error),\n            'metadata': metadata\n        })\n\nasync def risky_task(x):\n    if random.random() &lt; 0.5:\n        raise ValueError(\"Random failure\")\n    return x\n\nasync def main():\n    collector = FailureCollector()\n    items = range(10)\n\n    stage = Stage(\n        name=\"ProcessStage\",\n        workers=5,\n        tasks=[risky_task],\n        retry=\"per_task\",\n        task_attempts=3,\n        on_failure=collector.on_failure\n    )\n\n    pipeline = Pipeline(stages=[stage])\n    results = await pipeline.run(items)\n\n    # Analyze failures\n    print(f\"Success: {len(results)} items\")\n    print(f\"Failures: {len(collector.failures)} items\")\n\n    if collector.failures:\n        print(\"\\nFailed items:\")\n        for failure in collector.failures:\n            print(f\"  ID {failure['id']}: {failure['error']}\")\n\n        # Retry failed items with different configuration\n        retry_stage = Stage(\n            name=\"Retry\",\n            workers=2,\n            tasks=[risky_task],\n            retry=\"per_task\",\n            task_attempts=10,\n            task_wait_seconds=0.1\n        )\n\n        retry_pipeline = Pipeline(stages=[retry_stage])\n        # In a real scenario, you'd extract the original values to retry\n        retry_items = [f['id'] for f in collector.failures] \n        retry_results = await retry_pipeline.run(retry_items)\n\n        print(f\"\\nRetry results: {len(retry_results)} recovered\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/advanced/#context-manager-with-cleanup","title":"Context Manager with Cleanup","text":"<p>Use context managers for automatic resource cleanup:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def task(x): return x\n\nasync def main():\n    stage1 = Stage(name=\"S1\", workers=1, tasks=[task])\n    stage2 = Stage(name=\"S2\", workers=1, tasks=[task])\n    stage3 = Stage(name=\"S3\", workers=1, tasks=[task])\n    items = range(5)\n\n    async with Pipeline(stages=[stage1, stage2, stage3]) as pipeline:\n        # Pipeline runs and automatically cleans up\n        results = await pipeline.run(items)\n\n        # Do something with results\n        print(f\"Processed {len(results)} items\")\n\n    # Pipeline is automatically shut down here\n    print(\"Pipeline cleaned up\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Review the Error Handling Guide for comprehensive error strategies</li> <li>Check the API Reference for detailed class documentation</li> <li>Explore the User Guide for in-depth feature explanations</li> </ul>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>Simple examples to get started with AntFlow.</p>"},{"location":"examples/basic/#asyncexecutor-examples","title":"AsyncExecutor Examples","text":""},{"location":"examples/basic/#using-map","title":"Using map()","text":"<p>The <code>map()</code> method applies an async function to multiple inputs and returns a list:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def square(x: int) -&gt; int:\n    \"\"\"Square a number with a small delay.\"\"\"\n    await asyncio.sleep(0.1)\n    return x * x\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        # Map returns list directly\n        results = await executor.map(square, range(10))\n        print(results)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n</code></pre></p>"},{"location":"examples/basic/#using-submit","title":"Using submit()","text":"<p>Submit individual tasks and collect results:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def cube(x: int) -&gt; int:\n    \"\"\"Cube a number with a small delay.\"\"\"\n    await asyncio.sleep(0.15)\n    return x * x * x\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        # Submit individual tasks\n        futures = [executor.submit(cube, i) for i in range(5)]\n\n        # Collect results\n        results = [await f.result() for f in futures]\n        print(results)  # [0, 1, 8, 27, 64]\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>[0, 1, 8, 27, 64]\n</code></pre></p>"},{"location":"examples/basic/#using-map_iter-for-streaming","title":"Using map_iter() for Streaming","text":"<p>For streaming results as they arrive (useful for large datasets or progress feedback):</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def square(x: int) -&gt; int:\n    await asyncio.sleep(0.1)\n    return x * x\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        # Stream results with async for\n        async for result in executor.map_iter(square, range(10)):\n            print(f\"Got: {result}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic/#using-as_completed","title":"Using as_completed()","text":"<p>Process results as they complete (not in input order):</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def square(x: int) -&gt; int:\n    await asyncio.sleep(0.1)\n    return x * x\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        futures = [executor.submit(square, i) for i in range(5, 10)]\n\n        async for future in executor.as_completed(futures):\n            result = await future.result()\n            print(f\"Completed: {result}\")\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>Completed: 81\nCompleted: 25\nCompleted: 49\nCompleted: 36\nCompleted: 64\n</code></pre></p> <p>Note: The order varies because <code>as_completed()</code> returns results as they finish, not in input order.</p>"},{"location":"examples/basic/#map-vs-as_completed-comparison","title":"map() vs as_completed() Comparison","text":"<p>This example shows the key difference between the two approaches:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def variable_delay_task(x: int) -&gt; str:\n    # Item 0 is slow, others are fast\n    delay = 2.0 if x == 0 else 0.3\n    await asyncio.sleep(delay)\n    return f\"Result-{x}\"\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        items = range(5)\n\n        # map(): Results in INPUT order\n        print(\"=== map() - Input Order ===\")\n        results = await executor.map(variable_delay_task, items)\n        for r in results:\n            print(f\"  {r}\")\n\n        # as_completed(): Results in COMPLETION order\n        print(\"\\n=== as_completed() - Completion Order ===\")\n        futures = [executor.submit(variable_delay_task, i) for i in items]\n        async for future in executor.as_completed(futures):\n            print(f\"  {await future.result()}\")\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>=== map() - Input Order ===\n  Result-0\n  Result-1\n  Result-2\n  Result-3\n  Result-4\n\n=== as_completed() - Completion Order ===\n  Result-1\n  Result-3\n  Result-2\n  Result-4\n  Result-0\n</code></pre></p> <p>Key Insight:</p> <ul> <li><code>map()</code> waits for slow item 0 before returning anything</li> <li><code>as_completed()</code> returns fast items (1-4) immediately, then slow item (0)</li> </ul>"},{"location":"examples/basic/#pipeline-examples","title":"Pipeline Examples","text":""},{"location":"examples/basic/#two-stage-etl-pipeline","title":"Two-Stage ETL Pipeline","text":"<p>Extract, transform, and load data through a pipeline:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def extract(x: int) -&gt; str:\n    \"\"\"Extract/fetch data.\"\"\"\n    await asyncio.sleep(0.05)\n    return f\"data_{x}\"\n\nasync def transform(x: str) -&gt; str:\n    \"\"\"Transform data.\"\"\"\n    await asyncio.sleep(0.05)\n    return x.upper()\n\nasync def load(x: str) -&gt; str:\n    \"\"\"Load/save data.\"\"\"\n    await asyncio.sleep(0.05)\n    return f\"saved_{x}\"\n\nasync def main():\n    # Define stages\n    extract_stage = Stage(\n        name=\"Extract\",\n        workers=3,\n        tasks=[extract],\n        retry=\"per_task\",\n        task_attempts=3\n    )\n\n    transform_stage = Stage(\n        name=\"Transform\",\n        workers=2,\n        tasks=[transform, load],  # Multiple tasks in sequence\n        retry=\"per_task\",\n        task_attempts=3\n    )\n\n    # Create pipeline\n    pipeline = Pipeline(\n        stages=[extract_stage, transform_stage],\n        collect_results=True\n    )\n\n    # Process items\n    items = list(range(20))\n    results = await pipeline.run(items)\n\n    print(f\"Processed {len(results)} items\")\n    for result in results[:5]:\n        print(f\"  ID {result.id}: {result.value}\")\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>Processed 20 items\n  ID 0: saved_DATA_0\n  ID 1: saved_DATA_1\n  ID 2: saved_DATA_2\n  ID 3: saved_DATA_3\n  ID 4: saved_DATA_4\n</code></pre></p>"},{"location":"examples/basic/#pipeline-with-status-tracking","title":"Pipeline with Status Tracking","text":"<p>Monitor pipeline execution with StatusTracker:</p> <pre><code>from antflow import Pipeline, Stage, StatusTracker\n\nasync def on_status_change(event):\n    if event.status == \"completed\":\n        print(f\"\u2705 Completed item {event.item_id}\")\n    elif event.status == \"failed\":\n        print(f\"\u274c Failed item {event.item_id}: {event.metadata.get('error')}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\n\nstage = Stage(\n    name=\"ProcessStage\",\n    workers=3,\n    tasks=[process_data],\n    retry=\"per_task\",\n    task_attempts=3\n)\n\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\nresults = await pipeline.run(items)\n\n# Query statistics\nstats = tracker.get_stats()\nprint(f\"Completed: {stats['completed']}, Failed: {stats['failed']}\")\n</code></pre>"},{"location":"examples/basic/#getting-pipeline-statistics","title":"Getting Pipeline Statistics","text":"<p>Monitor pipeline progress:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def main():\n    pipeline = Pipeline(stages=[extract_stage, transform_stage])\n    results = await pipeline.run(range(100))\n\n    # Get statistics\n    stats = pipeline.get_stats()\n    print(f\"Items processed: {stats.items_processed}\")\n    print(f\"Items failed: {stats.items_failed}\")\n    print(f\"Items in-flight: {stats.items_in_flight}\")\n    print(f\"Queue sizes: {stats.queue_sizes}\")\n\nasyncio.run(main())\n</code></pre> <p>Output: <pre><code>Items processed: 100\nItems failed: 0\nItems in-flight: 0\nQueue sizes: {'Extract': 0, 'Transform': 0}\n\n---\n\n### Automatic Backpressure &amp; Flow Control\n\nAntFlow automatically prevents memory exhaustion by limiting stage queues.\n\n*   **Default Behavior**: Each stage has an input queue limited to `max(1, workers * 10)`.\n*   **Backpressure**: If a stage is slow, its input queue fills up, causing the **previous stage** (or the feeding process) to block until space is available.\n*   **Custom Limits**: You can explicitly set the limit using `queue_capacity`:\n\n```python\nstage = Stage(\n    \"SlowConsumer\", \n    workers=1, \n    tasks=[slow_task], \n    queue_capacity=5 # Only allow 5 items to wait in line\n)\n</code></pre> <pre><code>## Complete Basic Example\n\nHere's a complete working example combining executor and pipeline:\n\n```python\nimport asyncio\nfrom antflow import AsyncExecutor, Pipeline, Stage\n\n# Executor example\nasync def double(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\n# Pipeline example\nasync def fetch(x):\n    return f\"item_{x}\"\n\nasync def process(x):\n    return x.upper()\n\nasync def main():\n    # Use executor for quick parallel processing\n    print(\"=== AsyncExecutor ===\")\n    async with AsyncExecutor(max_workers=5) as executor:\n        results = await executor.map(double, range(5))\n        print(f\"Executor results: {results}\")\n\n    # Use pipeline for multi-stage processing\n    print(\"\\n=== Pipeline ===\")\n    stage1 = Stage(name=\"Fetch\", workers=3, tasks=[fetch])\n    stage2 = Stage(name=\"Process\", workers=2, tasks=[process])\n\n    pipeline = Pipeline(stages=[stage1, stage2])\n    results = await pipeline.run(range(5))\n\n    print(f\"Pipeline results:\")\n    for r in results:\n        print(f\"  {r.id}: {r.value}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre></p> <p>Output: <pre><code>=== AsyncExecutor ===\nExecutor results: [0, 2, 4, 6, 8]\n\n=== Pipeline ===\nPipeline results:\n  0: ITEM_0\n  1: ITEM_1\n  2: ITEM_2\n  3: ITEM_3\n  4: ITEM_4\n</code></pre></p>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Advanced Examples for more complex use cases</li> <li>Read the AsyncExecutor Guide for detailed documentation</li> <li>Learn about Pipeline features and patterns</li> <li>Check the API Reference for complete documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>pip</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install antflow\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/rodolfonobrega/antflow.git\ncd antflow\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import antflow\nprint(antflow.__version__)\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>Install development dependencies for testing and linting:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This includes: - pytest and pytest-asyncio for testing - mypy for type checking - ruff for linting</p>"},{"location":"getting-started/installation/#documentation-tools","title":"Documentation Tools","text":"<p>Install documentation dependencies to build the docs locally:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre> <p>This includes: - MkDocs and Material theme - mkdocstrings for API documentation generation</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with AntFlow in minutes!</p>"},{"location":"getting-started/quickstart/#asyncexecutor-example","title":"AsyncExecutor Example","text":"<p>The <code>AsyncExecutor</code> provides a familiar <code>concurrent.futures</code>-style API for async operations:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def process_item(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n        # Map over items - returns list directly\n        results = await executor.map(process_item, range(10))\n        print(results)  # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#pipeline-example","title":"Pipeline Example","text":"<p>Create multi-stage processing pipelines with ease:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def fetch(x):\n    return f\"data_{x}\"\n\nasync def process(x):\n    return x.upper()\n\nasync def main():\n    # Define stages\n    stage1 = Stage(name=\"Fetch\", workers=3, tasks=[fetch])\n    stage2 = Stage(name=\"Process\", workers=2, tasks=[process])\n\n    # Create and run pipeline\n    pipeline = Pipeline(stages=[stage1, stage2])\n    results = await pipeline.run(range(10))\n    print(results)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/quickstart/#asyncexecutor","title":"AsyncExecutor","text":"<p>Similar to <code>concurrent.futures.ThreadPoolExecutor</code> but for async operations:</p> <ul> <li><code>submit(fn, *args, **kwargs)</code> - Submit a single async task</li> <li><code>map(fn, *iterables)</code> - Map function over iterables, returns list</li> <li><code>map_iter(fn, *iterables)</code> - Map function over iterables, yields results (async iterator)</li> <li><code>as_completed(futures)</code> - Iterate over futures as they complete</li> </ul>"},{"location":"getting-started/quickstart/#pipeline","title":"Pipeline","text":"<p>Multi-stage processing with configurable workers and retry strategies:</p> <ul> <li>Stages - Define processing steps with worker pools</li> <li>Tasks - Sequence of async functions per stage</li> <li>Retry Strategies - Per-task or per-stage retry logic</li> <li>StatusTracker - Real-time monitoring with event callbacks</li> <li>Worker Tracking - Track which worker processes each item with custom item IDs</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the AsyncExecutor Guide for detailed usage</li> <li>Learn about Pipeline features and patterns</li> <li>See Worker Tracking for monitoring individual workers</li> <li>Check out Examples for real-world use cases</li> <li>Browse the API Reference for complete documentation</li> </ul>"},{"location":"user-guide/concurrency/","title":"Concurrency Control","text":"<p>AntFlow provides multiple levels of concurrency control to help you manage resources, respect rate limits, and optimize throughput.</p>"},{"location":"user-guide/concurrency/#levels-of-control","title":"Levels of Control","text":"<ol> <li>Worker Pool: Limits total concurrent tasks in an executor or stage.</li> <li>Task Limits: Limits specific task functions within a Pipeline Stage.</li> <li>Manual Semaphores: Granular control for individual <code>.submit()</code> calls.</li> </ol>"},{"location":"user-guide/concurrency/#1-worker-pool-global-limit","title":"1. Worker Pool (Global Limit)","text":"<p>The most basic control is the number of workers. This sets the hard limit on how many tasks can run in parallel for that executor or stage.</p>"},{"location":"user-guide/concurrency/#in-asyncexecutor","title":"In AsyncExecutor","text":"<pre><code>from antflow import AsyncExecutor\n\n# Maximum 10 tasks running at once\nasync with AsyncExecutor(max_workers=10) as executor:\n    results = await executor.map(task, items)\n</code></pre>"},{"location":"user-guide/concurrency/#in-pipeline-stage","title":"In Pipeline Stage","text":"<pre><code>from antflow import Stage\n\n# This stage will never run more than 5 tasks simultaneously\nstage = Stage(\n    name=\"Processing\",\n    workers=5,\n    tasks=[my_task]\n)\n</code></pre>"},{"location":"user-guide/concurrency/#2-pipeline-task-limits-granular-rate-limiting","title":"2. Pipeline Task Limits (Granular Rate Limiting)","text":"<p>In a Pipeline, a Stage might have multiple tasks (e.g., <code>validate</code> -&gt; <code>fetch_api</code> -&gt; <code>save_db</code>). You might have 50 workers to keep <code>validate</code> and <code>save_db</code> fast, but <code>fetch_api</code> has a strict rate limit.</p> <p>Use Case: High throughput stage, but one specific function must be rate-limited.</p> <pre><code>stage = Stage(\n    name=\"Enrichment\",\n    workers=50,  # High capacity for CPU tasks\n    tasks=[validate, fetch_from_api, save_to_db],\n    # Only 'fetch_from_api' is limited to 5 concurrent executions\n    task_concurrency_limits={\n        \"fetch_from_api\": 5\n    }\n)\n</code></pre> <ul> <li>Key Benefit: Workers are not blocked from doing other tasks (<code>validate</code>, <code>save_to_db</code>) while waiting for the API limit.</li> </ul>"},{"location":"user-guide/concurrency/#3-manual-semaphores-custom-control","title":"3. Manual Semaphores (Custom Control)","text":"<p>For complex workflows using <code>.submit()</code>, you can pass a shared <code>asyncio.Semaphore</code> to group tasks under a single limit.</p> <p>Use Case: You are submitting different types of tasks manually and want to limit a specific subset of them.</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def main():\n    # Create a semaphore for a specific resource (e.g., DB connection)\n    db_semaphore = asyncio.Semaphore(10)\n\n    async with AsyncExecutor(max_workers=100) as executor:\n        futures = []\n\n        # Submit tasks that need the DB\n        for item in items:\n            f = executor.submit(\n                db_task,\n                item,\n                semaphore=db_semaphore  # Share the limit\n            )\n            futures.append(f)\n\n        # Submit other tasks that don't need the DB (unlimited up to max_workers)\n        executor.submit(cpu_task, item)\n</code></pre>"},{"location":"user-guide/concurrency/#choosing-the-right-approach","title":"Choosing the Right Approach","text":"Scenario Recommended Approach Simple parallel processing Set <code>max_workers</code> appropriately Rate-limited API calls Create executor with matching workers Mixed task types in Pipeline Use <code>task_concurrency_limits</code> Complex submit workflows Use shared semaphores"},{"location":"user-guide/custom-dashboard/","title":"Custom Dashboard Guide","text":"<p>AntFlow allows you to create custom dashboards by implementing the <code>DashboardProtocol</code>.</p>"},{"location":"user-guide/custom-dashboard/#built-in-vs-custom-dashboards","title":"Built-in vs Custom Dashboards","text":"<p>AntFlow provides built-in dashboards for common use cases:</p> <pre><code># Simple progress bar (no Rich dependency)\nresults = await pipeline.run(items, progress=True)\n\n# Built-in Rich dashboards\nresults = await pipeline.run(items, dashboard=\"compact\")   # Single panel\nresults = await pipeline.run(items, dashboard=\"detailed\")  # Per-stage table\nresults = await pipeline.run(items, dashboard=\"full\")      # Full monitoring\n</code></pre> <p>For custom visualization needs, you can implement your own dashboard.</p>"},{"location":"user-guide/custom-dashboard/#two-approaches-polling-vs-callbacks","title":"Two Approaches: Polling vs Callbacks","text":"Approach Use Case How it Works DashboardProtocol (polling) Terminal UIs, web dashboards <code>on_update()</code> called periodically with full state StatusTracker callbacks (event-driven) Logging, real-time events Callbacks fired on each status change"},{"location":"user-guide/custom-dashboard/#when-to-use-each","title":"When to Use Each","text":"<ul> <li>DashboardProtocol: Best for rendering UIs that need complete state snapshots</li> <li>StatusTracker callbacks: Best for logging, event streaming, or reacting to specific events</li> </ul> <p>[!TIP] See Complete Examples: - Polling: <code>examples/custom_dashboard.py</code> - Callbacks: <code>examples/custom_dashboard_callbacks.py</code></p>"},{"location":"user-guide/custom-dashboard/#how-polling-works-dashboardprotocol","title":"How Polling Works (DashboardProtocol)","text":"<p>When you use <code>DashboardProtocol</code> (via <code>custom_dashboard</code>, <code>dashboard</code>, or <code>progress</code> parameters), AntFlow starts a background task that:</p> <ol> <li>Calls <code>get_dashboard_snapshot()</code> - Reads current pipeline state (worker states, metrics, stats)</li> <li>Calls <code>display.on_update(snapshot)</code> - Updates your dashboard with the snapshot</li> <li>Sleeps for <code>dashboard_update_interval</code> seconds (default: 0.5s)</li> <li>Repeats until the pipeline finishes</li> </ol> <pre><code># Internally, this is what happens:\nasync def _monitor_progress(display, total_items, update_interval=0.5):\n    while True:\n        snapshot = pipeline.get_dashboard_snapshot()  # Read current state\n        display.on_update(snapshot)                   # Update UI\n        await asyncio.sleep(update_interval)          # Wait\n</code></pre> <p>Key Points: - \u2705 Efficient: Snapshots just read existing state - no new objects created - \u2705 No Empty Events: Every update has current state to display - \u2705 Configurable: Adjust <code>dashboard_update_interval</code> to balance responsiveness vs CPU usage - \u2705 Automatic Cleanup: Task is cancelled immediately when pipeline finishes</p>"},{"location":"user-guide/custom-dashboard/#configuring-update-interval","title":"Configuring Update Interval","text":"<p>You can control how often the dashboard updates:</p> <pre><code># Default: 0.5 seconds (2 updates per second)\nresults = await pipeline.run(items, dashboard=\"detailed\")\n\n# Faster updates (5 updates per second) - more responsive but higher CPU\nresults = await pipeline.run(items, dashboard=\"detailed\", dashboard_update_interval=0.2)\n\n# Slower updates (1 update per second) - lower CPU usage\nresults = await pipeline.run(items, dashboard=\"detailed\", dashboard_update_interval=1.0)\n\n# Very fast updates (10 updates per second) - only for debugging\nresults = await pipeline.run(items, dashboard=\"detailed\", dashboard_update_interval=0.1)\n</code></pre> <p>Recommended Values: - 0.1s - Very responsive, good for debugging (10 updates/sec) - 0.2s - Fast updates, good for development (5 updates/sec) - 0.5s - Default, balanced (2 updates/sec) \u2b50 - 1.0s - Slower, lower CPU usage (1 update/sec)</p> <p>When to Use Lower Intervals: - Debugging fast-running pipelines - When you need to see every state change - Development and testing</p> <p>When to Use Higher Intervals: - Long-running production pipelines - When CPU usage is a concern - When you only need periodic status checks</p>"},{"location":"user-guide/custom-dashboard/#dashboardprotocol","title":"DashboardProtocol","text":"<p>The protocol defines three methods:</p> <pre><code>from antflow import DashboardProtocol\n\nclass MyDashboard:\n    def on_start(self, pipeline, total_items):\n        \"\"\"Called when pipeline execution starts.\"\"\"\n        pass\n\n    def on_update(self, snapshot):\n        \"\"\"Called periodically with current pipeline state.\"\"\"\n        pass\n\n    def on_finish(self, results, summary):\n        \"\"\"Called when pipeline execution completes.\"\"\"\n        pass\n</code></pre>"},{"location":"user-guide/custom-dashboard/#simple-example","title":"Simple Example","text":"<p>Here's a minimal custom dashboard that logs to console:</p> <pre><code>from antflow import Pipeline, Stage\n\nclass LoggingDashboard:\n    def on_start(self, pipeline, total_items):\n        print(f\"Starting pipeline with {total_items} items\")\n\n    def on_update(self, snapshot):\n        stats = snapshot.pipeline_stats\n        print(f\"Progress: {stats.items_processed}/{stats.items_processed + stats.items_in_flight}\")\n\n    def on_finish(self, results, summary):\n        print(f\"Done! {len(results)} succeeded, {summary.total_failed} failed\")\n\n# Use it\npipeline = Pipeline(stages=[...])\nresults = await pipeline.run(items, custom_dashboard=LoggingDashboard())\n</code></pre>"},{"location":"user-guide/custom-dashboard/#using-dashboardsnapshot","title":"Using DashboardSnapshot","text":"<p>The <code>on_update</code> method receives a <code>DashboardSnapshot</code> with:</p> <ul> <li><code>worker_states</code>: Dict of worker name to <code>WorkerState</code></li> <li><code>worker_metrics</code>: Dict of worker name to <code>WorkerMetrics</code></li> <li><code>pipeline_stats</code>: <code>PipelineStats</code> object</li> <li><code>timestamp</code>: Unix timestamp</li> </ul> <pre><code>def on_update(self, snapshot):\n    # Access pipeline statistics\n    stats = snapshot.pipeline_stats\n    print(f\"Processed: {stats.items_processed}\")\n    print(f\"Failed: {stats.items_failed}\")\n    print(f\"In flight: {stats.items_in_flight}\")\n\n    # Access per-stage statistics\n    for stage_name, stage_stat in stats.stage_stats.items():\n        print(f\"Stage {stage_name}:\")\n        print(f\"  Pending: {stage_stat.pending_items}\")\n        print(f\"  In progress: {stage_stat.in_progress_items}\")\n        print(f\"  Completed: {stage_stat.completed_items}\")\n\n    # Access worker states\n    busy_workers = [\n        name for name, state in snapshot.worker_states.items()\n        if state.status == \"busy\"\n    ]\n    print(f\"Busy workers: {busy_workers}\")\n\n    # Access worker metrics\n    for name, metrics in snapshot.worker_metrics.items():\n        print(f\"{name}: {metrics.items_processed} items, avg {metrics.avg_processing_time:.2f}s\")\n</code></pre>"},{"location":"user-guide/custom-dashboard/#using-errorsummary","title":"Using ErrorSummary","text":"<p>The <code>on_finish</code> method receives an <code>ErrorSummary</code>:</p> <pre><code>def on_finish(self, results, summary):\n    if summary.total_failed &gt; 0:\n        print(f\"Errors by type:\")\n        for error_type, count in summary.errors_by_type.items():\n            print(f\"  {error_type}: {count}\")\n\n        print(f\"Errors by stage:\")\n        for stage, count in summary.errors_by_stage.items():\n            print(f\"  {stage}: {count}\")\n\n        print(f\"Failed items:\")\n        for item in summary.failed_items:\n            print(f\"  ID {item.item_id}: {item.error}\")\n</code></pre>"},{"location":"user-guide/custom-dashboard/#subclassing-basedashboard","title":"Subclassing BaseDashboard","text":"<p>For more complex dashboards, subclass <code>BaseDashboard</code>:</p> <pre><code>from antflow.display import BaseDashboard\n\nclass MyRichDashboard(BaseDashboard):\n    def __init__(self):\n        self.total = 0\n\n    def on_start(self, pipeline, total_items):\n        self.total = total_items\n        print(\"Starting...\")\n\n    def render(self, snapshot):\n        # This is called by on_update\n        stats = snapshot.pipeline_stats\n        pct = (stats.items_processed / self.total * 100) if self.total else 0\n        print(f\"\\rProgress: {pct:.1f}%\", end=\"\")\n\n    def on_finish(self, results, summary):\n        print(f\"\\nDone: {len(results)} results\")\n</code></pre>"},{"location":"user-guide/custom-dashboard/#integration-with-rich","title":"Integration with Rich","text":"<p>For terminal UIs, use the Rich library:</p> <pre><code>from rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom antflow.display import BaseDashboard\n\nclass RichTableDashboard(BaseDashboard):\n    def __init__(self):\n        self.console = Console()\n        self.live = None\n        self.total = 0\n\n    def on_start(self, pipeline, total_items):\n        self.total = total_items\n        self.live = Live(self._make_table(None), console=self.console)\n        self.live.start()\n\n    def render(self, snapshot):\n        if self.live:\n            self.live.update(self._make_table(snapshot))\n\n    def on_finish(self, results, summary):\n        if self.live:\n            self.live.stop()\n        self.console.print(f\"[green]Done![/green] {len(results)} results\")\n\n    def _make_table(self, snapshot):\n        table = Table(title=\"Pipeline Status\")\n        table.add_column(\"Stage\")\n        table.add_column(\"Progress\")\n\n        if snapshot:\n            for name, stat in snapshot.pipeline_stats.stage_stats.items():\n                table.add_row(name, f\"{stat.completed_items} done\")\n\n        return table\n</code></pre>"},{"location":"user-guide/custom-dashboard/#web-dashboard-integration","title":"Web Dashboard Integration","text":"<p>AntFlow provides all the data you need to build custom web dashboards. Here's how to integrate with your frontend.</p>"},{"location":"user-guide/custom-dashboard/#available-data","title":"Available Data","text":"<p>The <code>DashboardSnapshot</code> contains everything you need:</p> <pre><code>snapshot = pipeline.get_dashboard_snapshot()\n\n# Overall stats\nsnapshot.pipeline_stats.items_processed   # Items completed (all stages)\nsnapshot.pipeline_stats.items_failed      # Items that failed\nsnapshot.pipeline_stats.items_in_flight   # Items currently being processed\nsnapshot.pipeline_stats.queue_sizes       # Dict[stage_name, queue_size]\n\n# Per-stage stats\nfor stage_name, stage_stat in snapshot.pipeline_stats.stage_stats.items():\n    stage_stat.pending_items      # Waiting in queue\n    stage_stat.in_progress_items  # Being processed\n    stage_stat.completed_items    # Finished this stage\n    stage_stat.failed_items       # Failed at this stage\n\n# Worker states\nfor worker_name, state in snapshot.worker_states.items():\n    state.stage            # Which stage this worker belongs to\n    state.status           # \"idle\" or \"busy\"\n    state.current_item_id  # Item being processed (or None)\n    state.processing_since # Timestamp when started (or None)\n\n# Worker metrics\nfor worker_name, metrics in snapshot.worker_metrics.items():\n    metrics.items_processed       # Total items processed by this worker\n    metrics.items_failed          # Total items failed by this worker\n    metrics.avg_processing_time   # Average time per item (seconds)\n</code></pre>"},{"location":"user-guide/custom-dashboard/#fastapi-rest-endpoint-example","title":"FastAPI REST Endpoint Example","text":"<pre><code>from fastapi import FastAPI\nfrom antflow import Pipeline, Stage\n\napp = FastAPI()\npipeline = None  # Will be set when pipeline starts\n\n@app.get(\"/api/pipeline/status\")\nasync def get_status():\n    \"\"\"REST endpoint that returns current pipeline state.\"\"\"\n    if not pipeline:\n        return {\"status\": \"not_running\"}\n\n    snapshot = pipeline.get_dashboard_snapshot()\n    stats = snapshot.pipeline_stats\n\n    return {\n        \"status\": \"running\",\n        \"progress\": {\n            \"processed\": stats.items_processed,\n            \"failed\": stats.items_failed,\n            \"in_flight\": stats.items_in_flight,\n            \"total\": total_items,\n        },\n        \"stages\": {\n            name: {\n                \"pending\": s.pending_items,\n                \"active\": s.in_progress_items,\n                \"completed\": s.completed_items,\n                \"failed\": s.failed_items,\n            }\n            for name, s in stats.stage_stats.items()\n        },\n        \"workers\": {\n            name: {\n                \"stage\": state.stage,\n                \"status\": state.status,\n                \"current_item\": state.current_item_id,\n            }\n            for name, state in snapshot.worker_states.items()\n        }\n    }\n\n@app.post(\"/api/pipeline/start\")\nasync def start_pipeline(items: list):\n    \"\"\"Start pipeline processing.\"\"\"\n    global pipeline, total_items\n    total_items = len(items)\n\n    pipeline = Pipeline(stages=[\n        Stage(\"Process\", workers=5, tasks=[my_task])\n    ])\n\n    # Run in background\n    asyncio.create_task(pipeline.run(items))\n    return {\"status\": \"started\", \"total\": total_items}\n</code></pre>"},{"location":"user-guide/custom-dashboard/#fastapi-websocket-example-real-time","title":"FastAPI WebSocket Example (Real-Time)","text":"<pre><code>from fastapi import FastAPI, WebSocket\nfrom antflow import Pipeline, Stage\n\napp = FastAPI()\n\n@app.websocket(\"/ws/pipeline\")\nasync def websocket_endpoint(websocket: WebSocket):\n    \"\"\"WebSocket endpoint for real-time updates.\"\"\"\n    await websocket.accept()\n\n    try:\n        while True:\n            if pipeline:\n                snapshot = pipeline.get_dashboard_snapshot()\n                stats = snapshot.pipeline_stats\n\n                await websocket.send_json({\n                    \"processed\": stats.items_processed,\n                    \"failed\": stats.items_failed,\n                    \"in_flight\": stats.items_in_flight,\n                    \"stages\": {\n                        name: {\n                            \"pending\": s.pending_items,\n                            \"active\": s.in_progress_items,\n                            \"completed\": s.completed_items,\n                        }\n                        for name, s in stats.stage_stats.items()\n                    }\n                })\n\n            await asyncio.sleep(0.1)  # Update rate: 10 Hz\n    except Exception:\n        pass  # Client disconnected\n</code></pre>"},{"location":"user-guide/custom-dashboard/#using-dashboardprotocol-with-websocket","title":"Using DashboardProtocol with WebSocket","text":"<p>For push-based updates (instead of polling), use <code>custom_dashboard</code>:</p> <pre><code>class WebSocketDashboard:\n    def __init__(self, websocket):\n        self.ws = websocket\n\n    def on_start(self, pipeline, total_items):\n        asyncio.create_task(self.ws.send_json({\n            \"type\": \"start\",\n            \"total\": total_items,\n            \"stages\": [s.name for s in pipeline.stages]\n        }))\n\n    def on_update(self, snapshot):\n        stats = snapshot.pipeline_stats\n        asyncio.create_task(self.ws.send_json({\n            \"type\": \"update\",\n            \"processed\": stats.items_processed,\n            \"failed\": stats.items_failed,\n            \"in_flight\": stats.items_in_flight,\n            \"stages\": {\n                name: {\"pending\": s.pending_items, \"active\": s.in_progress_items, \"done\": s.completed_items}\n                for name, s in stats.stage_stats.items()\n            }\n        }))\n\n    def on_finish(self, results, summary):\n        asyncio.create_task(self.ws.send_json({\n            \"type\": \"finish\",\n            \"results\": len(results),\n            \"failed\": summary.total_failed,\n            \"errors\": summary.errors_by_type\n        }))\n\n# Usage with FastAPI\n@app.websocket(\"/ws/pipeline/run\")\nasync def run_with_websocket(websocket: WebSocket, items: list):\n    await websocket.accept()\n\n    pipeline = Pipeline(stages=[...])\n    dashboard = WebSocketDashboard(websocket)\n\n    results = await pipeline.run(items, custom_dashboard=dashboard)\n    # Dashboard automatically sends start, updates, and finish events\n</code></pre>"},{"location":"user-guide/custom-dashboard/#frontend-javascript-example","title":"Frontend JavaScript Example","text":"<pre><code>// Connect to WebSocket\nconst ws = new WebSocket('ws://localhost:8000/ws/pipeline');\n\nws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n\n    // Update progress bar\n    const pct = (data.processed / totalItems) * 100;\n    document.getElementById('progress').style.width = `${pct}%`;\n\n    // Update stage table\n    for (const [stage, stats] of Object.entries(data.stages)) {\n        document.getElementById(`stage-${stage}-pending`).textContent = stats.pending;\n        document.getElementById(`stage-${stage}-active`).textContent = stats.active;\n        document.getElementById(`stage-${stage}-done`).textContent = stats.done;\n    }\n\n    // Update counters\n    document.getElementById('processed').textContent = data.processed;\n    document.getElementById('failed').textContent = data.failed;\n};\n</code></pre>"},{"location":"user-guide/custom-dashboard/#flask-example-synchronous","title":"Flask Example (Synchronous)","text":"<pre><code>from flask import Flask, jsonify\nimport asyncio\nfrom antflow import Pipeline, Stage\n\napp = Flask(__name__)\npipeline = None\nloop = asyncio.new_event_loop()\n\n@app.route('/api/status')\ndef get_status():\n    if not pipeline:\n        return jsonify({\"status\": \"not_running\"})\n\n    snapshot = pipeline.get_dashboard_snapshot()\n    stats = snapshot.pipeline_stats\n\n    return jsonify({\n        \"processed\": stats.items_processed,\n        \"failed\": stats.items_failed,\n        \"stages\": {\n            name: {\"done\": s.completed_items, \"active\": s.in_progress_items}\n            for name, s in stats.stage_stats.items()\n        }\n    })\n</code></pre>"},{"location":"user-guide/custom-dashboard/#multi-stage-progress-visualization","title":"Multi-Stage Progress Visualization","text":"<p>When running pipelines with multiple stages, understanding progress requires per-stage visibility.</p>"},{"location":"user-guide/custom-dashboard/#the-problem","title":"The Problem","text":"<p>The <code>items_processed</code> counter only increments when items complete all stages:</p> <pre><code>Pipeline: Fetch \u2192 Process \u2192 Save\nItems:    100       50        10\n\nitems_processed = 10  (only counts items that finished ALL stages)\n</code></pre>"},{"location":"user-guide/custom-dashboard/#solution-use-stage_stats","title":"Solution: Use stage_stats","text":"<p>The <code>DashboardSnapshot</code> provides <code>stage_stats</code> for per-stage visibility:</p> <pre><code>class MultiStageDashboard:\n    def on_update(self, snapshot):\n        stats = snapshot.pipeline_stats\n\n        print(f\"Overall: {stats.items_processed}/{self.total} completed end-to-end\")\n        print()\n\n        # Per-stage breakdown\n        for stage_name, stage_stat in stats.stage_stats.items():\n            total_in_stage = (\n                stage_stat.pending_items +\n                stage_stat.in_progress_items +\n                stage_stat.completed_items +\n                stage_stat.failed_items\n            )\n            print(f\"{stage_name}:\")\n            print(f\"  Pending:     {stage_stat.pending_items}\")\n            print(f\"  In Progress: {stage_stat.in_progress_items}\")\n            print(f\"  Completed:   {stage_stat.completed_items}\")\n            print(f\"  Failed:      {stage_stat.failed_items}\")\n</code></pre>"},{"location":"user-guide/custom-dashboard/#visual-example","title":"Visual Example","text":"<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Multi-Stage Pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                     \u2502\n\u2502  Overall: [\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 10% (10/100 end-to-end) \u2502\n\u2502                                                                     \u2502\n\u2502  Stage      \u2502 Pending \u2502 In Progress \u2502 Completed \u2502 Failed           \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500            \u2502\n\u2502  Fetch      \u2502    0    \u2502      3      \u2502    97     \u2502   0              \u2502\n\u2502  Process    \u2502   47    \u2502      5      \u2502    45     \u2502   0              \u2502\n\u2502  Save       \u2502   35    \u2502      2      \u2502    10     \u2502   0              \u2502\n\u2502                                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"user-guide/custom-dashboard/#alternative-statustracker-callbacks-event-driven","title":"Alternative: StatusTracker Callbacks (Event-Driven)","text":"<p>[!TIP] See a complete working example: <code>examples/monitoring_status_tracker.py</code></p> <p>This example demonstrates: - <code>on_status_change</code> callback for item-level events - <code>on_task_*</code> callbacks for task-level monitoring (start, complete, retry, fail) - Real-time polling with <code>get_stats()</code> - Query methods: <code>get_status()</code>, <code>get_by_status()</code>, <code>get_history()</code></p> <p>For event-driven monitoring (instead of polling), use <code>StatusTracker</code> callbacks:</p> <pre><code>from antflow import Pipeline, Stage, StatusTracker, StatusEvent\n\nasync def on_status_change(event: StatusEvent):\n    \"\"\"Called on EVERY status change (queued, in_progress, completed, failed).\"\"\"\n    print(f\"[{event.timestamp}] Item {event.item_id}: {event.status} @ {event.stage}\")\n\nasync def main():\n    tracker = StatusTracker(on_status_change=on_status_change)\n\n    pipeline = Pipeline(\n        stages=[\n            Stage(\"Fetch\", workers=3, tasks=[fetch]),\n            Stage(\"Process\", workers=5, tasks=[process]),\n        ],\n        status_tracker=tracker,\n    )\n\n    results = await pipeline.run(items)  # Callbacks fire during execution\n</code></pre>"},{"location":"user-guide/custom-dashboard/#task-level-callbacks","title":"Task-Level Callbacks","text":"<p>For even finer granularity, use task-level callbacks:</p> <pre><code>tracker = StatusTracker(\n    on_status_change=handle_item_status,    # Item-level events\n    on_task_start=handle_task_start,        # Task started\n    on_task_complete=handle_task_complete,  # Task succeeded\n    on_task_retry=handle_task_retry,        # Task being retried\n    on_task_fail=handle_task_fail,          # Task failed\n)\n</code></pre>"},{"location":"user-guide/custom-dashboard/#combining-both-approaches","title":"Combining Both Approaches","text":"<p>You can use both DashboardProtocol AND StatusTracker together:</p> <pre><code>async def log_failures(event: StatusEvent):\n    if event.status == \"failed\":\n        logging.error(f\"Item {event.item_id} failed at {event.stage}: {event.metadata}\")\n\ntracker = StatusTracker(on_status_change=log_failures)\n\npipeline = Pipeline(stages=[...], status_tracker=tracker)\n\n# Dashboard for UI + callbacks for logging\nresults = await pipeline.run(items, dashboard=\"compact\")\n</code></pre>"},{"location":"user-guide/custom-dashboard/#manual-polling-without-dashboardprotocol","title":"Manual Polling (Without DashboardProtocol)","text":"<p>If you prefer complete control, you can poll manually:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def main():\n    pipeline = Pipeline(stages=[...])\n\n    # Start pipeline in background\n    await pipeline.start()\n    await pipeline.feed(items)\n\n    # Manual polling loop\n    while True:\n        snapshot = pipeline.get_dashboard_snapshot()\n        stats = snapshot.pipeline_stats\n\n        print(f\"\\rProcessed: {stats.items_processed}\", end=\"\")\n\n        if stats.items_processed + stats.items_failed &gt;= len(items):\n            break\n\n        await asyncio.sleep(0.1)\n\n    await pipeline.join()\n    results = pipeline.results\n</code></pre> <p>This gives you full control over when and how to query pipeline state.</p>"},{"location":"user-guide/dashboard/","title":"Dashboard and Monitoring","text":"<p>AntFlow provides comprehensive tools for monitoring pipeline execution in real-time. You can choose from simple progress bars, detailed built-in dashboards, or build your own custom monitoring solution.</p>"},{"location":"user-guide/dashboard/#built-in-dashboards","title":"Built-in Dashboards","text":"<p>Built-in dashboards are the easiest way to monitor your pipeline. They use the <code>rich</code> library to provide beautiful, real-time terminal UIs with zero configuration.</p>"},{"location":"user-guide/dashboard/#dashboard-levels","title":"Dashboard Levels","text":"Option What it Shows Best For <code>progress=True</code> Minimal end-to-end progress bar Simple scripts <code>dashboard=\"compact\"</code> Single panel with rate, ETA, and progress General use <code>dashboard=\"detailed\"</code> Per-stage table and worker performance Multi-stage pipelines <code>dashboard=\"full\"</code> Everything + worker states + item tracker Debugging and deep monitoring <p>[!NOTE] <code>dashboard=\"full\"</code> works best when a <code>StatusTracker</code> is provided to track individual item history.</p>"},{"location":"user-guide/dashboard/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom antflow import Pipeline\n\nasync def task(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\nasync def main():\n    items = range(100)\n\n    # 1. Simple progress bar\n    await Pipeline.quick(items, task, workers=10, progress=True)\n\n    # 2. Compact dashboard\n    await Pipeline.quick(items, task, workers=10, dashboard=\"compact\")\n\n    # 3. Detailed dashboard (Recommended for multi-stage)\n    await (\n        Pipeline.create()\n        .add(\"Fetch\", task, workers=10)\n        .add(\"Process\", task, workers=5)\n        .run(items, dashboard=\"detailed\")\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/dashboard/#event-driven-monitoring-statustracker","title":"Event-Driven Monitoring (StatusTracker)","text":"<p>For logging, alerts, or reacting to specific events in real-time, use <code>StatusTracker</code> callbacks.</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage, StatusTracker\n\nasync def on_status_change(event):\n    if event.status == \"failed\":\n        print(f\"\u274c Item {event.item_id} failed at {event.stage}: {event.metadata.get('error')}\")\n    elif event.status == \"completed\" and event.stage == \"Save\":\n         print(f\"\u2705 Item {event.item_id} fully processed\")\n\nasync def main():\n    tracker = StatusTracker(on_status_change=on_status_change)\n    pipeline = Pipeline(\n        stages=[Stage(\"Process\", workers=5, tasks=[lambda x: x*2])],\n        status_tracker=tracker\n    )\n    await pipeline.run(range(10))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>[!TIP] Complete Working Example: See <code>examples/monitoring_status_tracker.py</code> for a comprehensive demonstration of: - Item-level callbacks (<code>on_status_change</code>) - Task-level callbacks (<code>on_task_start</code>, <code>on_task_complete</code>, <code>on_task_retry</code>, <code>on_task_fail</code>) - Real-time monitoring with polling - Querying item status and history</p> <p>See the StatusTracker API for task-level events like <code>on_task_retry</code> and <code>on_task_fail</code>.</p>"},{"location":"user-guide/dashboard/#snapshot-api-custom-uis","title":"Snapshot API (Custom UIs)","text":"<p>If you are building your own UI (e.g., a web dashboard with FastAPI), you can query the pipeline state at any time using snapshots.</p>"},{"location":"user-guide/dashboard/#getting-a-snapshot","title":"Getting a Snapshot","text":"<pre><code>snapshot = pipeline.get_dashboard_snapshot()\n\nprint(f\"Items processed: {snapshot.pipeline_stats.items_processed}\")\nprint(f\"Items failed: {snapshot.pipeline_stats.items_failed}\")\nprint(f\"Queue sizes: {snapshot.pipeline_stats.queue_sizes}\")\n\n# Worker states\nfor name, state in snapshot.worker_states.items():\n    print(f\"Worker {name} is {state.status}\")\n</code></pre>"},{"location":"user-guide/dashboard/#snapshot-structure","title":"Snapshot Structure","text":"<p>A <code>DashboardSnapshot</code> contains: - <code>worker_states</code>: Dict of <code>WorkerState</code> (status, current item, current task, duration)   - Includes <code>current_task</code> field updated by <code>set_task_status()</code> for internal task progress   - Note: <code>current_task</code> is only visible via polling (snapshots), NOT via <code>StatusTracker</code> callbacks - <code>worker_metrics</code>: Dict of <code>WorkerMetrics</code> (avg time, processed count, failures) - <code>pipeline_stats</code>: Aggregate statistics and per-stage metrics - <code>error_summary</code>: Detailed error statistics and failed item list - <code>timestamp</code>: Snapshot generation time</p> <p>\ud83d\udca1 TIP: For internal task progress (e.g., \"Uploading...\", \"Polling...\"), use <code>set_task_status()</code> inside your tasks. This updates <code>WorkerState.current_task</code> which is visible in snapshots. See Internal Task Status Updates for details.</p>"},{"location":"user-guide/dashboard/#advanced-combining-polling-and-events","title":"Advanced: Combining Polling and Events","text":"<p>You can manually poll the pipeline status while it runs in the background.</p> <pre><code>import asyncio\nfrom antflow import Pipeline\n\nasync def my_monitor(pipeline):\n    while True:\n        snapshot = pipeline.get_dashboard_snapshot()\n        print(f\"Monitoring: {snapshot.pipeline_stats.items_processed} items done\")\n\n        # Stop when pipeline is done (check your own condition)\n        # OR just rely on cancelling this task when main pipeline finishes\n        await asyncio.sleep(1.0)\n\nasync def main():\n    pipeline = Pipeline(stages=[...])\n\n    # Start monitor\n    monitor_task = asyncio.create_task(my_monitor(pipeline))\n\n    try:\n        await pipeline.run(items)\n    finally:\n        monitor_task.cancel()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/dashboard/#custom-dashboards","title":"Custom Dashboards","text":"<p>You can create your own dashboard class by implementing the <code>DashboardProtocol</code>. This allows you to pass your own display logic directly into <code>pipeline.run()</code>.</p> <pre><code>from antflow import DashboardProtocol\n\nclass MyLogDashboard:\n    def on_start(self, pipeline, total_items):\n        print(f\"Starting {total_items} items\")\n\n    def on_update(self, snapshot):\n        print(f\"Progress: {snapshot.pipeline_stats.items_processed}\")\n\n    def on_finish(self, results, summary):\n        print(\"Done!\")\n\n# Use it\nawait pipeline.run(items, custom_dashboard=MyLogDashboard())\n</code></pre> <p>For more details, see the Custom Dashboard Guide.</p>"},{"location":"user-guide/dashboard/#examples","title":"Examples","text":"<p>Check the <code>examples/</code> directory for full implementations:</p> <ul> <li>dashboard_levels.py: Comparing compact, detailed, and full dashboards</li> <li>custom_dashboard.py: Implementing a custom dashboard class</li> <li>web_dashboard/: Complete FastAPI + WebSocket dashboard</li> </ul>"},{"location":"user-guide/dashboard/#api-reference","title":"API Reference","text":"<ul> <li>Pipeline API - <code>get_dashboard_snapshot()</code></li> <li>Display Module - Built-in dashboards and protocols</li> <li>StatusTracker - Event-driven monitoring</li> </ul>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":"<p>AntFlow provides a robust error handling system with custom exceptions and flexible retry strategies.</p>"},{"location":"user-guide/error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>AntFlow defines a clear exception hierarchy for different error scenarios:</p> <pre><code>AntFlowError                    # Base exception\n\u251c\u2500\u2500 ExecutorShutdownError         # Executor has been shut down\n\u251c\u2500\u2500 PipelineError                 # Pipeline-specific errors\n\u2502   \u2514\u2500\u2500 StageValidationError      # Invalid stage configuration\n\u2514\u2500\u2500 TaskFailedError               # Task execution failure\n</code></pre>"},{"location":"user-guide/error-handling/#exception-types","title":"Exception Types","text":""},{"location":"user-guide/error-handling/#antflowerror","title":"AntFlowError","text":"<p>Base exception for all AntFlow errors:</p> <pre><code>from antflow import AntFlowError\n\ntry:\n    # AntFlow operations\n    pass\nexcept AntFlowError as e:\n    print(f\"AntFlow error: {e}\")\n</code></pre>"},{"location":"user-guide/error-handling/#executorshutdownerror","title":"ExecutorShutdownError","text":"<p>Raised when attempting to use a shut down executor:</p> <pre><code>from antflow import AsyncExecutor, ExecutorShutdownError\n\nexecutor = AsyncExecutor(max_workers=5)\nawait executor.shutdown()\n\ntry:\n    executor.submit(some_task, arg)\nexcept ExecutorShutdownError:\n    print(\"Cannot submit to shutdown executor\")\n</code></pre>"},{"location":"user-guide/error-handling/#pipelineerror","title":"PipelineError","text":"<p>Base exception for pipeline-specific errors:</p> <pre><code>from antflow import Pipeline, PipelineError\n\ntry:\n    # Example: Attempting invalid operation\n    raise PipelineError(\"Cannot modify pipeline while running\")\nexcept PipelineError as e:\n    print(f\"Pipeline error: {e}\")\n</code></pre>"},{"location":"user-guide/error-handling/#stagevalidationerror","title":"StageValidationError","text":"<p>Raised when stage configuration is invalid:</p> <pre><code>from antflow import Stage, StageValidationError\n\ntry:\n    stage = Stage(\n        name=\"Invalid\",\n        workers=0,  # Invalid: must be &gt;= 1\n        tasks=[my_task]\n    )\n    stage.validate()\nexcept StageValidationError as e:\n    print(f\"Invalid stage: {e}\")\n</code></pre>"},{"location":"user-guide/error-handling/#taskfailederror","title":"TaskFailedError","text":"<p>Wrapper for task failures that preserves the original exception:</p> <pre><code>from antflow import TaskFailedError\n\n# The original exception is available\ntry:\n    # ... task execution\n    pass\nexcept TaskFailedError as e:\n    print(f\"Task {e.task_name} failed\")\n    print(f\"Original error: {e.original_exception}\")\n</code></pre>"},{"location":"user-guide/error-handling/#handling-task-failures","title":"Handling Task Failures","text":""},{"location":"user-guide/error-handling/#in-asyncexecutor","title":"In AsyncExecutor","text":"<p>Exceptions in executor tasks are propagated to the caller:</p> <p><pre><code>from antflow import AsyncExecutor\n\nasync def failing_task(x):\n    if x &lt; 0:\n        raise ValueError(\"Negative value not allowed\")\n    return x * 2\n\nasync with AsyncExecutor(max_workers=3) as executor:\n    future = executor.submit(failing_task, -5)\n\n    try:\n        result = await future.result()\n    except ValueError as e:\n        print(f\"Task failed: {e}\")\n        # Handle the error appropriately\n\n### Automatic Retries\n\nYou can also configure automatic retries for `AsyncExecutor` tasks:\n\n```python\n# Retry 3 times with 0.1s delay\nfuture = executor.submit(failing_task, -5, retries=3, retry_delay=0.1)\n</code></pre> <pre><code>### In Pipeline\n\nPipeline provides multiple ways to handle failures:\n\n#### Stage-Level Callbacks\n\n```python\nfrom antflow import Stage\n\nasync def on_failure(item_id, error, metadata):\n    print(f\"Stage failed for item {item_id}: {error}\")\n    # Log to monitoring system, send alert, etc.\n\nstage = Stage(\n    name=\"ProcessStage\",\n    workers=3,\n    tasks=[risky_task],\n    on_failure=on_failure\n)\n</code></pre></p>"},{"location":"user-guide/error-handling/#task-level-callbacks-via-statustracker","title":"Task-Level Callbacks (via StatusTracker)","text":"<p>For granular tracking, use task-level callbacks on the <code>StatusTracker</code>:</p> <pre><code>from antflow import StatusTracker, TaskEvent\n\nasync def on_task_fail(event: TaskEvent):\n    print(f\"Task {event.task_name} failed for item {event.item_id}\")\n    print(f\"Error: {event.error}\")\n\ntracker = StatusTracker(\n    on_task_fail=on_task_fail\n)\n</code></pre>"},{"location":"user-guide/error-handling/#collecting-failed-items","title":"Collecting Failed Items","text":"<pre><code>from antflow import Pipeline, Stage\n\nfailed_items = []\n\nasync def collect_failures(item_id, error, metadata):\n    failed_items.append({\n        'id': item_id,\n        'error': error\n    })\n\nstage = Stage(\n    name=\"MyStage\",\n    workers=3,\n    tasks=[my_task],\n    retry=\"per_task\",\n    task_attempts=3,\n    on_failure=collect_failures\n)\n\npipeline = Pipeline(stages=[stage])\nresults = await pipeline.run(items)\n\n# Analyze failures\nprint(f\"Succeeded: {len(results)}\")\nprint(f\"Failed: {len(failed_items)}\")\nfor failed in failed_items:\n    print(f\"  Item {failed['id']}: {failed['error']}\")\n\n## Error Summary API\n\nAs of version 0.6.0, AntFlow provides a high-level error summary API. If a `StatusTracker` is configured, it provides a detailed report of all failures.\n\n```python\nresults = await pipeline.run(items)\nsummary = pipeline.get_error_summary()\n\nprint(f\"Total failed: {summary.total_failed}\")\n\n# Grouped by stage\nfor stage, count in summary.errors_by_stage.items():\n    print(f\"Stage {stage}: {count} errors\")\n\n# Grouped by error type\nfor err_type, count in summary.errors_by_type.items():\n    print(f\"Error {err_type}: {count} occurrences\")\n\n# List of all failed items with details\nfor item in summary.failed_items:\n    print(f\"Item {item.item_id} failed in {item.stage} \"\n          f\"after {item.attempts} attempts. Error: {item.error}\")\n</code></pre>"},{"location":"user-guide/error-handling/#retry-strategies","title":"Retry Strategies","text":"<p>AntFlow provides two retry strategies for pipelines:</p>"},{"location":"user-guide/error-handling/#per-task-retry","title":"Per-Task Retry","text":"<p>Each task retries independently using tenacity:</p> <pre><code>from antflow import Stage\n\nstage = Stage(\n    name=\"RobustStage\",\n    workers=5,\n    tasks=[api_call],\n    retry=\"per_task\",\n    task_attempts=5,\n    task_wait_seconds=2.0\n)\n</code></pre> <p>If a task fails after all retries, the stage fails for that item.</p>"},{"location":"user-guide/error-handling/#per-stage-retry","title":"Per-Stage Retry","text":"<p>The entire stage retries on any task failure:</p> <pre><code>from antflow import Stage\n\nstage = Stage(\n    name=\"TransactionalStage\",\n    workers=2,\n    tasks=[begin_tx, update_data, commit_tx],\n    retry=\"per_stage\",\n    stage_attempts=3\n)\n</code></pre> <p>Failed items are re-queued at the beginning of the stage.</p>"},{"location":"user-guide/error-handling/#extracting-original-exceptions","title":"Extracting Original Exceptions","text":"<p>AntFlow automatically extracts original exceptions from retry wrappers:</p> <pre><code>from antflow.utils import extract_exception\nfrom tenacity import RetryError\n\n# In callbacks, errors are already extracted\nasync def on_failure(item_id, error, metadata):\n    print(f\"Original error: {error}\")\n\n# Manual extraction if needed\ntry:\n    # ... operation with retry\n    pass\nexcept RetryError as e:\n    original = extract_exception(e)\n    print(f\"Original exception: {original}\")\n</code></pre>"},{"location":"user-guide/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/error-handling/#1-use-specific-exceptions","title":"1. Use Specific Exceptions","text":"<p>Catch specific exceptions rather than broad Exception types:</p> <pre><code>from antflow import AsyncExecutor, ExecutorShutdownError\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    result = await executor.submit(task, arg)\nexcept ExecutorShutdownError:\n    # Handle shutdown specifically\n    logger.warning(\"Executor was shut down\")\nexcept ValueError:\n    # Handle validation errors\n    logger.error(\"Invalid input\")\nexcept Exception as e:\n    # Catch-all for unexpected errors\n    logger.exception(\"Unexpected error\", exc_info=e)\n</code></pre>"},{"location":"user-guide/error-handling/#2-always-set-failure-callbacks","title":"2. Always Set Failure Callbacks","text":"<p>In production, always configure failure callbacks:</p> <pre><code>from antflow import Stage\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def log_failure(item_id, error, metadata):\n    logger.error(\n        f\"Stage failure for item {item_id}: {error}\"\n    )\n\nstage = Stage(\n    name=\"ProductionStage\",\n    workers=10,\n    tasks=[process],\n    on_failure=log_failure\n)\n</code></pre>"},{"location":"user-guide/error-handling/#3-configure-appropriate-retries","title":"3. Configure Appropriate Retries","text":"<p>Choose retry strategies based on your use case:</p> <ul> <li>Idempotent operations: Use per-task retry with high attempts</li> <li>Transactional operations: Use per-stage retry</li> <li>External APIs: Use longer wait times between retries</li> </ul> <pre><code>from antflow import Stage\n\n# For external API calls\napi_stage = Stage(\n    name=\"API\",\n    workers=5,\n    tasks=[call_api],\n    retry=\"per_task\",\n    task_attempts=5,\n    task_wait_seconds=3.0\n)\n\n# For transactional database operations\ndb_stage = Stage(\n    name=\"Database\",\n    workers=2,\n    tasks=[begin_tx, insert, update, commit],\n    retry=\"per_stage\",\n    stage_attempts=3\n)\n</code></pre>"},{"location":"user-guide/error-handling/#4-monitor-failure-rates","title":"4. Monitor Failure Rates","text":"<p>Track failures to identify issues:</p> <pre><code>stats = pipeline.get_stats()\nfailure_rate = stats.items_failed / (stats.items_processed + stats.items_failed)\n\nif failure_rate &gt; 0.1:  # More than 10% failures\n    logger.warning(f\"High failure rate: {failure_rate:.2%}\")\n    # Alert, adjust retry settings, etc.\n</code></pre>"},{"location":"user-guide/error-handling/#5-graceful-degradation","title":"5. Graceful Degradation","text":"<p>Handle failures gracefully without stopping the entire pipeline:</p> <pre><code>from antflow import Pipeline\n\nasync def on_failure(item_id, error, metadata):\n    # Log the failure\n    logger.error(f\"Item {item_id} failed: {error}\")\n\n    # Store for later retry\n    await failed_queue.put({\"id\": item_id, \"error\": error})\n\n# Pipeline continues processing other items\npipeline = Pipeline(stages=[stage])\nresults = await pipeline.run(items)\n\n# Process failures separately\nawait retry_failed_items(failed_queue)\n</code></pre>"},{"location":"user-guide/error-handling/#example-robust-etl-pipeline","title":"Example: Robust ETL Pipeline","text":"<p>Here's a complete example with comprehensive error handling:</p> <pre><code>import asyncio\nimport logging\nfrom antflow import Pipeline, Stage\n\nlogger = logging.getLogger(__name__)\n\nasync def on_stage_failure(item_id, error, metadata):\n    logger.error(f\"Stage failure for item {item_id}: {error}\")\n\nasync def on_task_retry(event):\n    logger.warning(f\"Retrying {event.task_name} for item {event.item_id}: {event.error}\")\n\nasync def fetch_data(item_id):\n    # May fail due to network issues\n    ...\n\nasync def validate_data(data):\n    # May fail due to invalid data\n    if not data.get('required_field'):\n        raise ValueError(\"Missing required field\")\n    return data\n\nasync def save_data(data):\n    # May fail due to database issues\n    ...\n\nasync def main():\n    # Tracker handles task-level events\n    tracker = StatusTracker(on_task_retry=on_task_retry)\n\n    # Fetch stage: retry on network errors\n    fetch_stage = Stage(\n        name=\"Fetch\",\n        workers=10,\n        tasks=[fetch_data],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=2.0,\n        on_failure=on_stage_failure\n    )\n\n    # Validate stage: don't retry validation errors\n    validate_stage = Stage(\n        name=\"Validate\",\n        workers=5,\n        tasks=[validate_data],\n        retry=\"per_task\",\n        task_attempts=1,\n        on_failure=on_stage_failure\n    )\n\n    # Save stage: retry on transient database errors\n    save_stage = Stage(\n        name=\"Save\",\n        workers=3,\n        tasks=[save_data],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=3.0,\n        on_failure=on_stage_failure\n    )\n\n    pipeline = Pipeline(\n        stages=[fetch_stage, validate_stage, save_stage],\n        status_tracker=tracker\n    )\n\n    items = range(100)\n    results = await pipeline.run(items)\n\n    # Report results using Error Summary API\n    summary = pipeline.get_error_summary()\n    logger.info(f\"Processed: {len(results)}\")\n    logger.info(f\"Failed: {summary.total_failed}\")\n\n    if summary.total_failed &gt; 0:\n        logger.warning(f\"Errors by stage: {summary.errors_by_stage}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/error-handling/#debugging-tips","title":"Debugging Tips","text":""},{"location":"user-guide/error-handling/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('antflow')\nlogger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"user-guide/error-handling/#use-task-level-callbacks-via-statustracker","title":"Use Task-Level Callbacks (via StatusTracker)","text":"<p>Get detailed information about task execution:</p> <pre><code>import logging\nfrom antflow import StatusTracker, TaskEvent\n\nlogger = logging.getLogger(__name__)\n\nasync def on_task_start(event: TaskEvent):\n    logger.debug(f\"Starting {event.task_name} for item {event.item_id}\")\n\nasync def on_task_fail(event: TaskEvent):\n    logger.error(f\"Task {event.task_name} failed for item {event.item_id}: {event.error}\")\n\ntracker = StatusTracker(\n    on_task_start=on_task_start,\n    on_task_fail=on_task_fail\n)\n\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\n</code></pre>"},{"location":"user-guide/error-handling/#check-pipeline-stats","title":"Check Pipeline Stats","text":"<p>Monitor pipeline health during execution:</p> <pre><code>import asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def monitor_pipeline(pipeline):\n    while True:\n        stats = pipeline.get_stats()\n        logger.info(\n            f\"Progress: {stats.items_processed} processed, \"\n            f\"{stats.items_failed} failed, \"\n            f\"{stats.items_in_flight} in-flight\"\n        )\n        await asyncio.sleep(5.0)\n\n# Run monitoring concurrently with pipeline\nasync with asyncio.TaskGroup() as tg:\n    tg.create_task(monitor_pipeline(pipeline))\n    results = await pipeline.run(items)\n</code></pre>"},{"location":"user-guide/executor/","title":"AsyncExecutor Guide","text":"<p><code>AsyncExecutor</code> provides a <code>concurrent.futures</code>-compatible API for executing async tasks concurrently with a worker pool.</p>"},{"location":"user-guide/executor/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/executor/#creating-an-executor","title":"Creating an Executor","text":"<pre><code>from antflow import AsyncExecutor\n\n# Create executor with 5 workers\nexecutor = AsyncExecutor(max_workers=5)\n\n# Or use as context manager (recommended)\nasync with AsyncExecutor(max_workers=5) as executor:\n    # Use executor here\n    pass\n</code></pre>"},{"location":"user-guide/executor/#submitting-tasks","title":"Submitting Tasks","text":"<p>The <code>submit()</code> method schedules a single async task for execution:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def process_data(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\nasync with AsyncExecutor(max_workers=3) as executor:\n    # Submit a task\n    future = executor.submit(process_data, 42)\n\n    # Wait for result\n    result = await future.result()\n    print(result)  # 84\n</code></pre>"},{"location":"user-guide/executor/#task-level-concurrency-limits","title":"Task-Level Concurrency Limits","text":"<p>A unique feature of <code>AsyncExecutor.submit()</code> is the ability to set granular concurrency limits using the <code>semaphore</code> parameter. This is useful when you have a high worker count for general tasks but need to throttle specific operations (like a rate-limited API).</p> <p>This limit is independent of the global <code>max_workers</code> setting.</p> <p>Use Case: You have 100 workers for general processing, but a specific API call used in your tasks is limited to 5 concurrent requests.</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\n# 1. Create a shared semaphore\napi_limit = asyncio.Semaphore(5)\n\nasync with AsyncExecutor(max_workers=100) as executor:\n    # 2. These tasks share the same 'api_limit' semaphore\n    # They will never exceed 5 concurrent executions\n    futures = [\n        executor.submit(process_api, i, semaphore=api_limit) \n        for i in range(50)\n    ]\n    results = await asyncio.gather(*[f.result() for f in futures])\n</code></pre>"},{"location":"user-guide/executor/#mapping-over-iterables","title":"Mapping Over Iterables","text":"<p>The <code>map()</code> method applies an async function to multiple inputs and returns a list:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def square(x):\n    await asyncio.sleep(0.1)\n    return x * x\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    # Map over inputs - returns list directly\n    results = await executor.map(square, range(10))\n    print(results)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n</code></pre>"},{"location":"user-guide/executor/#streaming-results-with-map_iter","title":"Streaming Results with map_iter()","text":"<p>For streaming behavior (processing results as they arrive), use <code>map_iter()</code>:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def square(x):\n    await asyncio.sleep(0.1)\n    return x * x\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    # Stream results with async for\n    async for result in executor.map_iter(square, range(10)):\n        print(result)  # Prints each result as it arrives\n</code></pre> <p>When to use each:</p> <ul> <li><code>map()</code>: Use for most cases - returns a list directly (like <code>list(executor.map(...))</code> in concurrent.futures)</li> <li><code>map_iter()</code>: Use for streaming, memory-constrained scenarios, or early exit patterns</li> </ul>"},{"location":"user-guide/executor/#multiple-iterables","title":"Multiple Iterables","text":"<p><code>map()</code> supports multiple iterables, similar to the built-in <code>map()</code>:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def add(x, y):\n    return x + y\n\nasync with AsyncExecutor(max_workers=3) as executor:\n    results = await executor.map(add, [1, 2, 3], [4, 5, 6])\n    print(results)  # [5, 7, 9]\n</code></pre>"},{"location":"user-guide/executor/#processing-as-completed","title":"Processing as Completed","text":"<p>The <code>as_completed()</code> method yields futures as they complete:</p> <pre><code>import asyncio\nimport random\nfrom antflow import AsyncExecutor\n\nasync def fetch_url(url):\n    await asyncio.sleep(random.uniform(0.1, 0.5))\n    return f\"Content from {url}\"\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    urls = [f\"http://example.com/page{i}\" for i in range(10)]\n    futures = [executor.submit(fetch_url, url) for url in urls]\n\n    async for future in executor.as_completed(futures):\n        result = await future.result()\n        print(f\"Got: {result}\")\n\n## Wait Strategies\n\nFor more complex coordination of multiple futures, use `AsyncExecutor.wait()`. This allows you to wait for a collection of futures with different completion requirements.\n\n```python\nfrom antflow import AsyncExecutor, WaitStrategy\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    futures = [executor.submit(long_task, i) for i in range(10)]\n\n    # Wait until ALL tasks are completed (default)\n    done, pending = await executor.wait(futures, return_when=WaitStrategy.ALL_COMPLETED)\n\n    # Return as soon as ANY task is completed\n    done, pending = await executor.wait(futures, return_when=WaitStrategy.FIRST_COMPLETED)\n\n    # Return when ANY task raises an exception\n    done, pending = await executor.wait(futures, return_when=WaitStrategy.FIRST_EXCEPTION)\n</code></pre> Strategy Description <code>ALL_COMPLETED</code> Return only when all futures have finished. <code>FIRST_COMPLETED</code> Return when at least one future has finished. <code>FIRST_EXCEPTION</code> Return when any future finishes with an exception."},{"location":"user-guide/executor/#map-vs-as_completed-when-to-use-each","title":"map() vs as_completed(): When to Use Each","text":"<p>Understanding the difference between <code>map()</code> and <code>as_completed()</code> is crucial for choosing the right approach.</p>"},{"location":"user-guide/executor/#key-difference-result-order","title":"Key Difference: Result Order","text":"Method Result Order Blocking Behavior <code>map()</code> Input order (deterministic) Waits for items in sequence <code>as_completed()</code> Completion order (non-deterministic) Returns as soon as any completes"},{"location":"user-guide/executor/#visual-example","title":"Visual Example","text":"<p>Consider processing 5 items where item 0 takes 5 seconds and items 1-4 take 1 second each:</p> <pre><code>Input: [A, B, C, D, E]\n       A=5s, B=1s, C=1s, D=1s, E=1s\n\nTimeline with map():\n\u251c\u2500 t=1s: B,C,D,E ready (waiting for A)\n\u251c\u2500 t=5s: A ready \u2192 return [A, B, C, D, E]\n\u2514\u2500 Total: 5s, all results at once\n\nTimeline with as_completed():\n\u251c\u2500 t=1s: yield B (or C,D,E - whoever finishes first)\n\u251c\u2500 t=1s: yield C\n\u251c\u2500 t=1s: yield D\n\u251c\u2500 t=1s: yield E\n\u251c\u2500 t=5s: yield A\n\u2514\u2500 Total: 5s, but you see 4 results at t=1s!\n</code></pre>"},{"location":"user-guide/executor/#code-comparison","title":"Code Comparison","text":"<pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def process(x):\n    delay = 3.0 if x == 0 else 0.5\n    await asyncio.sleep(delay)\n    return f\"Result-{x}\"\n\nasync def main():\n    async with AsyncExecutor(max_workers=5) as executor:\n\n        # Using map() - results in INPUT order\n        print(\"=== map() ===\")\n        results = await executor.map(process, range(5))\n        for r in results:\n            print(r)\n        # Output: Result-0, Result-1, Result-2, Result-3, Result-4\n        # (all printed at once after 3s)\n\n        # Using as_completed() - results in COMPLETION order\n        print(\"\\n=== as_completed() ===\")\n        futures = [executor.submit(process, i) for i in range(5)]\n        async for future in executor.as_completed(futures):\n            print(await future.result())\n        # Output: Result-1, Result-2, Result-3, Result-4 (at 0.5s)\n        #         Result-0 (at 3s)\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/executor/#when-to-use-each","title":"When to Use Each","text":""},{"location":"user-guide/executor/#use-map-when","title":"Use <code>map()</code> when:","text":"<ul> <li>\u2705 Order matters - Results must match input order</li> <li>\u2705 Batch processing - You need all results before proceeding</li> <li>\u2705 Simple code - One-liner: <code>results = await executor.map(...)</code></li> <li>\u2705 Database inserts - Maintaining referential integrity</li> </ul> <pre><code># Example: Processing records where order matters\nrecords = await executor.map(fetch_record, record_ids)\nawait database.bulk_insert(records)  # Order must match IDs\n</code></pre>"},{"location":"user-guide/executor/#use-as_completed-when","title":"Use <code>as_completed()</code> when:","text":"<ul> <li>\u2705 Progress feedback - Show results as they arrive</li> <li>\u2705 Early termination - Stop after finding what you need</li> <li>\u2705 Resource efficiency - Free memory as results complete</li> <li>\u2705 Slow outliers - Don't let one slow task block everything</li> </ul> <pre><code># Example: Search across multiple sources, return first match\nfutures = [executor.submit(search, source) for source in sources]\nasync for future in executor.as_completed(futures):\n    result = await future.result()\n    if result.found:\n        print(f\"Found: {result}\")\n        break  # Early exit!\n</code></pre>"},{"location":"user-guide/executor/#summary-table","title":"Summary Table","text":"Scenario Recommended Method Need results in input order <code>map()</code> Show progress to user <code>as_completed()</code> Batch insert to database <code>map()</code> Find first successful result <code>as_completed()</code> Simple parallel processing <code>map()</code> Minimize perceived latency <code>as_completed()</code> Memory-constrained streaming <code>map_iter()</code>"},{"location":"user-guide/executor/#automatic-retries","title":"Automatic Retries","text":"<p><code>AsyncExecutor</code> supports automatic retries for failed tasks using <code>tenacity</code>. You can configure retries for both <code>submit()</code> and <code>map()</code>.</p>"},{"location":"user-guide/executor/#retrying-submissions","title":"Retrying Submissions","text":"<pre><code>async with AsyncExecutor(max_workers=3) as executor:\n    # Retry up to 3 times (4 attempts total) with exponential backoff\n    # retry_delay sets the initial multiplier (e.g., 0.5s, 1s, 2s...)\n    future = executor.submit(\n        flaky_task,\n        arg,\n        retries=3,\n        retry_delay=0.5\n    )\n    result = await future.result()\n</code></pre>"},{"location":"user-guide/executor/#retrying-map-operations","title":"Retrying Map Operations","text":"<pre><code>async with AsyncExecutor(max_workers=5) as executor:\n    # Apply retry logic to all mapped tasks\n    results = await executor.map(\n        flaky_task,\n        items,\n        retries=3,\n        retry_delay=1.0\n    )\n</code></pre>"},{"location":"user-guide/executor/#asyncfuture","title":"AsyncFuture","text":"<p>The <code>AsyncFuture</code> object represents the result of an async task.</p>"},{"location":"user-guide/executor/#methods","title":"Methods","text":"<ul> <li><code>result(timeout=None)</code>: Wait for and return the result</li> <li><code>done()</code>: Return True if the future is done</li> <li><code>exception()</code>: Return the exception (if any)</li> </ul> <pre><code>future = executor.submit(some_task, arg)\n\n# Check if done\nif future.done():\n    result = await future.result()\n\n# Get exception if failed\nexc = future.exception()\nif exc:\n    print(f\"Task failed: {exc}\")\n</code></pre>"},{"location":"user-guide/executor/#error-handling","title":"Error Handling","text":"<p>Exceptions raised in tasks are captured and re-raised when accessing the result:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync def failing_task(x):\n    if x &lt; 0:\n        raise ValueError(\"Negative value not allowed\")\n    return x * 2\n\nasync with AsyncExecutor(max_workers=2) as executor:\n    future = executor.submit(failing_task, -5)\n\n    try:\n        result = await future.result()\n    except ValueError as e:\n        print(f\"Task failed: {e}\")\n</code></pre>"},{"location":"user-guide/executor/#shutdown","title":"Shutdown","text":""},{"location":"user-guide/executor/#manual-shutdown","title":"Manual Shutdown","text":"<pre><code>from antflow import AsyncExecutor\n\nexecutor = AsyncExecutor(max_workers=3)\n\n# Submit tasks...\nfuture = executor.submit(some_task, arg)\nawait future.result()\n\n# Shutdown\nawait executor.shutdown(wait=True)\n</code></pre>"},{"location":"user-guide/executor/#shutdown-options","title":"Shutdown Options","text":"<ul> <li><code>wait=True</code> (default): Wait for all pending tasks to complete</li> <li><code>cancel_futures=True</code>: Cancel all pending futures immediately</li> </ul> <pre><code># Wait for completion\nawait executor.shutdown(wait=True)\n\n# Cancel immediately\nawait executor.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"user-guide/executor/#context-manager-recommended","title":"Context Manager (Recommended)","text":"<p>Using a context manager ensures proper cleanup:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    # Tasks are automatically waited for on exit\n    results = await executor.map(task, items)\n# Executor is automatically shut down here\n</code></pre>"},{"location":"user-guide/executor/#timeouts","title":"Timeouts","text":"<p>Set timeouts on individual operations:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync with AsyncExecutor(max_workers=3) as executor:\n    future = executor.submit(slow_task, arg)\n\n    try:\n        result = await future.result(timeout=5.0)\n    except asyncio.TimeoutError:\n        print(\"Task took too long\")\n</code></pre> <p>Map with timeout:</p> <pre><code>import asyncio\nfrom antflow import AsyncExecutor\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    try:\n        results = await executor.map(task, items, timeout=10.0)\n        print(results)\n    except asyncio.TimeoutError:\n        print(\"One of the tasks timed out\")\n</code></pre>"},{"location":"user-guide/executor/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/executor/#choosing-worker-count","title":"Choosing Worker Count","text":"<p>The optimal number of workers depends on your workload:</p> <ul> <li>I/O-bound tasks (API calls, database queries, file I/O): Use more workers (10-100+)</li> <li>Rate-limited APIs: Match the rate limit (e.g., 10 requests/second = 10 workers)</li> <li>Memory constraints: Fewer workers if each task uses significant memory</li> <li>Benchmark and adjust: Start with 10-20 workers and measure performance</li> </ul> <pre><code>from antflow import AsyncExecutor\n\n# For I/O-bound tasks (API calls, database queries)\nexecutor = AsyncExecutor(max_workers=50)\n\n# For rate-limited APIs (e.g., 10 requests/second)\nexecutor = AsyncExecutor(max_workers=10)\n\n# For memory-intensive tasks\nexecutor = AsyncExecutor(max_workers=5)\n</code></pre> <p>Note: Since async workers are coroutines (not threads), CPU core count is not a limiting factor. The main constraints are I/O capacity, rate limits, and memory usage.</p>"},{"location":"user-guide/executor/#batching","title":"Batching","text":"<p>Process items in batches for better throughput:</p> <pre><code>import itertools\nfrom antflow import AsyncExecutor\n\nasync def process_batch(items):\n    return [await process_item(item) for item in items]\n\ndef chunks(iterable, size):\n    iterator = iter(iterable)\n    while batch := list(itertools.islice(iterator, size)):\n        yield batch\n\nasync with AsyncExecutor(max_workers=5) as executor:\n    batches = list(chunks(large_item_list, batch_size=100))\n    all_results = await executor.map(process_batch, batches)\n    # Flatten results\n    results = [item for batch in all_results for item in batch]\n</code></pre>"},{"location":"user-guide/executor/#comparison-with-concurrentfutures","title":"Comparison with concurrent.futures","text":"<p>AsyncExecutor is designed to be familiar to users of <code>concurrent.futures</code>:</p> concurrent.futures AsyncExecutor <code>ThreadPoolExecutor(max_workers=N)</code> <code>AsyncExecutor(max_workers=N)</code> <code>executor.submit(fn, *args)</code> <code>executor.submit(fn, *args)</code> <code>list(executor.map(fn, *iterables))</code> <code>await executor.map(fn, *iterables)</code> <code>as_completed(futures)</code> <code>executor.as_completed(futures)</code> <code>executor.shutdown(wait=True)</code> <code>await executor.shutdown(wait=True)</code> <code>future.result()</code> <code>await future.result()</code> <p>Key differences:</p> <ul> <li>AsyncExecutor works with async functions</li> <li><code>map()</code> returns a list directly (no need to wrap in <code>list()</code>)</li> <li>Uses <code>async with</code> instead of <code>with</code></li> <li>All operations are awaitable</li> <li><code>map_iter()</code> available for streaming behavior</li> </ul>"},{"location":"user-guide/executor/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nimport aiohttp\nfrom antflow import AsyncExecutor\n\nasync def fetch_and_process(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            text = await response.text()\n            return len(text)\n\nasync def main():\n    urls = [\n        \"http://example.com\",\n        \"http://python.org\",\n        \"http://github.com\",\n        # ... more URLs\n    ]\n\n    async with AsyncExecutor(max_workers=10) as executor:\n        print(\"Fetching URLs...\")\n\n        # Simple parallel processing with map()\n        results = await executor.map(fetch_and_process, urls)\n        for url, length in zip(urls, results):\n            print(f\"{url}: {length} chars\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/monitoring/","title":"Monitoring: Dashboard vs StatusTracker","text":"<p>AntFlow provides two primary mechanisms for monitoring pipeline execution. Understanding the differences between them will help you choose the right tool for your use case.</p>"},{"location":"user-guide/monitoring/#overview","title":"Overview","text":"Feature Dashboard StatusTracker Mechanism Polling (periodic updates) Event-driven (callbacks) Use Case Visual monitoring, progress display Logging, system integration, debugging Performance Low overhead, updates every N seconds Immediate updates on every event Complexity Simple (built-in dashboards) Flexible (custom callbacks) Data Access Current state snapshots Complete event history"},{"location":"user-guide/monitoring/#dashboard-polling-based-monitoring","title":"Dashboard: Polling-based Monitoring","text":""},{"location":"user-guide/monitoring/#how-it-works","title":"How It Works","text":"<p>Dashboard uses a polling mechanism where a background task periodically queries the pipeline state: 1. Background task calls <code>get_dashboard_snapshot()</code> every N seconds (default 0.5) 2. Snapshot contains current worker states, metrics, and pipeline statistics 3. Dashboard renders the snapshot to the terminal or custom UI</p>"},{"location":"user-guide/monitoring/#built-in-dashboards","title":"Built-in Dashboards","text":"<p>AntFlow includes three built-in dashboards:</p> <pre><code>from antflow import Pipeline, Stage\n\nasync def task(x):\n    await asyncio.sleep(0.1)\n    return x * 2\n\npipeline = Pipeline(stages=[Stage(name=\"Process\", workers=5, tasks=[task])])\n\n# Option 1: Minimal progress bar\nawait pipeline.run(items, progress=True)\n\n# Option 2: Compact dashboard\nawait pipeline.run(items, dashboard=\"compact\")\n\n# Option 3: Detailed dashboard (shows per-stage progress)\nawait pipeline.run(items, dashboard=\"detailed\")\n\n# Option 4: Full dashboard (includes error summary)\nawait pipeline.run(items, dashboard=\"full\")\n</code></pre>"},{"location":"user-guide/monitoring/#when-to-use-dashboard","title":"When to Use Dashboard","text":"<p>\u2705 Perfect for: - Interactive development and debugging - Real-time progress visualization - Identifying bottlenecks in multi-stage pipelines - Monitoring during production runs</p> <p>\u274c Not ideal for: - Storing events in external systems (databases, logs) - Complex business logic based on specific events - High-frequency event processing (&gt;10 events/second)</p>"},{"location":"user-guide/monitoring/#performance-considerations","title":"Performance Considerations","text":"<p>The polling interval is configurable:</p> <pre><code># Fast updates (more responsive, higher CPU)\nawait pipeline.run(items, dashboard=\"detailed\", dashboard_update_interval=0.1)\n\n# Slow updates (less responsive, lower CPU)\nawait pipeline.run(items, dashboard=\"detailed\", dashboard_update_interval=1.0)\n</code></pre> <p>Recommended range: 0.1 to 1.0 seconds.</p>"},{"location":"user-guide/monitoring/#custom-dashboards","title":"Custom Dashboards","text":"<p>You can create custom dashboards using the <code>DashboardProtocol</code>:</p> <pre><code>from antflow import Pipeline, PipelineResult, DashboardSnapshot, DashboardProtocol\n\nclass MyDashboard(DashboardProtocol):\n    def on_start(self, pipeline, total_items):\n        self.total = total_items\n        print(f\"Starting to process {total_items} items...\")\n\n    def on_update(self, snapshot: DashboardSnapshot):\n        stats = snapshot.pipeline_stats\n        progress = (stats.items_processed / self.total) * 100\n        print(f\"Progress: {progress:.1f}% ({stats.items_processed}/{self.total})\")\n\n    def on_finish(self, results: list[PipelineResult], summary):\n        print(f\"Done! Processed {len(results)} items\")\n        if summary.total_failed &gt; 0:\n            print(f\"Failed: {summary.total_failed}\")\n\n# Use custom dashboard\npipeline = Pipeline(stages=[...])\nawait pipeline.run(items, custom_dashboard=MyDashboard())\n</code></pre>"},{"location":"user-guide/monitoring/#statustracker-event-driven-monitoring","title":"StatusTracker: Event-driven Monitoring","text":""},{"location":"user-guide/monitoring/#how-it-works_1","title":"How It Works","text":"<p>StatusTracker uses async callbacks that are invoked immediately when events occur: 1. Pipeline emits events (queued, in_progress, completed, failed, etc.) 2. StatusTracker invokes registered callbacks 3. Callbacks can log, store, or process events immediately</p>"},{"location":"user-guide/monitoring/#basic-usage","title":"Basic Usage","text":"<pre><code>from antflow import Pipeline, StatusTracker, StatusEvent\n\nasync def on_status_change(event: StatusEvent):\n    print(f\"Item {event.item_id}: {event.status} @ {event.stage}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\n\npipeline = Pipeline(stages=[...], status_tracker=tracker)\nawait pipeline.run(items)\n</code></pre>"},{"location":"user-guide/monitoring/#event-types","title":"Event Types","text":"<p>StatusTracker can track multiple types of events:</p> <pre><code>async def on_queued(event: StatusEvent):\n    print(f\"Queued: {event.item_id}\")\n\nasync def on_start(event: StatusEvent):\n    print(f\"Started: {event.item_id} on worker {event.worker}\")\n\nasync def on_completed(event: StatusEvent):\n    print(f\"Completed: {event.item_id}\")\n\nasync def on_failed(event: StatusEvent):\n    error = event.metadata.get(\"error\", \"Unknown\")\n    print(f\"Failed: {event.item_id} - {error}\")\n\ntracker = StatusTracker(\n    on_status_change=on_queued,  # Called for ALL status changes\n)\n# Or use specific callbacks:\n# tracker = StatusTracker(\n#     on_status_change=lambda e: print(f\"Status: {e.status}\"),\n#     on_task_start=lambda e: print(f\"Task started: {e.task_name}\"),\n#     on_task_complete=lambda e: print(f\"Task done: {e.task_name}\"),\n#     on_task_retry=lambda e: print(f\"Retrying: {e.task_name}\"),\n#     on_task_fail=lambda e: print(f\"Task failed: {e.task_name}\"),\n# )\n</code></pre>"},{"location":"user-guide/monitoring/#querying-statustracker","title":"Querying StatusTracker","text":"<p>After execution, you can query the tracker for information:</p> <pre><code># Get current status of an item\nstatus = tracker.get_status(item_id=42)\nprint(f\"Current status: {status.status}\")\n\n# Get all items in a specific status\nfailed_items = tracker.get_by_status(\"failed\")\nfor item in failed_items:\n    print(f\"Failed item: {item.item_id}\")\n\n# Get aggregate statistics\nstats = tracker.get_stats()\nprint(f\"Completed: {stats['completed']}, Failed: {stats['failed']}\")\n\n# Get complete event history for an item\nhistory = tracker.get_history(item_id=42)\nfor event in history:\n    print(f\"{event.timestamp}: {event.status} @ {event.stage}\")\n</code></pre>"},{"location":"user-guide/monitoring/#error-summary","title":"Error Summary","text":"<p>StatusTracker provides aggregated error information:</p> <pre><code>error_summary = tracker.get_error_summary()\nprint(f\"Total failed: {error_summary.total_failed}\")\n\n# Group by error type\nfor error_type, count in error_summary.errors_by_type.items():\n    print(f\"  {error_type}: {count}\")\n\n# Group by stage\nfor stage, count in error_summary.errors_by_stage.items():\n    print(f\"  {stage}: {count}\")\n\n# Get individual failed items\nfor failed_item in error_summary.failed_items:\n    print(f\"Item {failed_item.item_id}: {failed_item.error}\")\n</code></pre>"},{"location":"user-guide/monitoring/#when-to-use-statustracker","title":"When to Use StatusTracker","text":"<p>\u2705 Perfect for: - Logging events to external systems (databases, log files) - Integrating with monitoring tools (Prometheus, DataDog, etc.) - Implementing custom business logic based on events - Debugging complex failure scenarios - Storing complete event history for audit trails</p> <p>\u274c Not ideal for: - Simple progress visualization (use Dashboard instead) - Real-time terminal output (use built-in dashboards) - Cases where you only need aggregate statistics (use <code>get_stats()</code>)</p>"},{"location":"user-guide/monitoring/#task-level-events","title":"Task-Level Events","text":"<p>StatusTracker also tracks events at the task level (within a stage):</p> <pre><code>async def on_task_start(event):\n    print(f\"Task '{event.task_name}' started on {event.worker}\")\n\nasync def on_task_complete(event):\n    duration = event.duration\n    print(f\"Task '{event.task_name}' completed in {duration:.2f}s\")\n\nasync def on_task_retry(event):\n    print(f\"Task '{event.task_name}' retrying (attempt {event.attempt})\")\n\nasync def on_task_fail(event):\n    print(f\"Task '{event.task_name}' failed: {event.error}\")\n\ntracker = StatusTracker(\n    on_task_start=on_task_start,\n    on_task_complete=on_task_complete,\n    on_task_retry=on_task_retry,\n    on_task_fail=on_task_fail,\n)\n</code></pre>"},{"location":"user-guide/monitoring/#combining-both-mechanisms","title":"Combining Both Mechanisms","text":"<p>You can use Dashboard and StatusTracker together:</p> <pre><code>from antflow import Pipeline, StatusTracker\n\nasync def log_to_db(event):\n    # Store event in database\n    await db.insert(event)\n\ntracker = StatusTracker(on_status_change=log_to_db)\n\npipeline = Pipeline(\n    stages=[...],\n    status_tracker=tracker,\n)\n\n# Use dashboard for visual monitoring + tracker for logging\nawait pipeline.run(items, dashboard=\"detailed\")\n</code></pre> <p>This gives you: - Visual feedback in the terminal (Dashboard) - Persistent logging to external systems (StatusTracker) - Complete event history for debugging (StatusTracker)</p>"},{"location":"user-guide/monitoring/#performance-comparison","title":"Performance Comparison","text":"Metric Dashboard StatusTracker CPU overhead Low (one snapshot every N seconds) Medium (callback per event) Memory usage Low (only current state) Medium (stores event history) Latency Up to N seconds Immediate Scalability Good for any scale Depends on callback complexity"},{"location":"user-guide/monitoring/#choosing-the-right-tool","title":"Choosing the Right Tool","text":""},{"location":"user-guide/monitoring/#use-dashboard-when","title":"Use Dashboard when:","text":"<ul> <li>You need visual progress feedback</li> <li>You're debugging in an interactive terminal</li> <li>You want to identify bottlenecks quickly</li> <li>You don't need to store event history</li> </ul>"},{"location":"user-guide/monitoring/#use-statustracker-when","title":"Use StatusTracker when:","text":"<ul> <li>You need to log events to external systems</li> <li>You want complete event history for debugging</li> <li>You need to implement custom business logic</li> <li>You're integrating with monitoring tools</li> </ul>"},{"location":"user-guide/monitoring/#use-both-when","title":"Use Both when:","text":"<ul> <li>You want visual feedback AND persistent logging</li> <li>You're debugging in production and need both views</li> <li>You need to show progress to users while logging internally</li> </ul>"},{"location":"user-guide/monitoring/#examples","title":"Examples","text":"<p>See the examples directory for complete working examples:</p> <ul> <li>monitoring_status_tracker.py - StatusTracker with callbacks</li> <li>dashboard_levels.py - Using different dashboard levels</li> <li>custom_dashboard_callbacks.py - Custom dashboard with tracker</li> <li>monitoring_workers.py - Worker-level monitoring</li> </ul>"},{"location":"user-guide/pipeline/","title":"Pipeline Guide","text":"<p>The <code>Pipeline</code> class enables multi-stage async processing with configurable worker pools, retry strategies, and real-time status tracking.</p>"},{"location":"user-guide/pipeline/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/pipeline/#stages","title":"Stages","text":"<p>A Stage represents a processing step with: - One or more sequential tasks - A pool of workers executing in parallel - A retry strategy (per-task or per-stage) - Automatic status tracking when StatusTracker is configured</p>"},{"location":"user-guide/pipeline/#pipeline","title":"Pipeline","text":"<p>A Pipeline connects multiple stages together: - Items flow from one stage to the next - Each stage has its own queue and workers - Order can be preserved across stages - Results are collected from the final stage</p>"},{"location":"user-guide/pipeline/#creating-pipelines-three-approaches","title":"Creating Pipelines (Three Approaches)","text":"<p>AntFlow offers three equivalent ways to create pipelines depending on your needs.</p>"},{"location":"user-guide/pipeline/#1-fluent-builder-api-concise-recommended","title":"1. Fluent Builder API (Concise &amp; Recommended)","text":"<p>Use <code>Pipeline.create()</code> for a clean, chainable builder pattern. This is ideal for most multi-stage pipelines.</p> <pre><code>from antflow import Pipeline\n\nasync def main():\n    results = await (\n        Pipeline.create()\n        .add(\"Fetch\", fetch_task, workers=10)\n        .add(\"Process\", process_task, workers=5)\n        .run(items, progress=True)\n    )\n</code></pre>"},{"location":"user-guide/pipeline/#2-manual-stage-construction-full-control","title":"2. Manual Stage Construction (Full Control)","text":"<p>Explicitly define <code>Stage</code> objects. Use this when you need fine-grained control over advanced parameters like <code>task_concurrency_limits</code> or <code>skip_if</code>.</p> <pre><code>from antflow import Pipeline, Stage\n\nstage1 = Stage(name=\"Fetch\", workers=10, tasks=[fetch_task])\nstage2 = Stage(name=\"Process\", workers=5, tasks=[process_task])\n\npipeline = Pipeline(stages=[stage1, stage2])\nresults = await pipeline.run(items)\n</code></pre>"},{"location":"user-guide/pipeline/#3-quick-one-liner-api","title":"3. Quick One-Liner API","text":"<p>For simple scripts or single-stage processing, use <code>Pipeline.quick()</code>.</p> <pre><code>from antflow import Pipeline\n\n# Single task\nresults = await Pipeline.quick(items, process_task, workers=5, progress=True)\n\n# Multiple tasks in one stage\nresults = await Pipeline.quick(items, [fetch, process], workers=10)\n</code></pre> Method When to Use Fluent API Multi-stage pipelines, clean code, prototyping Manual Stages Custom parameters, complex topologies, explicit configuration Pipeline.quick() Single-stage processing, simple scripts, one-off tasks"},{"location":"user-guide/pipeline/#stage-configuration-reference","title":"Stage Configuration Reference","text":"<p>The <code>Stage</code> class is the building block of your pipeline. Here are all the available configuration options:</p> Parameter Type Default Description <code>name</code> <code>str</code> Required Unique identifier for the stage. Used in logs and status events. <code>workers</code> <code>int</code> Required Number of concurrent workers for this stage. <code>tasks</code> <code>List[TaskFunc]</code> Required List of async functions to execute sequentially for each item. <code>retry</code> <code>str</code> <code>\"per_task\"</code> Retry strategy: <code>\"per_task\"</code> (retry individual function) or <code>\"per_stage\"</code> (restart stage from first task). <code>task_attempts</code> <code>int</code> <code>3</code> Max attempts per task (used in <code>\"per_task\"</code> mode). <code>stage_attempts</code> <code>int</code> <code>3</code> Max attempts per stage (used in <code>\"per_stage\"</code> mode). <code>task_wait_seconds</code> <code>float</code> <code>1.0</code> Time to wait between retries. <code>task_concurrency_limits</code> <code>Dict[str, int]</code> <code>None</code> Max concurrent executions for specific task functions (see example below). <code>unpack_args</code> <code>bool</code> <code>False</code> If <code>True</code>, unpacks the input item (<code>*args</code>/<code>**kwargs</code>) when calling the first task. <code>on_success</code> <code>Callable</code> <code>None</code> Callback or <code>async</code> function to run on successful item completion. <code>on_failure</code> <code>Callable</code> <code>None</code> Callback or <code>async</code> function to run on item failure (after all retries). <code>skip_if</code> <code>Callable</code> <code>None</code> Predicate function <code>(item) -&gt; bool</code>. If <code>True</code>, the item skips this stage entirely. <code>queue_capacity</code> <code>int</code> <code>None</code> Optional. Manually set input queue size. If <code>None</code> (default), uses smart limit (<code>workers * 10</code>). Set to <code>0</code> for infinite."},{"location":"user-guide/pipeline/#use-case-openai-batch-processing","title":"Use Case: OpenAI Batch Processing","text":"<p><code>task_concurrency_limits</code> is powerful when you have a large worker pool for general processing but need to throttle specific operations (like API uploads) due to strict rate limits.</p> <p>Scenario: You have 1000 jobs to process using OpenAI's Batch API. The process is: <code>Generate -&gt; Upload -&gt; Start Job -&gt; Poll -&gt; Download</code>.</p> <p>The Constraints:</p> <ol> <li>Strict Upload Limit: You can only upload 2 files at the same time (rate limit).</li> <li>Batch Queue Capacity: You can have max 50 active jobs running in the Batch API at once.</li> </ol> <p>The Problem: *   If you set <code>workers=2</code> (to satisfy the upload limit), you will only ever have 2 jobs running in the Batch API. You are wasting 48 slots of capacity! *   If you set <code>workers=50</code> (to fill the Batch API capacity), all 50 workers will try to upload files simultaneously at the start, causing you to hit the upload rate limit and crash.</p> <p>Solution: Use <code>workers=50</code> to maximize your Batch API usage, but use <code>task_concurrency_limits</code> to throttle only the upload task to 2.</p> <pre><code>import asyncio\nfrom antflow import Stage, Pipeline\n\nasync def upload_file(item):\n    await asyncio.sleep(0.1)\n    return f\"uploaded_{item}\"\n\nasync def poll_status(item):\n    await asyncio.sleep(0.1)\n    return \"done\"\n\nasync def main():\n    stage = Stage(\n        name=\"OpenAI_Batch_Job\",\n        workers=50,\n        tasks=[upload_file, poll_status],\n        task_concurrency_limits={\n            \"upload_file\": 2  # Limits concurrent uploads to 2\n        }\n    )\n    pipeline = Pipeline(stages=[stage])\n    await pipeline.run(range(10), progress=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Architecture Comparison: Which approach should you use?</p> <p>At first glance, you might think: \"If AntFlow has automatic backpressure, I can just separate everything into different stages and let the system handle it, right?\"</p> <p>Well, the answer is: It depends on how strict your limits are.</p>"},{"location":"user-guide/pipeline/#option-a-the-obvious-way-two-stages","title":"Option A: The \"Obvious\" Way (Two Stages) \u26a0\ufe0f","text":"<p>You might decide to separate the logic into two stages to keep things clean. After all, uploading and polling are different concerns.</p> <pre><code># \u26a0\ufe0f WARNING: The \"clean\" way that hides a technical trap\nstage_upload = Stage(\"Upload\", workers=2, tasks=[upload])\nstage_poll = Stage(\"Polling\", workers=50, tasks=[poll], queue_capacity=1)\n</code></pre> <p>The logic seems sound: You have 2 workers dedicated to uploading and 50 dedicated to polling. Since you set <code>queue_capacity=1</code>, you expect the uploaders to stop once the polling stage is full.</p> <p>But here is the catch: *   The Hidden Item: Every stage must have an input queue. Even with <code>capacity=1</code>, you have a \"waiting room\" slot. *   The Math: 50 workers (polling) + 1 queue slot = 51 active jobs on OpenAI. *   The Danger: The item in the queue has already been successfully uploaded by Stage A, but it is waiting for a Polling worker to become free. For OpenAI, that job is running. For your pipeline, it is currently unmonitored.</p> <p>Verdict: \u274c Risky. This approach is \"cleaner\" in code but technically less precise. Use it only if being \"off-by-one\" (or off-by-N) isn't a problem for your API limits.</p>"},{"location":"user-guide/pipeline/#option-b-the-correct-way-single-stage-task-limits","title":"Option B: The \"Correct\" Way (Single Stage + Task Limits) \ud83c\udfc6","text":"<p>This is the recommended approach for professional production pipelines where limits are non-negotiable.</p> <pre><code># \ud83c\udfc6 BEST PRACTICE: 100% Precision\nstage = Stage(\n    \"Combined\", \n    workers=50, \n    tasks=[upload, poll],\n    task_concurrency_limits={\"upload\": 2}\n)\n</code></pre> <p>Why this is superior: *   No Buffers: There is no internal queue between the <code>upload</code> and the <code>poll</code>. They happen sequentially inside the same worker. *   Total Control: A worker only fetches a new item from the source when it is 100% free. If you have 50 workers, you have exactly 50 jobs in flight. No more, no less. *   Zero Gap: As soon as a file is uploaded, that specific worker immediately starts monitoring it. No job ever sits \"unattended\" in a queue.</p> <p>Verdict: \u2705 Winner. This is the most robust way to ensure 100% compliance with external API constraints.</p>"},{"location":"user-guide/pipeline/#the-danger-life-without-backpressure-the-firehose-risk","title":"The Danger: Life Without Backpressure (The \"Firehose\" Risk) \u274c","text":"<p>To understand why backpressure matters, imagine we have infinite queues (which was the default before version 0.6.1).</p> <p>The Scenario:</p> <ol> <li>Stage A (Upload) has 2 workers and takes 1 second per job.</li> <li>Stage B (Polling) has 50 workers but takes 10 minutes per job (waiting for OpenAI).</li> <li>You feed 1000 items.</li> </ol> <p>What happens without Backpressure? *   Stage A starts fast. Every 1 second, it finishes 2 uploads and pushes them to Stage B. *   Stage B can only process 50 jobs at a time. The rest (950 jobs) sit in its queue. *   The Trap: Since Stage A doesn't know Stage B is full, it keeps uploading. After 10 minutes, Stage A has finished uploading all 1000 files. *   The Disaster: You now have 1000 active jobs on OpenAI, but you only have 50 workers monitoring them. This means 950 jobs are being processed by OpenAI but are NOT being monitored by your pipeline while they sit in the queue. If they fail or finish, you won't know until a worker frees up, potentially hours later.</p> <p>This is a \"Firehose\": A fast stage drowning a slow stage because there is no way to say \"Stop, I'm full!\".</p>"},{"location":"user-guide/pipeline/#automatic-backpressure-smart-limits","title":"Automatic Backpressure (Smart Limits)","text":"<p>As of version 0.6.1, AntFlow implements Smart Internal Limits for all stage queues.</p> <ul> <li>How it works: Each stage has a limited input queue size. The default limit is <code>max(1, workers * 10)</code>. </li> <li>Example: A stage with 5 workers allows ~50 items to be queued waiting for processing.</li> <li>Effect: If a stage is slow, its input queue fills up. Once full, the previous stage (or the <code>feed()</code> call) is blocked from adding more items until space clears up.</li> <li>Benefit: This automatically propagates backpressure upstream, preventing memory exhaustion and regulating flow without manual configuration.</li> </ul> <p>Customizing Limits:</p> <p>You can override this behavior using the <code>queue_capacity</code> parameter:</p> <pre><code>stage = Stage(\n    name=\"BufferStage\",\n    workers=2,\n    queue_capacity=1000, # Large buffer\n    tasks=[process]\n)\n</code></pre>"},{"location":"user-guide/pipeline/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/pipeline/#creating-a-simple-pipeline","title":"Creating a Simple Pipeline","text":"<pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def fetch(x):\n    await asyncio.sleep(0.1)\n    return f\"data_{x}\"\n\nasync def main():\n    stage = Stage(name=\"Fetch\", workers=3, tasks=[fetch])\n    pipeline = Pipeline(stages=[stage])\n    results = await pipeline.run(range(10), progress=True)\n    print(f\"Results: {len(results)} items\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/pipeline/#interactive-execution","title":"Interactive Execution","text":"<p>For more control, you can split execution into <code>start</code>, <code>feed</code>, and <code>join</code> steps. This allows you to inject items dynamically while the pipeline is running:</p> <pre><code># Start background workers\nawait pipeline.start()\n\n# Feed initial batch to first stage (default)\nawait pipeline.feed(batch_1)\n\n# Do other work...\n\n# Inject items directly into a specific stage (e.g., resuming from a checkpoint)\n# This item will skip previous stages and start processing at 'ProcessStage'\nawait pipeline.feed(recovered_items, target_stage=\"ProcessStage\")\n\n# Wait for completion\nawait pipeline.join()\n\n# Access results\nprint(pipeline.results)\n</code></pre>"},{"location":"user-guide/pipeline/#multiple-tasks-per-stage","title":"Multiple Tasks per Stage","text":"<p>A stage can have multiple tasks that execute sequentially for each item:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nasync def validate(x):\n    if x &lt; 0:\n        raise ValueError(\"Negative value\")\n    return x\n\nasync def transform(x):\n    return x * 2\n\nasync def format_output(x):\n    return f\"Result: {x}\"\n\nstage = Stage(\n    name=\"ProcessStage\",\n    workers=3,\n    tasks=[validate, transform, format_output]\n)\n\npipeline = Pipeline(stages=[stage])\nresults = await pipeline.run(range(10))\n</code></pre>"},{"location":"user-guide/pipeline/#retry-strategies","title":"Retry Strategies","text":""},{"location":"user-guide/pipeline/#per-task-retry","title":"Per-Task Retry","text":"<p>Each task retries independently using tenacity:</p> <pre><code>from antflow import Stage\n\nstage = Stage(\n    name=\"FetchStage\",\n    workers=5,\n    tasks=[fetch_from_api],\n    retry=\"per_task\",\n    task_attempts=3,\n    task_wait_seconds=1.0\n)\n</code></pre> <ul> <li><code>task_attempts</code>: Maximum retry attempts per task</li> <li><code>task_wait_seconds</code>: Wait time between retries</li> </ul> <p>If a task fails after all retries, the stage fails for that item.</p>"},{"location":"user-guide/pipeline/#per-stage-retry","title":"Per-Stage Retry","text":"<p>The entire stage (all tasks) retries on any failure:</p> <pre><code>from antflow import Stage\n\nstage = Stage(\n    name=\"TransactionStage\",\n    workers=2,\n    tasks=[begin_transaction, update_db, commit],\n    retry=\"per_stage\",\n    stage_attempts=3\n)\n</code></pre> <ul> <li><code>stage_attempts</code>: Maximum retry attempts for the entire stage</li> </ul> <p>If any task fails, the item is re-queued at the beginning of the stage.</p>"},{"location":"user-guide/pipeline/#when-to-use-which","title":"When to Use Which","text":"<p>Per-Task Retry: - Independent tasks that can fail separately - Fine-grained retry control - Tasks with different failure modes</p> <p>Per-Stage Retry: - Transactional operations - Tasks with dependencies - All-or-nothing processing</p>"},{"location":"user-guide/pipeline/#status-tracking","title":"Status Tracking","text":"<p>Track items in real-time as they flow through the pipeline with <code>StatusTracker</code>.</p>"},{"location":"user-guide/pipeline/#basic-status-tracking","title":"Basic Status Tracking","text":"<pre><code>from antflow import Pipeline, Stage, StatusTracker\n\ntracker = StatusTracker()\n\nstage = Stage(\n    name=\"ProcessStage\",\n    workers=3,\n    tasks=[my_task]\n)\n\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\nresults = await pipeline.run(items)\n\n# Query statistics\nstats = tracker.get_stats()\nprint(f\"Completed: {stats['completed']}\")\nprint(f\"Failed: {stats['failed']}\")\n</code></pre>"},{"location":"user-guide/pipeline/#real-time-event-monitoring","title":"Real-Time Event Monitoring","text":"<p>Get notified when items change status:</p> <pre><code>from antflow import Pipeline, StatusTracker\n\nasync def on_status_change(event):\n    print(f\"{event.status.upper()}: Item {event.item_id} @ {event.stage}\")\n\n    if event.status == \"failed\":\n        print(f\"  Error: {event.metadata.get('error')}\")\n    elif event.status == \"in_progress\":\n        print(f\"  Worker: {event.worker}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\n</code></pre>"},{"location":"user-guide/pipeline/#task-level-event-monitoring","title":"Task-Level Event Monitoring","text":"<p>For granular tracking, monitor individual tasks within stages:</p> <p><pre><code>from antflow import StatusTracker, TaskEvent\n\nasync def on_task_retry(event: TaskEvent):\n    print(f\"\u26a0\ufe0f  Task {event.task_name} retry #{event.attempt}\")\n    print(f\"   Item: {event.item_id}, Error: {event.error}\")\n\nasync def on_task_fail(event: TaskEvent):\n    print(f\"\u274c Task {event.task_name} FAILED after {event.attempt} attempts\")\n\n    if event.task_name == \"save_to_database\":\n        await send_critical_alert(f\"Database save failed for {event.item_id}\")\n\ntracker = StatusTracker(\n    on_success=handle_success,\n    on_failure=handle_failure\n)\n\n### Task Concurrency Limits\n\nYou can limit the concurrency of specific tasks within a stage using `task_concurrency_limits`. This is useful when you have a high number of workers (e.g., 50) but one specific task (like an API call) has a strict rate limit (e.g., 5 concurrent requests).\n\nSee the [Concurrency Control Guide](concurrency.md) for more details.\n\n```python\nstage = Stage(\n    name=\"ETL\",\n    tasks=[extract, transform, validate, save],\n    retry=\"per_task\",\n    task_attempts=3,\n    # Limit specific tasks (e.g., API calls) to avoid rate limits\n    task_concurrency_limits={\n        \"extract\": 5,  # Max 5 concurrent extract calls\n        \"save\": 20     # Max 20 concurrent save calls\n    }\n)\n\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\n</code></pre> <pre><code>**Task Events Available:**\n\n- `on_task_start`: Called when a task begins execution\n- `on_task_complete`: Called when a task completes successfully\n- `on_task_retry`: Called when a task is retrying after a failure\n- `on_task_fail`: Called when a task fails after all retry attempts\n\nEach `TaskEvent` contains:\n- `item_id`: Item being processed\n- `stage`: Stage name\n- `task_name`: Specific task function name\n- `worker`: Worker processing the task\n- `event_type`: \"start\", \"complete\", \"retry\", or \"fail\"\n- `attempt`: Current attempt number (1-indexed)\n- `timestamp`: When the event occurred\n- `error`: Exception if task failed (None otherwise)\n- `duration`: Task execution time in seconds (None for start events)\n\n### Query Item Status\n\n```python\n# Get specific item status\nstatus = tracker.get_status(item_id=42)\nprint(f\"Item 42: {status.status} @ {status.stage}\")\n\n# Get all failed items\nfailed = tracker.get_by_status(\"failed\")\nfor event in failed:\n    print(f\"Item {event.item_id}: {event.metadata['error']}\")\n\n# Get item history\nhistory = tracker.get_history(item_id=42)\nfor event in history:\n    print(f\"{event.timestamp}: {event.status}\")\n</code></pre></p>"},{"location":"user-guide/pipeline/#status-types","title":"Status Types","text":"<p>Items progress through these states: - <code>queued</code> - Waiting in stage queue - <code>in_progress</code> - Being processed by worker - <code>completed</code> - Successfully finished stage - <code>failed</code> - Failed to complete stage - <code>retrying</code> - Item is being retried after a failure - <code>skipped</code> - Item was skipped via <code>skip_if</code> predicate</p> <p>Important: Status tracking is at the stage level, not individual task level. If a stage has multiple tasks (e.g., <code>[validate, transform, enrich]</code>), you will know the stage failed but not which specific task caused the failure. The error message in <code>event.metadata['error']</code> will contain details about the failure. For task-level granularity, consider using separate stages (one task per stage) or adding logging within tasks.</p>"},{"location":"user-guide/pipeline/#order-preservation","title":"Order Preservation","text":"<p>Results are always returned in input order:</p> <pre><code>from antflow import Pipeline\n\npipeline = Pipeline(stages=[stage1, stage2])\nresults = await pipeline.run(items)\n# Results maintain input order\n</code></pre>"},{"location":"user-guide/pipeline/#internal-task-status-updates","title":"Internal Task Status Updates","text":"<p>When you have long-running tasks with multiple internal steps (like polling an external API), you can use <code>set_task_status()</code> to update the dashboard in real-time without waiting for the entire task to complete.</p>"},{"location":"user-guide/pipeline/#the-problem","title":"The Problem","text":"<p>Consider this scenario:</p> <pre><code>async def process_openai_batch(file_path: str):\n    # Upload file (takes 2 seconds)\n    file_id = await upload_to_openai(file_path)\n\n    # Poll for status (takes 5 minutes!)\n    while True:\n        status = await check_batch_status(file_id)\n        if status == \"completed\":\n            break\n        await asyncio.sleep(10)  # Poll every 10 seconds\n\n    # Download results (takes 3 seconds)\n    return await download_results(file_id)\n</code></pre> <p>The issue: The dashboard only shows <code>current_task = \"process_openai_batch\"</code> for the entire 5+ minutes. You can't see whether it's uploading, polling, or downloading.</p>"},{"location":"user-guide/pipeline/#the-solution-set_task_status","title":"The Solution: <code>set_task_status()</code>","text":"<p>Use <code>set_task_status()</code> to update the dashboard status from within your task:</p> <pre><code>from antflow import set_task_status\n\nasync def process_openai_batch(file_path: str):\n    # Step 1: Upload\n    set_task_status(\"\u2b06\ufe0f  Uploading file...\")\n    file_id = await upload_to_openai(file_path)\n\n    # Step 2: Poll with live status\n    set_task_status(\"\u23f3 Polling: preparing...\")\n    while True:\n        status = await check_batch_status(file_id)\n        set_task_status(f\"\u23f3 Polling: {status}...\")\n\n        if status == \"completed\":\n            break\n        await asyncio.sleep(10)\n\n    # Step 3: Download\n    set_task_status(\"\u2b07\ufe0f  Downloading results...\")\n    return await download_results(file_id)\n</code></pre> <p>Now the dashboard's \"Current Task\" column will show: - <code>\u2b06\ufe0f  Uploading file...</code> (for 2 seconds) - <code>\u23f3 Polling: preparing...</code> (for ~1 minute) - <code>\u23f3 Polling: processing...</code> (for ~3 minutes) - <code>\u23f3 Polling: finalizing...</code> (for ~1 minute) - <code>\u2b07\ufe0f  Downloading results...</code> (for 3 seconds)</p>"},{"location":"user-guide/pipeline/#how-it-works","title":"How It Works","text":"<p>\u26a0\ufe0f IMPORTANT: <code>set_task_status()</code> requires polling-based dashboards to work. It does NOT trigger <code>StatusTracker</code> callbacks. See Limitations for details.</p> <ol> <li>Context Variable: <code>set_task_status()</code> uses Python's <code>contextvars</code> to update the current worker's state</li> <li>Dashboard Polling: The dashboard polls <code>pipeline.get_dashboard_snapshot()</code> every 0.5s (default, configurable)</li> <li>Real-time Updates: Status changes appear in the dashboard on the next poll cycle</li> <li>No Events: This does NOT emit events - callbacks like <code>on_status_change</code> will never see these updates</li> </ol>"},{"location":"user-guide/pipeline/#example-using-with-pipelinecreate","title":"Example: Using with Pipeline.create()","text":"<pre><code>from antflow import Pipeline, set_task_status\n\nasync def process_file(filename: str):\n    set_task_status(\"\ud83d\udd0d Validating...\")\n    await asyncio.sleep(1)\n\n    set_task_status(\"\ud83d\udcd6 Reading...\")\n    await asyncio.sleep(1)\n\n    set_task_status(\"\u2699\ufe0f  Processing...\")\n    await asyncio.sleep(2)\n\n    set_task_status(\"\ud83d\udcbe Saving...\")\n    await asyncio.sleep(1)\n\n    return {\"file\": filename, \"status\": \"done\"}\n\n# Run with detailed dashboard to see the \"Current Task\" column\npipeline = Pipeline.create().add(\"Process\", process_file, workers=3).build()\nresults = await pipeline.run(files, dashboard=\"detailed\")\n</code></pre>"},{"location":"user-guide/pipeline/#example-using-with-stage","title":"Example: Using with Stage","text":"<pre><code>from antflow import Pipeline, Stage, set_task_status\n\nasync def upload_batch(file_path: str):\n    set_task_status(\"\ud83d\udce6 Preparing batch...\")\n    await asyncio.sleep(0.5)\n\n    set_task_status(\"\u2b06\ufe0f  Uploading to API...\")\n    await asyncio.sleep(1)\n\n    return f\"batch-{file_path}\"\n\nasync def poll_batch(batch_id: str):\n    statuses = [\"validating\", \"processing\", \"finalizing\", \"completed\"]\n\n    for status in statuses:\n        set_task_status(f\"\u23f3 Status: {status}...\")\n        await asyncio.sleep(2)\n\n    return {\"batch_id\": batch_id, \"status\": \"completed\"}\n\n# Multi-stage pipeline with status updates in each stage\nstages = [\n    Stage(name=\"Upload\", workers=5, tasks=[upload_batch]),\n    Stage(name=\"Poll\", workers=10, tasks=[poll_batch])\n]\n\npipeline = Pipeline(stages=stages)\nresults = await pipeline.run(files, dashboard=\"detailed\")\n</code></pre>"},{"location":"user-guide/pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Use Emojis: Visual indicators make status easier to scan (\ud83d\udce6 \ud83d\udd0d \u2b06\ufe0f \u2b07\ufe0f \u2699\ufe0f \u2705 \u274c)</li> <li>Be Specific: Include progress info when possible (<code>Processing 35/100...</code>)</li> <li>Update Frequently: For long operations, update every few seconds</li> <li>Keep It Short: Dashboard space is limited (max ~40 characters)</li> <li>Use with <code>dashboard=\"detailed\"</code>: The \"Current Task\" column is most visible in detailed mode</li> </ol>"},{"location":"user-guide/pipeline/#rate-limiting-updates","title":"Rate Limiting Updates","text":"<p>For tight loops or very frequent updates, you can use the <code>min_interval</code> parameter to avoid excessive updates:</p> <pre><code># Option 1: Rate limiting (max 2 updates per second)\nfor i in range(1000):\n    set_task_status(f\"Processing {i}/1000...\", min_interval=0.5)\n    await process_item(i)\n\n# Option 2: Update every N iterations (more efficient)\nfor i in range(1000):\n    if i % 50 == 0:  # Update every 50 items\n        set_task_status(f\"Processing {i}/1000...\")\n    await process_item(i)\n\n# Option 3: Polling with rate limiting\nwhile True:\n    status = await check_api_status()\n    # Update max once per second, even if polling every 100ms\n    set_task_status(f\"Status: {status}\", min_interval=1.0)\n\n    if status == \"completed\":\n        break\n    await asyncio.sleep(0.1)\n</code></pre> <p>When to use rate limiting: - Tight loops: Processing thousands of items very quickly - Fast polling: Checking status more frequently than you need to update the UI - High-frequency events: Any scenario with &gt;10 updates per second</p> <p>When NOT needed: - Normal tasks: Most tasks update naturally every few seconds - Polling at reasonable intervals: If you poll every 5-10 seconds, no rate limiting needed - Dashboard already limits: The dashboard polls every 100ms, so it naturally rate-limits to ~10 updates/second</p>"},{"location":"user-guide/pipeline/#complete-example","title":"Complete Example","text":"<p>See <code>examples/task_status_complete.py</code> for a comprehensive example demonstrating: - Basic Usage: Simple status updates through multiple steps - Real-World Polling: OpenAI batch processing with multiple API calls - Rate Limiting: Different strategies for controlling update frequency - Custom Dashboard: Full control over status display</p> <p>This single file contains all examples in well-organized sections.</p>"},{"location":"user-guide/pipeline/#limitations","title":"Limitations","text":"<p>IMPORTANT: <code>set_task_status()</code> only works with POLLING, not CALLBACKS</p> <ul> <li>\u2705 Works with Polling (<code>on_update</code>): </li> <li><code>set_task_status()</code> updates <code>WorkerState.current_task</code></li> <li>Dashboards using <code>on_update</code> will see these changes on the next poll (default: every 0.5s)</li> <li> <p>Built-in dashboards (<code>dashboard=\"detailed\"</code>) use polling and show updates automatically</p> </li> <li> <p>\u274c Does NOT work with Callbacks (<code>on_status_change</code>):</p> </li> <li><code>set_task_status()</code> does NOT emit any events</li> <li><code>StatusTracker</code> callbacks (<code>on_status_change</code>, <code>on_task_start</code>, etc.) will NEVER be called</li> <li> <p>This is by design to avoid flooding the event system with hundreds of internal status updates</p> </li> <li> <p>Why this design?</p> </li> <li>Polling reads state when needed (efficient for UI updates)</li> <li>Callbacks fire immediately on events (efficient for alerts/logging)</li> <li>Internal task status is \"state\", not \"events\" - it changes frequently and is best read via polling</li> </ul> <p>Example - What Works and What Doesn't:</p> <pre><code>from antflow import Pipeline, StatusTracker, set_task_status\n\nasync def my_task(item):\n    set_task_status(\"Step 1\")  # \u2190 Updates internal state\n    await asyncio.sleep(1)\n    set_task_status(\"Step 2\")  # \u2190 Updates internal state\n    return item\n\n# \u2705 WORKS: Polling dashboard sees the updates\nclass PollingDashboard:\n    def on_update(self, snapshot):\n        for name, state in snapshot.worker_states.items():\n            print(f\"{name}: {state.current_task}\")\n            # Output: \"Process-W0: Step 1\"\n            #         \"Process-W0: Step 2\"\n\nawait pipeline.run(items, custom_dashboard=PollingDashboard())\n\n# \u274c DOESN'T WORK: Callbacks never see internal status\nasync def on_change(event):\n    print(f\"Event: {event.status}\")\n    # Only prints: \"queued\", \"in_progress\", \"completed\"\n    # NEVER sees \"Step 1\" or \"Step 2\"\n\ntracker = StatusTracker(on_status_change=on_change)\npipeline = Pipeline(stages=[...], status_tracker=tracker)\nawait pipeline.run(items)  # Callbacks won't see set_task_status() calls\n</code></pre> <p>Best Practice - Use Both Together:</p> <pre><code># Use callbacks for critical events (immediate alerts)\nasync def on_failure(event):\n    if event.status == \"failed\":\n        await send_alert(f\"CRITICAL: Item {event.item_id} failed!\")\n\n# Use polling for detailed progress (visual dashboard)\nclass DetailedDashboard:\n    def on_update(self, snapshot):\n        for name, state in snapshot.worker_states.items():\n            if state.current_task:\n                print(f\"{name}: {state.current_task}\")\n\n# Combine both\ntracker = StatusTracker(on_status_change=on_failure)\npipeline = Pipeline(stages=[...], status_tracker=tracker)\nawait pipeline.run(items, custom_dashboard=DetailedDashboard())\n</code></pre> <p>Other Limitations:</p> <ul> <li>Worker-Scoped: Only updates the current worker's status (not global)</li> <li>No History: Previous status messages are not stored (only current status is visible)</li> </ul>"},{"location":"user-guide/pipeline/#using-with-custom-dashboards","title":"Using with Custom Dashboards","text":"<p>The built-in dashboards (<code>dashboard=\"detailed\"</code>) show the \"Current Task\" column automatically. If you create a custom dashboard, you have full access to <code>WorkerState.current_task</code>:</p> <pre><code>class MyCustomDashboard:\n    def on_update(self, snapshot: DashboardSnapshot):\n        for worker_name, state in snapshot.worker_states.items():\n            # Access the status set by set_task_status()\n            current_status = state.current_task\n\n            print(f\"{worker_name}: {current_status}\")\n            # Example output: \"Process-W0: \u23f3 Polling: processing...\"\n</code></pre> <p>See the \"Custom Dashboard\" section in <code>examples/task_status_complete.py</code> for a complete implementation with a rich table display.</p>"},{"location":"user-guide/pipeline/#monitoring-strategies","title":"Monitoring Strategies","text":"<p>AntFlow supports two primary ways to monitor your pipeline: Event-Driven (Callbacks) and Polling.</p>"},{"location":"user-guide/pipeline/#strategy-1-event-driven-callbacks","title":"Strategy 1: Event-Driven (Callbacks)","text":"<p>Use <code>StatusTracker</code> callbacks to react immediately when events occur. This is best for: - Real-time dashboards (low latency) - Logging specific events (e.g., errors) - Triggering external actions (e.g., alerts)</p> <p>Example: <code>examples/rich_callback_dashboard.py</code> demonstrates this approach.</p> <pre><code>async def on_status_change(event):\n    # React immediately to status changes\n    print(f\"Item {event.item_id} is now {event.status}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\npipeline = Pipeline(stages=[...], status_tracker=tracker)\nawait pipeline.run(items)\n</code></pre>"},{"location":"user-guide/pipeline/#strategy-2-polling-loop","title":"Strategy 2: Polling (Loop)","text":"<p>Run a separate loop to periodically check <code>pipeline.get_stats()</code> or <code>pipeline.get_dashboard_snapshot()</code>. This is best for: - Periodic metrics aggregation - Decoupled monitoring (UI runs at its own FPS) - Reducing overhead (batching updates)</p> <p>Example: <code>examples/rich_polling_dashboard.py</code> demonstrates this approach.</p> <pre><code>async def monitor_loop(pipeline):\n    while True:\n        # Poll current state every second\n        stats = pipeline.get_stats()\n        print(f\"Processed: {stats.items_processed}, In-Flight: {stats.items_in_flight}\")\n        await asyncio.sleep(1.0)\n\nasync with asyncio.TaskGroup() as tg:\n    tg.create_task(monitor_loop(pipeline))\n    await pipeline.run(items)\n</code></pre>"},{"location":"user-guide/pipeline/#strategy-3-programmatic-monitoring-get_stats","title":"Strategy 3: Programmatic Monitoring (get_stats)","text":"<p>For simple scripts that just need a summary after execution, or for background metrics collection, use <code>pipeline.get_stats()</code>. This returns a <code>PipelineStats</code> object containing high-level metrics and detailed <code>StageStats</code> for each stage.</p> <pre><code>stats = pipeline.get_stats()\n\nprint(f\"Total processed: {stats.items_processed}\")\nprint(f\"Total failed: {stats.items_failed}\")\n\n# Access per-stage metrics\nfor stage_name, stage_stat in stats.stage_stats.items():\n    print(f\"Stage {stage_name}:\")\n    print(f\"  Completed: {stage_stat.completed_items}\")\n    print(f\"  Failed: {stage_stat.failed_items}\")\n    print(f\"  In progress: {stage_stat.in_progress_items}\")\n</code></pre>"},{"location":"user-guide/pipeline/#feeding-data","title":"Feeding Data","text":""},{"location":"user-guide/pipeline/#synchronous-iterable","title":"Synchronous Iterable","text":"<pre><code>items = list(range(100))\nresults = await pipeline.run(items)\n</code></pre>"},{"location":"user-guide/pipeline/#async-iterable","title":"Async Iterable","text":"<pre><code>import asyncio\n\nasync def data_generator():\n    for i in range(100):\n        await asyncio.sleep(0.01)\n        yield i\n\nawait pipeline.feed_async(data_generator())\n</code></pre>"},{"location":"user-guide/pipeline/#streaming-results","title":"Streaming Results","text":"<p>For memory-intensive workloads or when you want to process results as they arrive (out-of-order), use <code>pipeline.stream()</code>. This returns an async generator yielding results in completion order.</p> <pre><code>async for result in pipeline.stream(range(100)):\n    print(f\"Processing result for item {result.id}: {result.value}\")\n    # Process result immediately, freeing memory\n</code></pre> <p>Unlike <code>run()</code>, which returns a complete list, <code>stream()</code> is more memory-efficient for very large datasets.</p>"},{"location":"user-guide/pipeline/#dict-input","title":"Dict Input","text":"<p>Pass dict items with custom IDs:</p> <pre><code>items = [\n    {\"id\": \"user_1\", \"value\": {\"name\": \"Alice\"}},\n    {\"id\": \"user_2\", \"value\": {\"name\": \"Bob\"}},\n]\nresults = await pipeline.run(items)\n</code></pre>"},{"location":"user-guide/pipeline/#context-manager","title":"Context Manager","text":"<p>Use pipeline as a context manager for automatic cleanup:</p> <pre><code>from antflow import Pipeline\n\nasync with Pipeline(stages=[stage1, stage2]) as pipeline:\n    results = await pipeline.run(items)\n# Pipeline is automatically shut down\n</code></pre>"},{"location":"user-guide/pipeline/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/pipeline/#tracking-failures-with-statustracker","title":"Tracking Failures with StatusTracker","text":"<pre><code>from antflow import Pipeline, Stage, StatusTracker\n\ntracker = StatusTracker()\n\nstage = Stage(\n    name=\"RiskyStage\",\n    workers=3,\n    tasks=[risky_task],\n    retry=\"per_task\",\n    task_attempts=3\n)\n\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\nresults = await pipeline.run(items)\n\n# Get statistics\nstats = tracker.get_stats()\nprint(f\"Succeeded: {stats['completed']}\")\nprint(f\"Failed: {stats['failed']}\")\n\n# Get failed items\nfailed_items = tracker.get_by_status(\"failed\")\nfor event in failed_items:\n    print(f\"Item {event.item_id}: {event.metadata['error']}\")\n</code></pre>"},{"location":"user-guide/pipeline/#extracting-error-information","title":"Extracting Error Information","text":"<p>Errors are available in the event metadata:</p> <pre><code>from antflow import StatusTracker\n\nasync def on_status_change(event):\n    if event.status == \"failed\":\n        error = event.metadata.get('error')\n        print(f\"Item {event.item_id} failed: {error}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\n</code></pre>"},{"location":"user-guide/pipeline/#worker-level-tracking","title":"Worker-Level Tracking","text":"<p>Each worker has a unique ID (0 to N-1). Track which worker processes which item:</p> <pre><code>from antflow import Pipeline, StatusTracker\n\nasync def on_status_change(event):\n    if event.status == \"in_progress\":\n        print(f\"Worker {event.worker_id}: processing {event.item_id}\")\n    elif event.status == \"completed\":\n        print(f\"Worker {event.worker_id}: completed {event.item_id}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\n\nworker_names = pipeline.get_worker_names()\n</code></pre> <p>See the Worker Tracking Guide for detailed examples including:</p> <ul> <li>Custom item IDs for better tracking</li> <li>Worker utilization analysis</li> <li>Load balancing monitoring</li> <li>Error tracking by worker</li> </ul>"},{"location":"user-guide/pipeline/#priority-queues","title":"Priority Queues","text":"<p>AntFlow uses Priority Queues internally. You can assign a priority level to items when feeding them into the pipeline. Lower numbers indicate higher priority (processed first). The default priority is <code>100</code>.</p> <ul> <li>Items with the same priority are processed in FIFO order.</li> <li>Priority is preserved across stages (unless a custom <code>feed</code> injects with different priority).</li> <li>Retries (per-task or per-stage) currently preserve the original priority.</li> </ul> <pre><code># Expedited items (Priority 10)\nawait pipeline.feed(vip_items, priority=10)\n\n# Normal items (Priority 100)\nawait pipeline.feed(regular_items)\n\n# Background/Low priority (Priority 500)\nawait pipeline.feed(maintenance_items, priority=500)\n</code></pre>"},{"location":"user-guide/pipeline/#complete-etl-example","title":"Complete ETL Example","text":"<pre><code>import asyncio\nfrom antflow import Pipeline, Stage\n\nclass ETLProcessor:\n    async def extract(self, item_id):\n        # Fetch from API/database\n        await asyncio.sleep(0.1)\n        return {\"id\": item_id, \"data\": f\"raw_{item_id}\"}\n\n    async def validate(self, data):\n        # Validate data\n        if \"data\" not in data:\n            raise ValueError(\"Invalid data\")\n        return data\n\n    async def transform(self, data):\n        # Transform data\n        data[\"processed\"] = data[\"data\"].upper()\n        return data\n\n    async def enrich(self, data):\n        # Enrich with additional data\n        data[\"metadata\"] = {\"timestamp\": \"2025-10-09\"}\n        return data\n\n    async def load(self, data):\n        # Save to database\n        await asyncio.sleep(0.1)\n        data[\"saved\"] = True\n        return data\n\nasync def main():\n    processor = ETLProcessor()\n\n    # Extract stage with high concurrency\n    extract_stage = Stage(\n        name=\"Extract\",\n        workers=10,\n        tasks=[processor.extract],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=2.0\n    )\n\n    # Transform stage with validation\n    transform_stage = Stage(\n        name=\"Transform\",\n        workers=5,\n        tasks=[processor.validate, processor.transform, processor.enrich],\n        retry=\"per_stage\",  # Transactional\n        stage_attempts=3\n    )\n\n    # Load stage with retries\n    load_stage = Stage(\n        name=\"Load\",\n        workers=3,\n        tasks=[processor.load],\n        retry=\"per_task\",\n        task_attempts=5,\n        task_wait_seconds=3.0\n    )\n\n    # Build pipeline\n    pipeline = Pipeline(\n        stages=[extract_stage, transform_stage, load_stage]\n    )\n\n    # Process items\n    item_ids = range(100)\n    results = await pipeline.run(item_ids)\n\n    print(f\"Processed {len(results)} items\")\n    print(f\"Stats: {pipeline.get_stats()}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/pipeline/#best-practices_1","title":"Best Practices","text":""},{"location":"user-guide/pipeline/#worker-pool-sizing","title":"Worker Pool Sizing","text":"<ul> <li>Extract/Fetch: More workers (I/O-bound)</li> <li>Transform: Moderate workers (CPU-bound)</li> <li>Load/Save: Fewer workers (rate-limited)</li> </ul>"},{"location":"user-guide/pipeline/#retry-configuration","title":"Retry Configuration","text":"<ul> <li>Use per-task for independent operations</li> <li>Use per-stage for transactional operations</li> <li>Set appropriate <code>task_wait_seconds</code> for rate limiting</li> <li>Increase <code>task_attempts</code> for flaky external services</li> </ul>"},{"location":"user-guide/pipeline/#callbacks","title":"Callbacks","text":"<ul> <li>Use callbacks for logging and monitoring</li> <li>Keep callbacks lightweight (use queues for heavy operations)</li> <li>Avoid long-running operations in callbacks</li> </ul>"},{"location":"user-guide/pipeline/#error-handling_1","title":"Error Handling","text":"<ul> <li>Always set up <code>on_failure</code> callbacks for production</li> <li>Log failed items for later retry/analysis</li> <li>Monitor <code>items_failed</code> metric</li> </ul>"},{"location":"user-guide/pipeline/#advanced-internals","title":"Advanced Internals","text":"<p>[!WARNING] This section covers internal implementation details. These APIs are protected (<code>_prefix</code>) and may change between minor versions. Use them with caution when building custom subclasses.</p>"},{"location":"user-guide/pipeline/#internal-queue-structure-_queues","title":"Internal Queue Structure (<code>_queues</code>)","text":"<p>AntFlow manages data flow between stages using a list of internal queues, accessible via <code>self._queues</code>.</p> <ul> <li>Type: <code>List[asyncio.PriorityQueue]</code></li> <li>Indexing: <code>_queues[i]</code> corresponds to <code>stages[i]</code>.</li> <li>Item Structure: Items are stored as tuples to ensure correct priority ordering and FIFO stability:     <pre><code>(priority, sequence_id, (payload, attempt))\n</code></pre><ul> <li>priority (<code>int</code>): Lower number = higher priority.</li> <li>sequence_id (<code>int</code>): Monotonically increasing counter to ensure FIFO order for same-priority items.</li> <li>payload (<code>dict</code>): The internal item wrapper (see below).</li> <li>attempt (<code>int</code>): Current retry attempt number (1-indexed).</li> </ul> </li> </ul>"},{"location":"user-guide/pipeline/#payload-structure-_prepare_payload","title":"Payload Structure (<code>_prepare_payload</code>)","text":"<p>When items enter the pipeline (via <code>feed</code> or <code>run</code>), they are wrapped in an internal dictionary structure to track metadata and ensure unique identification. The <code>_prepare_payload(item)</code> method handles this normalization.</p> <p>Internal Payload Format: <pre><code>{\n    \"id\": Any,          # Unique identifier (extracted from dict or generated)\n    \"value\": Any,       # The actual data being processed\n    \"_sequence_id\": int # Global sequence ID for result ordering\n}\n</code></pre></p> <ul> <li>ID Extraction:<ul> <li>If item is a <code>dict</code> and has an \"id\" key, that is used.</li> <li>Otherwise, the item's index in the input iterable is used as the ID.</li> </ul> </li> <li>Value Extraction:<ul> <li>If item is a <code>dict</code> and has a \"value\" key, that is used.</li> <li>Otherwise, the item itself is used as the value.</li> </ul> </li> </ul> <p>Example Override: If you need custom ID generation logic, you can override <code>_prepare_payload</code> in a subclass:</p> <pre><code>class CustomPipeline(Pipeline):\n    def _prepare_payload(self, item):\n        # Custom logic: use 'uuid' field as ID if present\n        if isinstance(item, dict) and 'uuid' in item:\n            return {\n                \"id\": item['uuid'], \n                \"value\": item, \n                \"_sequence_id\": self._msg_counter\n            }\n        return super()._prepare_payload(item)\n</code></pre>"},{"location":"user-guide/progress/","title":"Progress Bar Guide","text":"<p>AntFlow provides built-in progress visualization options that require zero configuration.</p>"},{"location":"user-guide/progress/#quick-start","title":"Quick Start","text":"<p>The simplest way to add progress visualization is with the <code>progress=True</code> flag:</p> <pre><code>import asyncio\nfrom antflow import Pipeline\n\nasync def task(x):\n    return x * 2\n\nasync def main():\n    results = await Pipeline.quick(range(100), task, workers=10, progress=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This displays a minimal progress bar in your terminal:</p> <pre><code>[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 42% | 126/300 | 24.5/s | 2 failed\n</code></pre>"},{"location":"user-guide/progress/#dashboard-options","title":"Dashboard Options","text":"<p>For more detailed monitoring, use the <code>dashboard</code> parameter:</p> <pre><code>import asyncio\nfrom antflow import Pipeline\n\nasync def task(x):\n    await asyncio.sleep(0.01)\n    return x * 2\n\nasync def main():\n    items = range(100)\n    # Use: \"compact\", \"detailed\", or \"full\"\n    results = await Pipeline.quick(items, task, workers=5, dashboard=\"detailed\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/progress/#compact-dashboard","title":"Compact Dashboard","text":"<p>Shows a single panel with:</p> <ul> <li>Progress bar</li> <li>Current stage activity</li> <li>Processing rate and ETA</li> <li>Success/failure counts</li> </ul> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 AntFlow Pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 42%              \u2502\n\u2502  Stage: Process (3/5 workers busy)                 \u2502\n\u2502  Rate: 24.5 items/sec | ETA: 00:02:15              \u2502\n\u2502  OK: 126 | Failed: 2 | Remaining: 172              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"user-guide/progress/#detailed-dashboard","title":"Detailed Dashboard","text":"<p>Shows:</p> <ul> <li>Overall progress bar with rate and ETA</li> <li>Per-stage progress table with worker counts</li> <li>Worker performance metrics</li> </ul>"},{"location":"user-guide/progress/#full-dashboard","title":"Full Dashboard","text":"<p>The most comprehensive option, showing:</p> <ul> <li>Overview statistics</li> <li>Stage metrics</li> <li>Individual worker monitoring</li> <li>Item tracking (requires <code>StatusTracker</code>)</li> </ul> <p>For item tracking, add a <code>StatusTracker</code>:</p> <pre><code>import asyncio\nfrom antflow import Pipeline, StatusTracker\n\nasync def task(x):\n    return x\n\nasync def main():\n    tracker = StatusTracker()\n    results = await (\n        Pipeline.create()\n        .add(\"Process\", task, workers=5)\n        .with_tracker(tracker)\n        .run(range(50), dashboard=\"full\")\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user-guide/progress/#using-with-pipelinequick","title":"Using with Pipeline.quick()","text":"<p>Progress works with all Pipeline APIs:</p> <pre><code># With quick()\nresults = await Pipeline.quick(items, process, workers=10, progress=True)\n\n# With dashboard\nresults = await Pipeline.quick(\n    items,\n    [fetch, process, save],\n    workers=5,\n    dashboard=\"compact\"\n)\n</code></pre>"},{"location":"user-guide/progress/#using-with-builder-api","title":"Using with Builder API","text":"<pre><code>results = await (\n    Pipeline.create()\n    .add(\"Fetch\", fetch, workers=10)\n    .add(\"Process\", process, workers=5)\n    .run(items, dashboard=\"detailed\")\n)\n</code></pre>"},{"location":"user-guide/progress/#note-on-mutual-exclusion","title":"Note on Mutual Exclusion","text":"<p>You cannot use both <code>progress=True</code> and <code>dashboard</code> at the same time:</p> <pre><code># This will raise ValueError\nresults = await pipeline.run(items, progress=True, dashboard=\"compact\")\n</code></pre> <p>Choose one or the other based on your needs.</p>"},{"location":"user-guide/worker-tracking/","title":"Worker Tracking","text":"<p>Track which specific worker is processing each item in your pipeline for detailed monitoring and debugging.</p>"},{"location":"user-guide/worker-tracking/#overview","title":"Overview","text":"<p>Each worker in a pipeline stage has a unique ID (0 to N-1). The <code>StatusEvent.worker_id</code> property allows you to track which worker is processing each item.</p>"},{"location":"user-guide/worker-tracking/#worker-ids","title":"Worker IDs","text":"<p>Worker IDs are zero-indexed integers:</p> <ul> <li>Stage with 1 worker: worker ID <code>0</code></li> <li>Stage with 3 workers: worker IDs <code>0</code>, <code>1</code>, <code>2</code></li> <li>Stage with 10 workers: worker IDs <code>0</code> through <code>9</code></li> </ul>"},{"location":"user-guide/worker-tracking/#worker-naming-convention","title":"Worker Naming Convention","text":"<p>Workers are internally named using the pattern <code>{StageName}-W{WorkerID}</code>:</p> <ul> <li><code>\"Fetch-W0\"</code> - Worker 0 in Fetch stage</li> <li><code>\"Process-W5\"</code> - Worker 5 in Process stage</li> <li><code>\"Transform-W12\"</code> - Worker 12 in Transform stage</li> </ul> <p>The <code>worker_id</code> property extracts the numeric ID from the worker name.</p>"},{"location":"user-guide/worker-tracking/#getting-worker-names","title":"Getting Worker Names","text":"<p>Use <code>pipeline.get_worker_names()</code> to get all worker names before running:</p> <pre><code>from antflow import Pipeline, Stage\n\nstage1 = Stage(name=\"Fetch\", workers=3, tasks=[fetch_data])\nstage2 = Stage(name=\"Process\", workers=5, tasks=[process_data])\n\npipeline = Pipeline(stages=[stage1, stage2])\n\nworker_names = pipeline.get_worker_names()\nprint(worker_names)\n# {\n#     \"Fetch\": [\"Fetch-W0\", \"Fetch-W1\", \"Fetch-W2\"],\n#     \"Process\": [\"Process-W0\", \"Process-W1\", \"Process-W2\", \"Process-W3\", \"Process-W4\"]\n# }\n</code></pre> <p>This is useful for:</p> <ul> <li>Setting up monitoring dashboards before pipeline runs</li> <li>Pre-allocating tracking structures</li> <li>Understanding pipeline topology</li> </ul>"},{"location":"user-guide/worker-tracking/#tracking-worker-assignments","title":"Tracking Worker Assignments","text":"<p>Track which worker processes which item:</p> <pre><code>from antflow import StatusTracker\nfrom collections import defaultdict\n\nitem_to_worker = {}\nworker_activity = defaultdict(list)\n\nasync def on_status_change(event):\n    if event.status == \"in_progress\":\n        item_to_worker[event.item_id] = event.worker_id\n        worker_activity[event.worker_id].append(event.item_id)\n        print(f\"Worker {event.worker_id} started processing {event.item_id}\")\n\ntracker = StatusTracker(on_status_change=on_status_change)\npipeline = Pipeline(stages=[stage], status_tracker=tracker)\n\nresults = await pipeline.run(items)\n\nfor worker_id in sorted(worker_activity.keys()):\n    items_count = len(worker_activity[worker_id])\n    print(f\"Worker {worker_id} processed {items_count} items\")\n</code></pre>"},{"location":"user-guide/worker-tracking/#custom-item-ids","title":"Custom Item IDs","text":"<p>By default, items are assigned sequential IDs (0, 1, 2, ...). You can provide custom IDs for better tracking.</p>"},{"location":"user-guide/worker-tracking/#using-custom-ids","title":"Using Custom IDs","text":"<p>Pass items as dictionaries with an <code>\"id\"</code> field:</p> <pre><code>items = [\n    {\"id\": \"batch_0001\", \"value\": data1},\n    {\"id\": \"batch_0002\", \"value\": data2},\n    {\"id\": \"user_12345\", \"value\": user_data},\n]\n\nresults = await pipeline.run(items)\n</code></pre> <p>Now you can track: <code>\"batch_0001 was processed by Worker 5\"</code></p>"},{"location":"user-guide/worker-tracking/#without-custom-ids","title":"Without Custom IDs","text":"<p>For simple use cases, just pass values directly:</p> <pre><code>items = [data1, data2, data3]\nresults = await pipeline.run(items)\n</code></pre> <p>Items will be tracked as <code>0</code>, <code>1</code>, <code>2</code>, etc.</p>"},{"location":"user-guide/worker-tracking/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/worker-tracking/#1-worker-load-balancing-analysis","title":"1. Worker Load Balancing Analysis","text":"<p>Identify if work is distributed evenly:</p> <pre><code>from collections import defaultdict\n\nworker_times = defaultdict(list)\n\nasync def track_performance(event):\n    if event.status == \"in_progress\":\n        start_times[event.item_id] = event.timestamp\n    elif event.status == \"completed\":\n        duration = event.timestamp - start_times[event.item_id]\n        worker_times[event.worker_id].append(duration)\n\ntracker = StatusTracker(on_status_change=track_performance)\n\nfor worker_id, times in worker_times.items():\n    avg_time = sum(times) / len(times)\n    print(f\"Worker {worker_id}: avg {avg_time:.2f}s per item\")\n</code></pre>"},{"location":"user-guide/worker-tracking/#2-live-dashboard","title":"2. Live Dashboard","text":"<p>Display real-time worker activity:</p> <pre><code>from antflow import StatusTracker\n\nasync def update_worker_dashboard(event):\n    if event.status == \"in_progress\":\n        await websocket.send_json({\n            \"worker_id\": event.worker_id,\n            \"worker_name\": event.worker,\n            \"item_id\": event.item_id,\n            \"stage\": event.stage,\n            \"timestamp\": event.timestamp\n        })\n\ntracker = StatusTracker(on_status_change=update_worker_dashboard)\n</code></pre>"},{"location":"user-guide/worker-tracking/#3-error-tracking-by-worker","title":"3. Error Tracking by Worker","text":"<p>Identify problematic workers:</p> <pre><code>from collections import defaultdict\n\nworker_errors = defaultdict(int)\n\nasync def track_errors(event):\n    if event.status == \"failed\" and event.worker_id is not None:\n        worker_errors[event.worker_id] += 1\n        error = event.metadata.get(\"error\", \"Unknown\")\n        print(f\"Worker {event.worker_id} error: {error}\")\n\ntracker = StatusTracker(on_status_change=track_errors)\n\nfor worker_id, error_count in worker_errors.items():\n    print(f\"Worker {worker_id} had {error_count} errors\")\n</code></pre>"},{"location":"user-guide/worker-tracking/#4-worker-utilization-metrics","title":"4. Worker Utilization Metrics","text":"<p>Track worker efficiency:</p> <pre><code>from collections import defaultdict\n\nworker_stats = defaultdict(lambda: {\"completed\": 0, \"failed\": 0})\n\nasync def track_utilization(event):\n    if event.status in (\"completed\", \"failed\") and event.worker_id is not None:\n        worker_stats[event.worker_id][event.status] += 1\n\ntracker = StatusTracker(on_status_change=track_utilization)\n\nfor worker_id, stats in sorted(worker_stats.items()):\n    total = stats[\"completed\"] + stats[\"failed\"]\n    success_rate = (stats[\"completed\"] / total * 100) if total &gt; 0 else 0\n    print(f\"Worker {worker_id}: {stats['completed']}/{total} ({success_rate:.1f}%)\")\n</code></pre>"},{"location":"user-guide/worker-tracking/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom collections import defaultdict\nfrom antflow import Pipeline, Stage, StatusTracker\n\nasync def process_batch(batch_data):\n    await asyncio.sleep(0.2)\n    return f\"processed_{batch_data}\"\n\nasync def main():\n    item_to_worker = {}\n    worker_activity = defaultdict(list)\n\n    async def on_status_change(event):\n        if event.status == \"in_progress\":\n            item_to_worker[event.item_id] = event.worker_id\n            worker_activity[event.worker_id].append(event.item_id)\n            print(f\"[Worker {event.worker_id:2d}] Processing {event.item_id}\")\n        elif event.status == \"completed\":\n            print(f\"[Worker {event.worker_id:2d}] Completed {event.item_id}\")\n\n    tracker = StatusTracker(on_status_change=on_status_change)\n\n    stage = Stage(name=\"ProcessBatch\", workers=5, tasks=[process_batch])\n    pipeline = Pipeline(stages=[stage], status_tracker=tracker)\n\n    worker_names = pipeline.get_worker_names()\n    print(f\"Available workers: {worker_names}\\n\")\n\n    items = [\n        {\"id\": f\"batch_{i:04d}\", \"value\": f\"data_{i}\"}\n        for i in range(20)\n    ]\n\n    results = await pipeline.run(items)\n\n    print(\"\\n=== Worker Utilization ===\")\n    for worker_id in sorted(worker_activity.keys()):\n        items_processed = worker_activity[worker_id]\n        print(f\"Worker {worker_id}: {len(items_processed)} items\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/worker-tracking/#key-points","title":"Key Points","text":"<ul> <li>Worker IDs are 0-indexed: First worker is <code>0</code>, not <code>1</code></li> <li>Custom IDs are optional: Use them only if you need tracking</li> <li>Worker names available before run: Use <code>get_worker_names()</code> for setup</li> <li>Property extraction: <code>worker_id</code> is extracted from <code>worker</code> name automatically</li> <li>Stage-specific: Each stage has its own set of worker IDs starting from 0</li> </ul>"},{"location":"user-guide/worker-tracking/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Error Handling</li> <li>Explore Advanced Pipeline Usage</li> <li>See Examples</li> </ul>"}]}